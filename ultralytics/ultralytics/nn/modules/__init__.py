# Ultralytics ðŸš€ AGPL-3.0 License - https://ultralytics.com/license
"""
Ultralytics neural network modules.

This module provides access to various neural network components used in Ultralytics models, including convolution
blocks, attention mechanisms, transformer components, and detection/segmentation heads.

Examples:
    Visualize a module with Netron
    >>> from ultralytics.nn.modules import Conv
    >>> import torch
    >>> import subprocess
    >>> x = torch.ones(1, 128, 40, 40)
    >>> m = Conv(128, 128)
    >>> f = f"{m._get_name()}.onnx"
    >>> torch.onnx.export(m, x, f)
    >>> subprocess.run(f"onnxslim {f} {f} && open {f}", shell=True, check=True)  # pip install onnxslim
"""

from .block import (
    C1,
    C2,
    C2PSA,
    C3,
    C3TR,
    CIB,
    DFL,
    ELAN1,
    PSA,
    SPP,
    SPPELAN,
    SPPF,
    A2C2f,
    AConv,
    ADown,
    Attention,
    BNContrastiveHead,
    Bottleneck,
    BottleneckCSP,
    C2f,
    C2fAttn,
    C2fCIB,
    C2fPSA,
    C3Ghost,
    C3k2,
    C3x,
    CBFuse,
    CBLinear,
    ContrastiveHead,
    GhostBottleneck,
    HGBlock,
    HGStem,
    ImagePoolingAttn,
    MaxSigmoidAttnBlock,
    Proto,
    RepC3,
    RepNCSPELAN4,
    RepVGGDW,
    BiFPNLite,
    ResNetLayer,
    SCDown,
    TorchVision,
)
from .conv import (
    CBAM,
    ChannelAttention,
    Concat,
    Conv,
    Conv2,
    ConvTranspose,
    DWConv,
    DWConvTranspose2d,
    Focus,
    GhostConv,
    Index,
    LightConv,
    RepConv,
    SpatialAttention,
)
from .head import (
    OBB,
    Classify,
    Detect,
    LRPCHead,
    Pose,
    RTDETRDecoder,
    RTDETRBiFPNDecoder,
    Segment,
    WorldDetect,
    YOLOEDetect,
    YOLOESegment,
    v10Detect,
)
from .transformer import (
    AIFI,
    MLP,
    DeformableTransformerDecoder,
    DeformableTransformerDecoderLayer,
    LayerNorm2d,
    MLPBlock,
    MSDeformAttn,
    TransformerBlock,
    TransformerEncoderLayer,
    TransformerLayer,
)
from .mobilenetv4 import (
    EdgeResidual,
    UniversalInvertedResidual,
    MobileViTBlock,
)
from .sea_attention import (
    SEA_Attention,
    SEA_Attention_Light,
    Sea_Attention_Simplified,
    OptimizedSEA_Attention,
    EfficientSEA_Attention_Light,
    TransformerEnhancedSEA,
    create_sea_attention,
)
from .fusion import (
    ASFF,
    ASFF_Simple,
    DySample,
)

__all__ = (
    "C1",
    "C2",
    "C3",
    "C2f",
    "C2fCIB",
    "C3x",
    "C3TR",
    "C3Ghost",
    "GhostBottleneck",
    "Bottleneck",
    "BottleneckCSP",
    "Proto",
    "DWConv",
    "DWConvTranspose2d",
    "Conv",
    "Conv2",
    "LightConv",
    "RepConv",
    "ConvTranspose",
    "Focus",
    "GhostConv",
    "ChannelShuffle",
    "Classify",
    "TransformerEncoderLayer",
    "TransformerLayer",
    "TransformerBlock",
    "MLPBlock",
    "LayerNorm2d",
    "DFL",
    "HGBlock",
    "HGStem",
    "SPP",
    "SPPF",
    "C2fPSA",
    "PSA",
    "SCDown",
    "SPPELAN",
    "RepC3",
    "RepNCSPELAN4",
    "ADown",
    "AConv",
    "ELAN1",
    "PWConv",
    "RepVGGDW",
    "CIB",
    "C2fCIB",
    "Attention",
    "ImagePoolingAttn",
    "ContrastiveHead",
    "BNContrastiveHead",
    "WorldDetect",
    "v10Detect",
    "EdgeResidual",
    "UniversalInvertedResidual",
    "MobileViTBlock",
    "SEA_Attention",
    "SEA_Attention_Light",
    "Sea_Attention_Simplified", 
    "OptimizedSEA_Attention",
    "EfficientSEA_Attention_Light",
    "TransformerEnhancedSEA",
    "create_sea_attention",
    "BiFPNLite",
    "RTDETRBiFPNDecoder",
    "ASFF",
    "ASFF_Simple",
    "DySample",
)
