#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ViLD-RTDETR ä¸ åŸå§‹ RTDETR æ€§èƒ½å¯¹æ¯”è¯„ä¼°è„šæœ¬
æ­¤è„šæœ¬ç”¨äºæ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹çš„æ€§èƒ½å·®å¼‚å’Œæå‡æŒ‡æ ‡
"""

import os
import sys
import time
import numpy as np
import cv2
import torch
import torchvision.transforms as T
from PIL import Image, ImageDraw, ImageFont
from tqdm import tqdm
import matplotlib.pyplot as plt
import json

# æ·»åŠ å®‰å…¨çš„å…¨å±€å˜é‡ï¼Œå…è®¸åŠ è½½ ultralytics æ¨¡å‹
try:
    import torch.serialization
    torch.serialization.add_safe_globals(['ultralytics.nn.tasks.DetectionModel'])
except (ImportError, AttributeError):
    print("âš ï¸ æ— æ³•æ·»åŠ å®‰å…¨å…¨å±€å˜é‡ï¼Œå°†å°è¯•ä½¿ç”¨ weights_only=False åŠ è½½æ¨¡å‹")

# è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
current_dir = os.path.dirname(os.path.abspath(__file__))
# è·å–é¡¹ç›®æ ¹ç›®å½•
project_root = os.path.dirname(os.path.dirname(current_dir))
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°æ¨¡å—æœç´¢è·¯å¾„
sys.path.append(project_root)
# ä¸æ·»åŠ RTDETRè·¯å¾„ï¼Œé¿å…å¯¼å…¥é—®é¢˜
# sys.path.append(os.path.join(project_root, 'src/RT-DETR/rtdetr_pytorch'))

def compare_models(image_path, output_dir=None, conf_threshold=0.35):
    """æ¯”è¾ƒViLD-RTDETRæ¨¡å‹ä¸åŸå§‹RTDETRæ¨¡å‹çš„æ€§èƒ½"""
    from src.vild.vild_modular.config import MODEL_CONFIG, INFERENCE_CONFIG
    from src.vild.vild_modular.detector import FixedViLDDetector
    from src.vild.vild_modular.scene_classifier import SceneClassifier
    from src.vild.vild_modular.model import load_models
    
    # å®šä¹‰COCOç±»åˆ«åç§°ï¼Œé¿å…å¯¼å…¥RTDETRçš„ç±»åˆ«
    COCO_CLASSES = [
        'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',
        'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',
        'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',
        'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',
        'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',
        'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',
        'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',
        'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',
        'hair drier', 'toothbrush'
    ]
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    if output_dir is None:
        output_dir = os.path.join(current_dir, "model_comparison_results")
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"ğŸ“ ç»“æœå°†ä¿å­˜åœ¨: {output_dir}")
    
    # ç¡®å®šè®¾å¤‡
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½ViLD-RTDETRæ¨¡å‹
    print("ğŸ“¦ æ­£åœ¨åŠ è½½ViLD-RTDETRæ¨¡å‹...")
    rtdetr_model, clip_model, image_processor, clip_preprocess = load_models(
        rtdetr_path=MODEL_CONFIG['rtdetr_model_path'],
        clip_name=MODEL_CONFIG['clip_model_name'],
        device=device
    )
    
    # åˆ›å»ºåœºæ™¯åˆ†ç±»å™¨
    scene_classifier = SceneClassifier(
        clip_model=clip_model,
        clip_preprocess=clip_preprocess,
        device=device
    )
    
    # åˆ›å»ºViLDæ£€æµ‹å™¨
    vild_detector = FixedViLDDetector(
        clip_model=clip_model,
        detector_model=rtdetr_model,
        image_processor=image_processor,
        clip_preprocess=clip_preprocess,
        device=device,
        projector_path=MODEL_CONFIG['projector_path'],
        config={'model': MODEL_CONFIG, 'inference': INFERENCE_CONFIG}
    )
    
    # åŠ è½½åŸå§‹RTDETRæ¨¡å‹
    print("ğŸ“¦ æ­£åœ¨åŠ è½½åŸå§‹RTDETRæ¨¡å‹...")
    try:
        # ä½¿ç”¨ transformers åº“ç›´æ¥åŠ è½½ RTDETR æ¨¡å‹
        from transformers import RTDetrForObjectDetection, RTDetrImageProcessor
        
        rtdetr_orig_path = os.path.join(project_root, "rtdetr-l.pt")
        
        if os.path.exists(rtdetr_orig_path):
            # åˆ›å»ºåŸå§‹æ¨¡å‹å®ä¾‹
            print("ğŸ“„ æ‰¾åˆ°åŸå§‹ RTDETR æ¨¡å‹æƒé‡æ–‡ä»¶")
            print("ğŸ”„ åŠ è½½åŸå§‹ RTDETR æ¨¡å‹")
            
            # å°è¯•ç›´æ¥ä½¿ç”¨ torch.hub.load åŠ è½½æ¨¡å‹
            try:
                # ç›´æ¥ä»æœ¬åœ°åŠ è½½
                print(f"å°è¯•ä½¿ç”¨ torch.hub.load åŠ è½½æ¨¡å‹")
                rtdetr_orig = torch.hub.load('ultralytics/yolov5', 'custom', path=rtdetr_orig_path, device=device)
                rtdetr_orig.eval()
                rtdetr_orig_processor = None  # YOLOv5ä¸éœ€è¦å•ç‹¬çš„å¤„ç†å™¨
                print("âœ… ä½¿ç”¨ torch.hub.load æˆåŠŸåŠ è½½æ¨¡å‹")
            except Exception as e:
                print(f"âš ï¸ torch.hub.load åŠ è½½å¤±è´¥: {e}")
                print(f"å°è¯•ä½¿ç”¨ transformers åŠ è½½æ¨¡å‹")
                
                # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ transformers åŠ è½½æ¨¡å‹
                rtdetr_orig = RTDetrForObjectDetection.from_pretrained("PekingU/rtdetr_r50vd_coco_o365").to(device)
                
                # ä½¿ç”¨ weights_only=False åŠ è½½æ¨¡å‹æƒé‡ï¼Œè§£å†³ PyTorch 2.6 ä¸­çš„é™åˆ¶
                try:
                    model_weights = torch.load(rtdetr_orig_path, map_location=device, weights_only=False)
                    rtdetr_orig.load_state_dict(model_weights)
                    print("âœ… ä½¿ç”¨ weights_only=False æˆåŠŸåŠ è½½æ¨¡å‹")
                except Exception as e2:
                    print(f"âš ï¸ weights_only=False åŠ è½½å¤±è´¥: {e2}")
                    # æœ€åå°è¯• context manager æ–¹æ³•
                    try:
                        with torch.serialization.safe_globals(['ultralytics.nn.tasks.DetectionModel']):
                            model_weights = torch.load(rtdetr_orig_path, map_location=device)
                        rtdetr_orig.load_state_dict(model_weights)
                        print("âœ… ä½¿ç”¨ safe_globals context manager æˆåŠŸåŠ è½½æ¨¡å‹")
                    except Exception as e3:
                        print(f"âŒ æ‰€æœ‰åŠ è½½æ–¹æ³•éƒ½å¤±è´¥: {e3}")
                        raise e3
                
                rtdetr_orig.eval()
                rtdetr_orig_processor = RTDetrImageProcessor.from_pretrained("PekingU/rtdetr_r50vd_coco_o365")
            
            print("âœ… åŸå§‹RTDETRæ¨¡å‹åŠ è½½æˆåŠŸ")
            has_rtdetr = True
        else:
            print(f"âš ï¸ æ‰¾ä¸åˆ°åŸå§‹RTDETRæ¨¡å‹æƒé‡æ–‡ä»¶: {rtdetr_orig_path}")
            print("âš ï¸ å°†ä»…è¯„ä¼°ViLD-RTDETRæ¨¡å‹")
            has_rtdetr = False
    except Exception as e:
        print(f"âŒ æ— æ³•åŠ è½½åŸå§‹RTDETRæ¨¡å‹: {e}")
        print("âš ï¸ å°†ä»…è¯„ä¼°ViLD-RTDETRæ¨¡å‹")
        has_rtdetr = False
    
    # ç¡®å®šè¾“å…¥æ˜¯å•å¼ å›¾åƒè¿˜æ˜¯ç›®å½•
    if os.path.isdir(image_path):
        print(f"ğŸ” æ‰¹é‡è¯„ä¼°å›¾åƒç›®å½•: {image_path}")
        image_files = [os.path.join(image_path, f) for f in os.listdir(image_path) 
                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
        print(f"âœ“ æ‰¾åˆ° {len(image_files)} ä¸ªå›¾åƒæ–‡ä»¶")
    else:
        print(f"ğŸ” è¯„ä¼°å•å¼ å›¾åƒ: {image_path}")
        image_files = [image_path]
    
    # å‡†å¤‡ç»“æœç»Ÿè®¡
    vild_results = []
    rtdetr_results = []
    vild_total_time = 0
    rtdetr_total_time = 0
    vild_category_counts = {}
    rtdetr_category_counts = {}
    comparison_metrics = {
        'total_images': len(image_files),
        'per_image_comparison': [],
        'category_comparison': {},
        'average_metrics': {
            'vild_avg_detections': 0,
            'rtdetr_avg_detections': 0,
            'vild_avg_time': 0,
            'rtdetr_avg_time': 0,
            'vild_fps': 0,
            'rtdetr_fps': 0,
            'detection_increase_percent': 0,
            'speed_difference_percent': 0
        }
    }
    
    # å›¾åƒè½¬æ¢å™¨
    transform = T.Compose([
        T.ToTensor(),
        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
    # å¤„ç†æ¯ä¸ªå›¾åƒ
    for img_path in tqdm(image_files, desc="å¤„ç†å›¾åƒ"):
        try:
            # åŠ è½½å›¾åƒ
            image = Image.open(img_path).convert('RGB')
            img_width, img_height = image.size
            
            # åˆ›å»ºæ¯”è¾ƒç»“æœå›¾åƒ
            comparison_image = Image.new('RGB', (img_width*2, img_height), (255, 255, 255))
            comparison_image.paste(image, (0, 0))
            comparison_image.paste(image, (img_width, 0))
            
            # ç»˜åˆ¶ä¸¤ä¸ªæ¨¡å‹çš„ç»“æœ
            draw = ImageDraw.Draw(comparison_image)
            
            # å°è¯•åŠ è½½å­—ä½“
            try:
                font = ImageFont.truetype("Arial.ttf", 15)
            except:
                font = ImageFont.load_default()
            
            # 1. åŸå§‹RTDETRæ¨¡å‹æ£€æµ‹
            rtdetr_image_detections = []
            if has_rtdetr:
                rtdetr_start_time = time.time()
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯ YOLOv5 æ¨¡å‹
                is_yolov5 = rtdetr_orig_processor is None
                
                if is_yolov5:
                    # YOLOv5 æ–¹å¼å¤„ç†
                    img_np = np.array(image)
                    
                    # æ¨ç†
                    with torch.no_grad():
                        results = rtdetr_orig(img_np)
                        
                    rtdetr_end_time = time.time()
                    rtdetr_process_time = rtdetr_end_time - rtdetr_start_time
                    rtdetr_total_time += rtdetr_process_time
                    
                    # å¤„ç† YOLOv5 ç»“æœ
                    boxes = []
                    scores = []
                    labels = []
                    
                    # æå– YOLOv5 ç»“æœ
                    if hasattr(results, 'xyxy'):
                        # YOLOv5 ç»“æœæ ¼å¼
                        yolo_results = results.xyxy[0].cpu().numpy()
                        for det in yolo_results:
                            x1, y1, x2, y2, conf, cls = det
                            if conf >= conf_threshold:
                                boxes.append([x1, y1, x2, y2])
                                scores.append(conf)
                                labels.append(int(cls))
                    elif hasattr(results, 'pandas') and callable(getattr(results, 'pandas')):
                        # å¦ä¸€ç§ YOLOv5 æ ¼å¼
                        df = results.pandas().xyxy[0]
                        for _, row in df.iterrows():
                            if row['confidence'] >= conf_threshold:
                                boxes.append([row['xmin'], row['ymin'], row['xmax'], row['ymax']])
                                scores.append(row['confidence'])
                                labels.append(row['class'])
                    else:
                        # ç›´æ¥å¤„ç† results
                        try:
                            for det in results:
                                for *xyxy, conf, cls in det:
                                    if conf >= conf_threshold:
                                        x1, y1, x2, y2 = [float(val) for val in xyxy]
                                        boxes.append([x1, y1, x2, y2])
                                        scores.append(float(conf))
                                        labels.append(int(cls))
                        except Exception as e:
                            print(f"âš ï¸ æ— æ³•è§£æ YOLOv5 ç»“æœ: {e}")
                    
                    # è½¬æ¢ä¸º numpy æ•°ç»„
                    if boxes:
                        boxes = np.array(boxes)
                        scores = np.array(scores)
                        labels = np.array(labels)
                    else:
                        boxes = np.array([])
                        scores = np.array([])
                        labels = np.array([])
                else:
                    # Transformers æ–¹å¼å¤„ç†
                    # ä½¿ç”¨ transformers çš„å¤„ç†å™¨é¢„å¤„ç†å›¾åƒ
                    inputs = rtdetr_orig_processor(images=image, return_tensors="pt").to(device)
                    
                    # æ¨ç†
                    with torch.no_grad():
                        outputs = rtdetr_orig(**inputs)
                    
                    rtdetr_end_time = time.time()
                    rtdetr_process_time = rtdetr_end_time - rtdetr_start_time
                    rtdetr_total_time += rtdetr_process_time
                    
                    # å¤„ç†æ£€æµ‹ç»“æœ - transformersæ ¼å¼
                    results = rtdetr_orig_processor.post_process_object_detection(outputs, threshold=conf_threshold)
                    boxes = results[0]['boxes'].cpu().numpy()
                    scores = results[0]['scores'].cpu().numpy()
                    labels = results[0]['labels'].cpu().numpy()
                
                # ç”±äºpost_process_object_detectionå·²ç»è¿‡æ»¤äº†ä½ç½®ä¿¡åº¦ç»“æœï¼Œè¿™é‡Œç›´æ¥ä½¿ç”¨
                valid_boxes = boxes
                valid_scores = scores
                valid_labels = labels
                
                # æ›´æ–°ç±»åˆ«è®¡æ•°
                for label_idx in valid_labels:
                    category_name = COCO_CLASSES[label_idx] if label_idx < len(COCO_CLASSES) else f"class_{label_idx}"
                    rtdetr_category_counts[category_name] = rtdetr_category_counts.get(category_name, 0) + 1
                
                # ç»˜åˆ¶æ£€æµ‹æ¡† - å³ä¾§å›¾åƒ
                for box, score, label_idx in zip(valid_boxes, valid_scores, valid_labels):
                    # éšæœºé¢œè‰²
                    color = tuple(np.random.randint(0, 255, 3).tolist())
                    
                    # è·å–ç±»åˆ«åç§°
                    category_name = COCO_CLASSES[label_idx] if label_idx < len(COCO_CLASSES) else f"class_{label_idx}"
                    
                    # ç»˜åˆ¶è¾¹ç•Œæ¡† - å‘å³åç§»img_width
                    x1, y1, x2, y2 = box
                    draw.rectangle([x1+img_width, y1, x2+img_width, y2], outline=color, width=2)
                    
                    # ç»˜åˆ¶æ ‡ç­¾
                    label = f"{category_name} {score:.2f}"
                    draw.rectangle([x1+img_width, y1, x1+img_width + len(label) * 8, y1 + 15], fill=color)
                    draw.text((x1+img_width, y1), label, fill="white", font=font)
                    
                    # ä¿å­˜æ£€æµ‹ç»“æœ
                    rtdetr_image_detections.append({
                        'bbox': box.tolist(),
                        'score': float(score),
                        'category': category_name
                    })
                
                # åœ¨å³ä¸Šè§’æ·»åŠ RTDETRä¿¡æ¯
                rtdetr_info = f"åŸå§‹RTDETR: {len(valid_boxes)}ä¸ªç‰©ä½“, {rtdetr_process_time:.3f}s ({1/rtdetr_process_time:.1f} FPS)"
                draw.rectangle([img_width, 0, img_width + len(rtdetr_info) * 8, 20], fill="blue")
                draw.text((img_width+5, 5), rtdetr_info, fill="white", font=font)
            
            # 2. ViLD-RTDETRæ¨¡å‹æ£€æµ‹
            vild_start_time = time.time()
            
            # åœºæ™¯åˆ†ç±»
            scene_type, scene_score = scene_classifier.classify_scene(image)
            
            # ç‰©ä½“æ£€æµ‹
            detection_result = vild_detector.detect_objects(
                image, 
                scene_type=scene_type,
                use_macro_categories=True  # ä½¿ç”¨å¤§ç±»åˆ«åˆ†ç»„
            )
            
            vild_end_time = time.time()
            vild_process_time = vild_end_time - vild_start_time
            vild_total_time += vild_process_time
            
            # å¤„ç†æ£€æµ‹ç»“æœ
            vild_image_detections = []
            if detection_result and 'boxes' in detection_result and len(detection_result['boxes']) > 0:
                boxes = detection_result['boxes']
                scores = detection_result['scores']
                categories = detection_result['labels']
                
                # è¿‡æ»¤ä½ç½®ä¿¡åº¦ç»“æœ
                valid_indices = [i for i, score in enumerate(scores) if score >= conf_threshold]
                valid_boxes = [boxes[i] for i in valid_indices]
                valid_scores = [scores[i] for i in valid_indices]
                valid_categories = [categories[i] for i in valid_indices]
                
                # æ›´æ–°ç±»åˆ«è®¡æ•°
                for category in valid_categories:
                    vild_category_counts[category] = vild_category_counts.get(category, 0) + 1
                
                # ç»˜åˆ¶æ£€æµ‹æ¡† - å·¦ä¾§å›¾åƒ
                for box, score, category in zip(valid_boxes, valid_scores, valid_categories):
                    # éšæœºé¢œè‰²
                    color = tuple(np.random.randint(0, 255, 3).tolist())
                    
                    # ç»˜åˆ¶è¾¹ç•Œæ¡†
                    if len(box) == 4:  # [x1, y1, x2, y2]
                        x1, y1, x2, y2 = box
                    else:  # [x, y, w, h]
                        x1, y1, w, h = box
                        x2, y2 = x1 + w, y1 + h
                        
                    draw.rectangle([x1, y1, x2, y2], outline=color, width=2)
                    
                    # ç»˜åˆ¶æ ‡ç­¾
                    label = f"{category} {score:.2f}"
                    draw.rectangle([x1, y1, x1 + len(label) * 8, y1 + 15], fill=color)
                    draw.text((x1, y1), label, fill="white", font=font)
                    
                    # ä¿å­˜æ£€æµ‹ç»“æœ
                    vild_image_detections.append({
                        'bbox': box.tolist() if isinstance(box, np.ndarray) else box,
                        'score': float(score),
                        'category': category
                    })
            
            # åœ¨å·¦ä¸Šè§’æ·»åŠ ViLDä¿¡æ¯
            vild_info = f"ViLD-RTDETR: {len(vild_image_detections)}ä¸ªç‰©ä½“, {vild_process_time:.3f}s ({1/vild_process_time:.1f} FPS)"
            scene_info = f"åœºæ™¯: {scene_type} ({scene_score:.2f})"
            draw.rectangle([0, 0, len(vild_info) * 8, 40], fill="green")
            draw.text((5, 5), vild_info, fill="white", font=font)
            draw.text((5, 22), scene_info, fill="white", font=font)
            
            # ä¿å­˜æ¯”è¾ƒå›¾åƒ
            output_file = os.path.join(output_dir, f"compare_{os.path.basename(img_path)}")
            comparison_image.save(output_file)
            
            # è®¡ç®—å›¾åƒçº§åˆ«çš„æ¯”è¾ƒæŒ‡æ ‡
            rtdetr_detections_count = len(rtdetr_image_detections) if has_rtdetr else 0
            vild_detections_count = len(vild_image_detections)
            
            detection_increase = vild_detections_count - rtdetr_detections_count
            detection_increase_percent = ((vild_detections_count / rtdetr_detections_count) - 1) * 100 if rtdetr_detections_count > 0 else float('inf')
            
            speed_difference = rtdetr_process_time - vild_process_time if has_rtdetr else 0
            speed_difference_percent = ((rtdetr_process_time / vild_process_time) - 1) * 100 if has_rtdetr and vild_process_time > 0 else float('inf')
            
            # æ·»åŠ åˆ°æ¯å¼ å›¾åƒçš„æ¯”è¾ƒç»“æœ
            comparison_metrics['per_image_comparison'].append({
                'image_path': img_path,
                'vild_detections': vild_detections_count,
                'rtdetr_detections': rtdetr_detections_count,
                'detection_increase': detection_increase,
                'detection_increase_percent': detection_increase_percent if detection_increase_percent != float('inf') else 'N/A',
                'vild_process_time': vild_process_time,
                'rtdetr_process_time': rtdetr_process_time if has_rtdetr else 'N/A',
                'vild_fps': 1/vild_process_time if vild_process_time > 0 else 0,
                'rtdetr_fps': 1/rtdetr_process_time if has_rtdetr and rtdetr_process_time > 0 else 'N/A',
                'speed_difference': speed_difference,
                'speed_difference_percent': speed_difference_percent if speed_difference_percent != float('inf') else 'N/A',
            })
            
            # æ·»åŠ åˆ°æ€»ä½“ç»“æœ
            vild_results.append({
                'image_path': img_path,
                'scene_type': scene_type,
                'scene_score': float(scene_score),
                'process_time': vild_process_time,
                'fps': 1/vild_process_time if vild_process_time > 0 else 0,
                'detections': vild_image_detections
            })
            
            if has_rtdetr:
                rtdetr_results.append({
                    'image_path': img_path,
                    'process_time': rtdetr_process_time,
                    'fps': 1/rtdetr_process_time if rtdetr_process_time > 0 else 0,
                    'detections': rtdetr_image_detections
                })
            
            # è¾“å‡ºæ¯å¼ å›¾åƒçš„æ¯”è¾ƒç»“æœ
            print(f"\n--- å›¾åƒ: {os.path.basename(img_path)} ---")
            print(f"ViLD-RTDETR: {vild_detections_count}ä¸ªç‰©ä½“, {vild_process_time:.3f}s ({1/vild_process_time:.1f} FPS)")
            if has_rtdetr:
                print(f"åŸå§‹RTDETR: {rtdetr_detections_count}ä¸ªç‰©ä½“, {rtdetr_process_time:.3f}s ({1/rtdetr_process_time:.1f} FPS)")
                print(f"æ£€æµ‹å¢é‡: {detection_increase}ä¸ªç‰©ä½“ ({detection_increase_percent:.1f}%)")
                print(f"é€Ÿåº¦å·®å¼‚: {speed_difference:.3f}s ({speed_difference_percent:.1f}%)")
        
        except Exception as e:
            print(f"âŒ å¤„ç†å›¾åƒ {os.path.basename(img_path)} å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    # è®¡ç®—æ€»ä½“ç»Ÿè®¡ä¿¡æ¯
    vild_avg_time = vild_total_time / len(image_files) if image_files else 0
    vild_avg_fps = 1.0 / vild_avg_time if vild_avg_time > 0 else 0
    vild_total_detections = sum(len(r['detections']) for r in vild_results)
    vild_avg_detections = vild_total_detections / len(vild_results) if vild_results else 0
    
    comparison_metrics['average_metrics']['vild_avg_detections'] = vild_avg_detections
    comparison_metrics['average_metrics']['vild_avg_time'] = vild_avg_time
    comparison_metrics['average_metrics']['vild_fps'] = vild_avg_fps
    
    if has_rtdetr:
        rtdetr_avg_time = rtdetr_total_time / len(image_files) if image_files else 0
        rtdetr_avg_fps = 1.0 / rtdetr_avg_time if rtdetr_avg_time > 0 else 0
        rtdetr_total_detections = sum(len(r['detections']) for r in rtdetr_results)
        rtdetr_avg_detections = rtdetr_total_detections / len(rtdetr_results) if rtdetr_results else 0
        
        comparison_metrics['average_metrics']['rtdetr_avg_detections'] = rtdetr_avg_detections
        comparison_metrics['average_metrics']['rtdetr_avg_time'] = rtdetr_avg_time
        comparison_metrics['average_metrics']['rtdetr_fps'] = rtdetr_avg_fps
        
        # è®¡ç®—å¹³å‡æå‡
        avg_detection_increase_percent = ((vild_avg_detections / rtdetr_avg_detections) - 1) * 100 if rtdetr_avg_detections > 0 else float('inf')
        avg_speed_difference_percent = ((rtdetr_avg_time / vild_avg_time) - 1) * 100 if vild_avg_time > 0 else float('inf')
        
        comparison_metrics['average_metrics']['detection_increase_percent'] = avg_detection_increase_percent if avg_detection_increase_percent != float('inf') else 'N/A'
        comparison_metrics['average_metrics']['speed_difference_percent'] = avg_speed_difference_percent if avg_speed_difference_percent != float('inf') else 'N/A'
    
    # è®¡ç®—ç±»åˆ«å¯¹æ¯”
    all_categories = set(vild_category_counts.keys()) | set(rtdetr_category_counts.keys())
    for category in all_categories:
        vild_count = vild_category_counts.get(category, 0)
        rtdetr_count = rtdetr_category_counts.get(category, 0)
        
        # è®¡ç®—ç±»åˆ«æ£€æµ‹å¢é‡
        if rtdetr_count > 0:
            category_increase_percent = ((vild_count / rtdetr_count) - 1) * 100
        elif vild_count > 0:
            category_increase_percent = float('inf')  # ViLDæ£€æµ‹åˆ°äº†ï¼ŒRTDETRæ²¡æœ‰æ£€æµ‹åˆ°
        else:
            category_increase_percent = 0  # ä¸¤è€…éƒ½æ²¡æœ‰æ£€æµ‹åˆ°
            
        comparison_metrics['category_comparison'][category] = {
            'vild_count': vild_count,
            'rtdetr_count': rtdetr_count,
            'increase': vild_count - rtdetr_count,
            'increase_percent': category_increase_percent if category_increase_percent != float('inf') else 'N/A'
        }
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    vild_results_file = os.path.join(output_dir, 'vild_detection_results.json')
    with open(vild_results_file, 'w') as f:
        json.dump(vild_results, f, indent=2)
    
    if has_rtdetr:
        rtdetr_results_file = os.path.join(output_dir, 'rtdetr_detection_results.json')
        with open(rtdetr_results_file, 'w') as f:
            json.dump(rtdetr_results, f, indent=2)
    
    # ä¿å­˜æ¯”è¾ƒæŒ‡æ ‡
    comparison_file = os.path.join(output_dir, 'comparison_metrics.json')
    with open(comparison_file, 'w') as f:
        json.dump(comparison_metrics, f, indent=2)
    
    # ç»˜åˆ¶æ£€æµ‹æ•°é‡å¯¹æ¯”å›¾
    plt.figure(figsize=(12, 6))
    
    # æå–æ¯å¼ å›¾åƒçš„æ£€æµ‹æ•°é‡
    image_names = [os.path.basename(img_path) for img_path in image_files]
    vild_counts = [len(r['detections']) for r in vild_results]
    
    if has_rtdetr:
        rtdetr_counts = [len(r['detections']) for r in rtdetr_results]
        
        # ç»˜åˆ¶åŒæŸ±çŠ¶å›¾
        x = np.arange(len(image_names))
        width = 0.35
        
        plt.bar(x - width/2, vild_counts, width, label='ViLD-RTDETR')
        plt.bar(x + width/2, rtdetr_counts, width, label='åŸå§‹RTDETR')
        
        plt.xlabel('å›¾åƒ')
        plt.ylabel('æ£€æµ‹ç‰©ä½“æ•°é‡')
        plt.title('ViLD-RTDETR vs åŸå§‹RTDETR æ£€æµ‹æ•°é‡å¯¹æ¯”')
        plt.xticks(x, [name[:10] + '...' if len(name) > 10 else name for name in image_names], rotation=45)
        plt.legend()
        
        # ä¿å­˜å›¾è¡¨
        plt.tight_layout()
        detection_chart_file = os.path.join(output_dir, 'detection_count_comparison.png')
        plt.savefig(detection_chart_file)
        
        # ç»˜åˆ¶FPSå¯¹æ¯”å›¾
        plt.figure(figsize=(12, 6))
        
        vild_fps = [1/r['process_time'] if r['process_time'] > 0 else 0 for r in vild_results]
        rtdetr_fps = [1/r['process_time'] if r['process_time'] > 0 else 0 for r in rtdetr_results]
        
        plt.bar(x - width/2, vild_fps, width, label='ViLD-RTDETR')
        plt.bar(x + width/2, rtdetr_fps, width, label='åŸå§‹RTDETR')
        
        plt.xlabel('å›¾åƒ')
        plt.ylabel('FPS (å¸§æ¯ç§’)')
        plt.title('ViLD-RTDETR vs åŸå§‹RTDETR é€Ÿåº¦å¯¹æ¯”')
        plt.xticks(x, [name[:10] + '...' if len(name) > 10 else name for name in image_names], rotation=45)
        plt.legend()
        
        # ä¿å­˜å›¾è¡¨
        plt.tight_layout()
        fps_chart_file = os.path.join(output_dir, 'fps_comparison.png')
        plt.savefig(fps_chart_file)
        
        # ç»˜åˆ¶ç±»åˆ«æ£€æµ‹æ•°é‡å¯¹æ¯”å›¾ (ä»…æ˜¾ç¤ºå‰10ä¸ªç±»åˆ«)
        plt.figure(figsize=(14, 8))
        
        # æŒ‰ViLDæ£€æµ‹æ•°é‡æ’åº
        top_categories = sorted(vild_category_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        category_names = [cat for cat, _ in top_categories]
        
        vild_cat_counts = [vild_category_counts.get(cat, 0) for cat in category_names]
        rtdetr_cat_counts = [rtdetr_category_counts.get(cat, 0) for cat in category_names]
        
        x = np.arange(len(category_names))
        
        plt.bar(x - width/2, vild_cat_counts, width, label='ViLD-RTDETR')
        plt.bar(x + width/2, rtdetr_cat_counts, width, label='åŸå§‹RTDETR')
        
        plt.xlabel('ç‰©ä½“ç±»åˆ«')
        plt.ylabel('æ£€æµ‹æ•°é‡')
        plt.title('ViLD-RTDETR vs åŸå§‹RTDETR ç±»åˆ«æ£€æµ‹æ•°é‡å¯¹æ¯” (å‰10ç±»)')
        plt.xticks(x, category_names, rotation=45)
        plt.legend()
        
        # ä¿å­˜å›¾è¡¨
        plt.tight_layout()
        category_chart_file = os.path.join(output_dir, 'category_comparison.png')
        plt.savefig(category_chart_file)
    
    # æ‰“å°æ€»ä½“æ¯”è¾ƒç»“æœ
    print("\n===== æ¨¡å‹æ€§èƒ½å¯¹æ¯”æ‘˜è¦ =====")
    print(f"æ€»å›¾åƒæ•°: {len(image_files)}")
    print("\nViLD-RTDETRæ¨¡å‹:")
    print(f"- æ€»æ£€æµ‹ç‰©ä½“æ•°: {vild_total_detections}")
    print(f"- å¹³å‡æ¯å›¾åƒæ£€æµ‹ç‰©ä½“æ•°: {vild_avg_detections:.2f}")
    print(f"- å¹³å‡å¤„ç†æ—¶é—´: {vild_avg_time:.3f}ç§’/å›¾åƒ")
    print(f"- å¹³å‡å¸§ç‡: {vild_avg_fps:.2f} FPS")
    
    if has_rtdetr:
        print("\nåŸå§‹RTDETRæ¨¡å‹:")
        print(f"- æ€»æ£€æµ‹ç‰©ä½“æ•°: {rtdetr_total_detections}")
        print(f"- å¹³å‡æ¯å›¾åƒæ£€æµ‹ç‰©ä½“æ•°: {rtdetr_avg_detections:.2f}")
        print(f"- å¹³å‡å¤„ç†æ—¶é—´: {rtdetr_avg_time:.3f}ç§’/å›¾åƒ")
        print(f"- å¹³å‡å¸§ç‡: {rtdetr_avg_fps:.2f} FPS")
        
        # è®¡ç®—æ€§èƒ½æå‡
        detection_improvement = ((vild_avg_detections / rtdetr_avg_detections) - 1) * 100 if rtdetr_avg_detections > 0 else float('inf')
        speed_improvement = ((rtdetr_avg_time / vild_avg_time) - 1) * 100 if vild_avg_time > 0 else float('inf')
        
        print("\næ€§èƒ½æå‡:")
        print(f"- æ£€æµ‹èƒ½åŠ›æå‡: {vild_avg_detections - rtdetr_avg_detections:.2f}ä¸ªç‰©ä½“/å›¾åƒ ({detection_improvement:.1f}%)")
        
        if speed_improvement > 0:
            print(f"- é€Ÿåº¦æå‡: {rtdetr_avg_time - vild_avg_time:.3f}ç§’/å›¾åƒ ({speed_improvement:.1f}%)")
        else:
            print(f"- é€Ÿåº¦å˜åŒ–: {vild_avg_time - rtdetr_avg_time:.3f}ç§’/å›¾åƒ ({-speed_improvement:.1f}%)")
        
        # æ‰“å°ç±»åˆ«å·®å¼‚
        vild_only = set(vild_category_counts.keys()) - set(rtdetr_category_counts.keys())
        rtdetr_only = set(rtdetr_category_counts.keys()) - set(vild_category_counts.keys())
        
        print("\nç±»åˆ«æ£€æµ‹å·®å¼‚:")
        print(f"- ä»…ViLD-RTDETRæ£€æµ‹åˆ°çš„ç±»åˆ«: {', '.join(vild_only) if vild_only else 'æ— '}")
        print(f"- ä»…åŸå§‹RTDETRæ£€æµ‹åˆ°çš„ç±»åˆ«: {', '.join(rtdetr_only) if rtdetr_only else 'æ— '}")
    
    print(f"\nâœ… è¯¦ç»†æ¯”è¾ƒç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
    return comparison_metrics

def main():
    """ä¸»å‡½æ•°"""
    # ç›´æ¥åœ¨ä»£ç ä¸­æŒ‡å®šå›¾åƒè·¯å¾„/ç›®å½•ï¼Œä¸éœ€è¦å‘½ä»¤è¡Œå‚æ•°
    
    # === åœ¨è¿™é‡Œè®¾ç½®è¯„ä¼°å‚æ•° ===
    # å›¾åƒè·¯å¾„ï¼ˆå•å¼ å›¾åƒæˆ–å›¾åƒç›®å½•ï¼‰
    image_path = "datasets/indoor_inference/images/indoor_000010.jpg"
    output_dir = os.path.join(project_root, "results/model_comparison")
    # æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼
    conf_threshold = 0.35
    # ========================
    
    if not os.path.exists(image_path):
        print(f"âŒ é”™è¯¯: è·¯å¾„ä¸å­˜åœ¨: {image_path}")
        return
    
    print(f"ğŸ” å¼€å§‹æ¯”è¾ƒæ¨¡å‹æ€§èƒ½: {image_path}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ”¢ ç½®ä¿¡åº¦é˜ˆå€¼: {conf_threshold}")
    
    try:
        compare_models(image_path, output_dir, conf_threshold)
    except Exception as e:
        print(f"âŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
