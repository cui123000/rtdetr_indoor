#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ViLD-RTDETR æ¨¡å‹æ€§èƒ½è¯„ä¼°è„šæœ¬
æ­¤è„šæœ¬ç”¨äºè¯„ä¼°æ¨¡å‹åœ¨å®¤å†…åœºæ™¯æ£€æµ‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½
"""

import os
import sys
import time
import numpy as np
import cv2
import torch
from PIL import Image, ImageDraw, ImageFont
from tqdm import tqdm
import matplotlib.pyplot as plt
import json

# è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
current_dir = os.path.dirname(os.path.abspath(__file__))
# è·å–é¡¹ç›®æ ¹ç›®å½•
project_root = os.path.dirname(os.path.dirname(current_dir))
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°æ¨¡å—æœç´¢è·¯å¾„
sys.path.append(project_root)

def evaluate_model(image_path, output_dir=None, conf_threshold=0.35):
    """è¯„ä¼°æ¨¡å‹åœ¨å•å¼ å›¾åƒæˆ–å›¾åƒç›®å½•ä¸Šçš„æ€§èƒ½"""
    from src.vild.vild_modular.config import MODEL_CONFIG, INFERENCE_CONFIG
    from src.vild.vild_modular.detector import FixedViLDDetector
    from src.vild.vild_modular.scene_classifier import SceneClassifier
    from src.vild.vild_modular.model import load_models
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    if output_dir is None:
        output_dir = os.path.join(current_dir, "evaluation_results")
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"ğŸ“ ç»“æœå°†ä¿å­˜åœ¨: {output_dir}")
    
    # ç¡®å®šè®¾å¤‡
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ¨¡å‹
    print("ğŸ“¦ æ­£åœ¨åŠ è½½æ¨¡å‹...")
    rtdetr_model, clip_model, image_processor, clip_preprocess = load_models(
        rtdetr_path=MODEL_CONFIG['rtdetr_model_path'],
        clip_name=MODEL_CONFIG['clip_model_name'],
        device=device
    )
    
    # åˆ›å»ºåœºæ™¯åˆ†ç±»å™¨
    scene_classifier = SceneClassifier(
        clip_model=clip_model,
        clip_preprocess=clip_preprocess,
        device=device
    )
    
    # åˆ›å»ºæ£€æµ‹å™¨
    detector = FixedViLDDetector(
        clip_model=clip_model,
        detector_model=rtdetr_model,
        image_processor=image_processor,
        clip_preprocess=clip_preprocess,
        device=device,
        projector_path=MODEL_CONFIG['projector_path'],
        config={'model': MODEL_CONFIG, 'inference': INFERENCE_CONFIG}
    )
    
    # ç¡®å®šè¾“å…¥æ˜¯å•å¼ å›¾åƒè¿˜æ˜¯ç›®å½•
    if os.path.isdir(image_path):
        print(f"ğŸ” æ‰¹é‡è¯„ä¼°å›¾åƒç›®å½•: {image_path}")
        image_files = [os.path.join(image_path, f) for f in os.listdir(image_path) 
                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
        print(f"âœ“ æ‰¾åˆ° {len(image_files)} ä¸ªå›¾åƒæ–‡ä»¶")
    else:
        print(f"ğŸ” è¯„ä¼°å•å¼ å›¾åƒ: {image_path}")
        image_files = [image_path]
    
    # å‡†å¤‡ç»“æœç»Ÿè®¡
    results = []
    total_time = 0
    category_counts = {}
    
    # å¤„ç†æ¯ä¸ªå›¾åƒ
    for img_path in tqdm(image_files, desc="å¤„ç†å›¾åƒ"):
        try:
            # åŠ è½½å›¾åƒ
            image = Image.open(img_path).convert('RGB')
            
            # è®°å½•å¼€å§‹æ—¶é—´
            start_time = time.time()
            
            # åœºæ™¯åˆ†ç±»
            scene_type, scene_score = scene_classifier.classify_scene(image)
            
            # ç‰©ä½“æ£€æµ‹
            detection_result = detector.detect_objects(
                image, 
                scene_type=scene_type,
                use_macro_categories=True  # ä½¿ç”¨å¤§ç±»åˆ«åˆ†ç»„
            )
            
            # è®°å½•ç»“æŸæ—¶é—´
            end_time = time.time()
            process_time = end_time - start_time
            total_time += process_time
            
            # è·å–æ£€æµ‹ç»“æœ
            if detection_result and 'boxes' in detection_result and len(detection_result['boxes']) > 0:
                boxes = detection_result['boxes']
                scores = detection_result['scores']
                categories = detection_result['labels']
                
                # è¿‡æ»¤ä½ç½®ä¿¡åº¦ç»“æœ
                valid_indices = [i for i, score in enumerate(scores) if score >= conf_threshold]
                valid_boxes = [boxes[i] for i in valid_indices]
                valid_scores = [scores[i] for i in valid_indices]
                valid_categories = [categories[i] for i in valid_indices]
                
                # æ›´æ–°ç±»åˆ«è®¡æ•°
                for category in valid_categories:
                    category_counts[category] = category_counts.get(category, 0) + 1
                
                # å¯è§†åŒ–ç»“æœ
                draw = ImageDraw.Draw(image)
                
                # å°è¯•åŠ è½½å­—ä½“
                try:
                    font = ImageFont.truetype("Arial.ttf", 15)
                except:
                    font = ImageFont.load_default()
                
                # ç»˜åˆ¶æ£€æµ‹æ¡†
                for box, score, category in zip(valid_boxes, valid_scores, valid_categories):
                    # éšæœºé¢œè‰²
                    color = tuple(np.random.randint(0, 255, 3).tolist())
                    
                    # ç»˜åˆ¶è¾¹ç•Œæ¡†
                    if len(box) == 4:  # [x1, y1, x2, y2]
                        x1, y1, x2, y2 = box
                    else:  # [x, y, w, h]
                        x1, y1, w, h = box
                        x2, y2 = x1 + w, y1 + h
                        
                    draw.rectangle([x1, y1, x2, y2], outline=color, width=2)
                    
                    # ç»˜åˆ¶æ ‡ç­¾
                    label = f"{category} {score:.2f}"
                    draw.rectangle([x1, y1, x1 + len(label) * 8, y1 + 15], fill=color)
                    draw.text((x1, y1), label, fill="white", font=font)
                
                # åœ¨å›¾åƒé¡¶éƒ¨æ·»åŠ åœºæ™¯ä¿¡æ¯
                scene_info = f"åœºæ™¯: {scene_type} ({scene_score:.2f}) - å¤„ç†æ—¶é—´: {process_time:.3f}s"
                draw.rectangle([0, 0, len(scene_info) * 8, 20], fill="black")
                draw.text((5, 5), scene_info, fill="white", font=font)
                
                # ä¿å­˜ç»“æœå›¾åƒ
                output_file = os.path.join(output_dir, os.path.basename(img_path))
                image.save(output_file)
                
                # æ”¶é›†ç»“æœä¿¡æ¯
                results.append({
                    'image_path': img_path,
                    'scene_type': scene_type,
                    'scene_score': float(scene_score),
                    'process_time': process_time,
                    'detections': [
                        {
                            'bbox': box.tolist() if isinstance(box, np.ndarray) else box,
                            'score': float(score),
                            'category': cat
                        }
                        for box, score, cat in zip(valid_boxes, valid_scores, valid_categories)
                    ]
                })
                
                print(f"âœ“ å¤„ç†å›¾åƒ {os.path.basename(img_path)}: åœºæ™¯={scene_type}, "
                      f"æ£€æµ‹åˆ° {len(valid_boxes)} ä¸ªç‰©ä½“, ç”¨æ—¶ {process_time:.3f}s")
            else:
                print(f"âš ï¸ å›¾åƒ {os.path.basename(img_path)} æœªæ£€æµ‹åˆ°ç‰©ä½“")
        
        except Exception as e:
            print(f"âŒ å¤„ç†å›¾åƒ {os.path.basename(img_path)} å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    # è®¡ç®—æ€»ä½“ç»Ÿè®¡ä¿¡æ¯
    avg_time = total_time / len(image_files) if image_files else 0
    avg_fps = 1.0 / avg_time if avg_time > 0 else 0
    total_detections = sum(len(r['detections']) for r in results)
    avg_detections = total_detections / len(results) if results else 0
    
    # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    stats = {
        'total_images': len(image_files),
        'total_detections': total_detections,
        'avg_detections_per_image': avg_detections,
        'total_processing_time': total_time,
        'avg_processing_time': avg_time,
        'avg_fps': avg_fps,
        'category_statistics': {
            cat: count for cat, count in sorted(
                category_counts.items(), 
                key=lambda x: x[1], 
                reverse=True
            )
        }
    }
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    results_file = os.path.join(output_dir, 'detection_results.json')
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    stats_file = os.path.join(output_dir, 'detection_stats.json')
    with open(stats_file, 'w') as f:
        json.dump(stats, f, indent=2)
    
    # ç»˜åˆ¶ç±»åˆ«åˆ†å¸ƒé¥¼å›¾
    if category_counts:
        plt.figure(figsize=(10, 8))
        
        # é™åˆ¶æ˜¾ç¤ºå‰10ä¸ªç±»åˆ«ï¼Œå…¶ä½™å½’ä¸º"å…¶ä»–"
        top_categories = sorted(category_counts.items(), key=lambda x: x[1], reverse=True)
        if len(top_categories) > 10:
            top_10 = top_categories[:10]
            others_count = sum(count for _, count in top_categories[10:])
            chart_data = dict(top_10)
            chart_data['å…¶ä»–'] = others_count
        else:
            chart_data = dict(top_categories)
        
        # ç»˜åˆ¶é¥¼å›¾
        plt.pie(
            chart_data.values(), 
            labels=chart_data.keys(), 
            autopct='%1.1f%%',
            shadow=True, 
            startangle=140
        )
        plt.axis('equal')
        plt.title('æ£€æµ‹ç‰©ä½“ç±»åˆ«åˆ†å¸ƒ')
        
        # ä¿å­˜å›¾è¡¨
        chart_file = os.path.join(output_dir, 'category_distribution.png')
        plt.savefig(chart_file)
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("\n===== è¯„ä¼°ç»“æœæ‘˜è¦ =====")
    print(f"æ€»å›¾åƒæ•°: {stats['total_images']}")
    print(f"æ€»æ£€æµ‹ç‰©ä½“æ•°: {stats['total_detections']}")
    print(f"å¹³å‡æ¯å›¾åƒæ£€æµ‹ç‰©ä½“æ•°: {stats['avg_detections_per_image']:.2f}")
    print(f"æ€»å¤„ç†æ—¶é—´: {stats['total_processing_time']:.2f}ç§’")
    print(f"å¹³å‡å¤„ç†æ—¶é—´: {stats['avg_processing_time']:.3f}ç§’/å›¾åƒ")
    print(f"å¹³å‡å¸§ç‡: {stats['avg_fps']:.2f} FPS")
    
    print("\nç‰©ä½“ç±»åˆ«ç»Ÿè®¡ (å‰10):")
    for i, (cat, count) in enumerate(sorted(category_counts.items(), key=lambda x: x[1], reverse=True)[:10]):
        print(f"{i+1}. {cat}: {count} ä¸ª")
    
    print(f"\nâœ… è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
    return stats

def main():
    """ä¸»å‡½æ•°"""
    # ç›´æ¥åœ¨ä»£ç ä¸­æŒ‡å®šå›¾åƒè·¯å¾„/ç›®å½•ï¼Œä¸éœ€è¦å‘½ä»¤è¡Œå‚æ•°
    
    # === åœ¨è¿™é‡Œè®¾ç½®è¯„ä¼°å‚æ•° ===
    # å›¾åƒè·¯å¾„ï¼ˆå•å¼ å›¾åƒæˆ–å›¾åƒç›®å½•ï¼‰
    image_path = "datasets/indoor_inference/images"
    # è¾“å‡ºç›®å½•è·¯å¾„
    output_dir = "results/model_evaluation"
    # æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼
    conf_threshold = 0.35
    # ========================
    
    if not os.path.exists(image_path):
        print(f"âŒ é”™è¯¯: è·¯å¾„ä¸å­˜åœ¨: {image_path}")
        return
    
    print(f"ğŸ” å¼€å§‹è¯„ä¼°å›¾åƒ: {image_path}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ”¢ ç½®ä¿¡åº¦é˜ˆå€¼: {conf_threshold}")
    
    try:
        evaluate_model(image_path, output_dir, conf_threshold)
    except Exception as e:
        print(f"âŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
