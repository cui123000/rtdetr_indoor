# -*- coding: utf-8 -*-
"""
åŸºäºViLDçš„å¼€æ”¾ä¸–ç•Œå®¤å†…ç‰©ä½“æ£€æµ‹ - è®­ç»ƒæ¨¡å—
"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import time
import gc
import json
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader
from tqdm import tqdm
import clip  
from torch.amp import autocast, GradScaler  # æ›´æ–°ä¸ºæ¨èçš„å¯¼å…¥æ–¹å¼

from data_loader import ImprovedCOCOIndoorDataset, collate_fn
from config import TRAINING_CONFIG

class LossTracker:
    """æŸå¤±è¿½è¸ªå’Œå¯è§†åŒ–"""
    
    def __init__(self):
        self.train_losses = []
        self.epoch_losses = []
        self.best_loss = float('inf')
        self.best_epoch = 0
        
    def update(self, epoch_loss, epoch):
        """æ›´æ–°æŸå¤±è®°å½•"""
        self.epoch_losses.append(epoch_loss)
        if epoch_loss < self.best_loss:
            self.best_loss = epoch_loss
            self.best_epoch = epoch
            
    def plot_losses(self, save_path=None, train_losses=None, val_losses=None, lr_history=None):
        """ç»˜åˆ¶æŸå¤±æ›²çº¿"""
        plt.figure(figsize=(15, 10))
        
        # åˆ›å»ºå¤šå­å›¾
        gs = plt.GridSpec(2, 2, height_ratios=[2, 1])
        ax1 = plt.subplot(gs[0, :])
        ax2 = plt.subplot(gs[1, 0])
        ax3 = plt.subplot(gs[1, 1])
        
        # ä¸»æŸå¤±æ›²çº¿
        epochs = range(1, len(self.epoch_losses) + 1)
        ax1.plot(epochs, self.epoch_losses, 'b-', linewidth=2.5, label='Validation Loss', marker='o')
        
        # æ ‡æ³¨æœ€ä½³æŸå¤±ç‚¹
        ax1.plot(self.best_epoch + 1, self.best_loss, 'r*', markersize=20, 
                label=f'Best Loss: {self.best_loss:.4f} (Epoch {self.best_epoch + 1})')
        
        # ç§»åŠ¨å¹³å‡çº¿
        if len(self.epoch_losses) >= 3:
            window_size = min(3, len(self.epoch_losses))
            moving_avg = []
            for i in range(len(self.epoch_losses)):
                start_idx = max(0, i - window_size + 1)
                moving_avg.append(np.mean(self.epoch_losses[start_idx:i+1]))
            ax1.plot(epochs, moving_avg, 'g--', linewidth=2, alpha=0.7, label='Moving Average')
        
        # è®¾ç½®æ ·å¼
        ax1.set_xlabel('Epoch', fontsize=14)
        ax1.set_ylabel('Loss Value', fontsize=14)
        ax1.set_title('Validation Loss Curve', fontsize=16, fontweight='bold')
        ax1.legend(fontsize=12, loc='upper right')
        ax1.grid(True, alpha=0.3)
        
        # è®­ç»ƒ/éªŒè¯æŸå¤±å¯¹æ¯”
        if train_losses and val_losses and len(train_losses) == len(val_losses):
            train_epochs = range(1, len(train_losses) + 1)
            ax2.plot(train_epochs, train_losses, 'b-', linewidth=2, label='Training')
            ax2.plot(train_epochs, val_losses, 'r-', linewidth=2, label='Validation')
            ax2.set_title('Training vs Validation Loss', fontsize=12)
            ax2.set_xlabel('Epoch', fontsize=10)
            ax2.set_ylabel('Loss', fontsize=10)
            ax2.legend(fontsize=10)
            ax2.grid(True, alpha=0.3)
        else:
            ax2.text(0.5, 0.5, "è®­ç»ƒ/éªŒè¯æŸå¤±æ•°æ®ä¸å¯ç”¨", 
                    ha='center', va='center', transform=ax2.transAxes)
        
        # å­¦ä¹ ç‡æ›²çº¿
        if lr_history and len(lr_history) > 0:
            lr_epochs = range(1, len(lr_history) + 1)
            ax3.plot(lr_epochs, lr_history, 'g-', linewidth=2)
            ax3.set_title('Learning Rate Schedule', fontsize=12)
            ax3.set_xlabel('Epoch', fontsize=10)
            ax3.set_ylabel('Learning Rate', fontsize=10)
            ax3.grid(True, alpha=0.3)
            ax3.yaxis.set_major_formatter(plt.FormatStrFormatter('%.0e'))
        else:
            ax3.text(0.5, 0.5, "å­¦ä¹ ç‡æ•°æ®ä¸å¯ç”¨", 
                    ha='center', va='center', transform=ax3.transAxes)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š Loss plot saved to: {save_path}")
        
        # é¿å…æ˜¾ç¤º
        try:
            plt.close()
        except:
            pass

class StableTrainer:
    """ç¨³å®šè®­ç»ƒå™¨"""
    
    def __init__(self, clip_model, device):
        self.clip_model = clip_model
        self.device = device
        
        # åˆ›å»ºæŠ•å½±å™¨
        self.visual_projector = self.create_projector().to(device)
        self.text_projector = self.create_projector().to(device)
        
        # ä½¿ç”¨æ’ç­‰æ˜ å°„åˆå§‹åŒ–
        self.initialize_as_identity()
        
        # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
        self.visual_projector.train()
        self.text_projector.train()
        
        print("ğŸ¯ è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def create_projector(self):
        """åˆ›å»ºç®€åŒ–ç‰ˆæŠ•å½±å™¨"""
        projector = nn.Sequential(
            nn.Linear(512, 512, dtype=torch.float32),
            nn.GELU(),
            nn.Linear(512, 512, dtype=torch.float32)
        )
        
        # ç¡®ä¿ä½¿ç”¨float32
        for param in projector.parameters():
            param.data = param.data.float()
            
        return projector
    
    def initialize_as_identity(self):
        """åˆå§‹åŒ–æŠ•å½±å™¨ä¸ºæ’ç­‰æ˜ å°„"""
        with torch.no_grad():
            # ç¬¬ä¸€å±‚
            torch.nn.init.eye_(self.visual_projector[0].weight)
            if self.visual_projector[0].bias is not None:
                torch.nn.init.zeros_(self.visual_projector[0].bias)
            
            torch.nn.init.eye_(self.text_projector[0].weight)
            if self.text_projector[0].bias is not None:
                torch.nn.init.zeros_(self.text_projector[0].bias)
            
            # æœ€åä¸€å±‚
            torch.nn.init.eye_(self.visual_projector[2].weight)
            if self.visual_projector[2].bias is not None:
                torch.nn.init.zeros_(self.visual_projector[2].bias)
                
            torch.nn.init.eye_(self.text_projector[2].weight)
            if self.text_projector[2].bias is not None:
                torch.nn.init.zeros_(self.text_projector[2].bias)
    
    def get_trainable_parameters(self):
        """è·å–å¯è®­ç»ƒå‚æ•°"""
        params = []
        params.extend(self.visual_projector.parameters())
        params.extend(self.text_projector.parameters())
        return params
    
    def compute_distillation_loss(self, visual_features, text_features, temperature=0.05):
        """è®¡ç®—çŸ¥è¯†è’¸é¦æŸå¤±"""
        # L2å½’ä¸€åŒ–
        visual_features = F.normalize(visual_features, p=2, dim=1)
        text_features = F.normalize(text_features, p=2, dim=1)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = torch.mm(visual_features, text_features.t()) / temperature
        
        # å¯¹è§’çº¿æŸå¤±ï¼ˆè‡ªç›¸ä¼¼ï¼‰
        batch_size = visual_features.size(0)
        targets = torch.arange(batch_size).to(self.device)
        
        # å¦‚æœæ–‡æœ¬ç‰¹å¾æ•°é‡ä¸å¤Ÿï¼Œä½¿ç”¨å¾ªç¯ç´¢å¼•
        if text_features.size(0) < batch_size:
            targets = targets % text_features.size(0)
        
        # å¸¦æœ‰æ ‡ç­¾å¹³æ»‘çš„äº¤å‰ç†µæŸå¤±
        label_smoothing = 0.2
        loss_v2t = F.cross_entropy(similarity_matrix, targets, label_smoothing=label_smoothing)
        loss_t2v = F.cross_entropy(similarity_matrix.t(), targets[:text_features.size(0)], label_smoothing=label_smoothing)
        
        # ç‰¹å¾å¯¹é½æŸå¤±
        alignment_loss = torch.diagonal(1 - similarity_matrix).mean()
        
        # ç‰¹å¾å‡åŒ€æ€§æŸå¤±
        uniformity_loss = torch.log(torch.exp(torch.mm(visual_features, visual_features.t()) / temperature).mean())
        
        # æ·»åŠ L2æ­£åˆ™åŒ–
        l2_reg = 0.0005 * (
            torch.norm(self.visual_projector[0].weight, p=2) +
            torch.norm(self.text_projector[0].weight, p=2)
        )
        
        # æ€»æŸå¤±
        total_loss = 0.3 * (loss_v2t + loss_t2v) / 2 + \
                    0.3 * alignment_loss + \
                    0.3 * uniformity_loss + \
                    0.1 * l2_reg
        
        return total_loss
    
    def encode_text_features_batch(self, categories, batch_size):
        """ä¸ºæ¯ä¸ªæ‰¹æ¬¡ç¼–ç æ–‡æœ¬ç‰¹å¾"""
        all_text_features = []
        templates = ["a {}", "indoor {}", "a {} in a room"]
        
        for category in categories:
            category_features = []
            
            for template in templates:
                text = template.format(category)
                text_tokens = clip.tokenize([text]).to(self.device)
                
                with torch.no_grad():
                    text_features = self.clip_model.encode_text(text_tokens).float()
                
                # åº”ç”¨æ–‡æœ¬æŠ•å½±å™¨
                projected_text = self.text_projector(text_features)
                category_features.append(projected_text)
            
            # å¹³å‡å¤šä¸ªæ¨¡æ¿çš„ç‰¹å¾
            if category_features:
                avg_features = torch.stack(category_features).mean(dim=0)
                all_text_features.append(avg_features)
        
        if all_text_features:
            text_features = torch.cat(all_text_features, dim=0)
            
            # éšæœºé€‰æ‹©æ–‡æœ¬ç‰¹å¾ï¼ˆåŒ¹é…batch sizeï¼‰
            if text_features.size(0) >= batch_size:
                selected_indices = torch.randperm(text_features.size(0))[:batch_size]
                selected_text_features = text_features[selected_indices]
            else:
                # é‡å¤æ–‡æœ¬ç‰¹å¾
                repeat_times = (batch_size + text_features.size(0) - 1) // text_features.size(0)
                repeated_text = text_features.repeat(repeat_times, 1)
                selected_text_features = repeated_text[:batch_size]
            
            return selected_text_features
        else:
            return torch.empty(batch_size, 512, dtype=torch.float32).to(self.device)
    
    def validate(self, dataloader):
        """åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹"""
        self.visual_projector.eval()
        self.text_projector.eval()
        
        val_losses = []
        
        # å®¤å†…ç±»åˆ«
        indoor_categories = [
            "chair", "table", "bed", "sofa", "cabinet", "toilet", "sink",
            "refrigerator", "microwave", "bottle", "cup", "bowl",
            "lamp", "clock", "vase", "plant", "computer", "bookshelf"
        ]
        
        with torch.no_grad():
            for batch in dataloader:
                try:
                    # è·å–å›¾åƒ
                    images = batch['images'].to(self.device)
                    batch_size = images.size(0)
                    
                    # æå–è§†è§‰ç‰¹å¾
                    visual_features = []
                    for i in range(batch_size):
                        # ä½¿ç”¨CLIPç¼–ç æ•´ä¸ªå›¾åƒ
                        image_features = self.clip_model.encode_image(images[i:i+1]).float()
                        
                        # åº”ç”¨æŠ•å½±å™¨
                        projected_features = self.visual_projector(image_features)
                        visual_features.append(projected_features)
                    
                    visual_features = torch.cat(visual_features, dim=0)
                    
                    # ç¼–ç æ–‡æœ¬ç‰¹å¾
                    text_features = self.encode_text_features_batch(indoor_categories, batch_size)
                    
                    # è®¡ç®—æŸå¤±
                    loss = self.compute_distillation_loss(visual_features, text_features)
                    
                    # è®°å½•æŸå¤±
                    val_losses.append(loss.item())
                    
                except Exception as e:
                    print(f"âš ï¸ éªŒè¯æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
                    continue
        
        avg_loss = np.mean(val_losses) if val_losses else float('inf')
        return avg_loss
    
    def train_epoch(self, dataloader, optimizer, scheduler=None, scaler=None, gradient_accumulation_steps=1):
        """è®­ç»ƒä¸€ä¸ªepochï¼Œæ”¯æŒæ··åˆç²¾åº¦å’Œæ¢¯åº¦ç´¯ç§¯"""
        self.visual_projector.train()
        self.text_projector.train()
        
        # ç¡®ä¿schedulerä¸ä¸ºNoneæ—¶ç±»å‹æ­£ç¡®
        if scheduler is not None and not isinstance(scheduler, torch.optim.lr_scheduler._LRScheduler):
            print("âš ï¸ è­¦å‘Š: æä¾›çš„è°ƒåº¦å™¨ç±»å‹ä¸æ­£ç¡®ï¼Œå·²ç¦ç”¨")
            scheduler = None
        
        epoch_losses = []
        use_amp = scaler is not None
        
        # å®¤å†…ç±»åˆ«
        indoor_categories = [
            "chair", "table", "bed", "sofa", "cabinet", "toilet", "sink",
            "refrigerator", "microwave", "bottle", "cup", "bowl",
            "lamp", "clock", "vase", "plant", "computer", "bookshelf"
        ]
        
        with tqdm(total=len(dataloader), desc="ğŸš€ è®­ç»ƒè¿›è¡Œä¸­") as pbar:
            for batch_idx, batch in enumerate(dataloader):
                try:
                    # ä»…åœ¨ç´¯ç§¯çš„ç¬¬ä¸€æ­¥æ¸…é›¶æ¢¯åº¦
                    if (batch_idx % gradient_accumulation_steps) == 0:
                        optimizer.zero_grad()
                    
                    # è·å–å›¾åƒ
                    images = batch['images'].to(self.device)
                    batch_size = images.size(0)
                    
                    # ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦
                    with autocast('cuda', enabled=use_amp):
                        # æ‰¹é‡å¤„ç†è§†è§‰ç‰¹å¾ï¼ˆæ›´é«˜æ•ˆï¼‰
                        try:
                            with torch.no_grad():
                                # ç›´æ¥æ‰¹é‡ç¼–ç æ‰€æœ‰å›¾åƒ
                                image_features = self.clip_model.encode_image(images).float()
                            
                            # åº”ç”¨è§†è§‰æŠ•å½±å™¨
                            visual_features = self.visual_projector(image_features)
                        except RuntimeError as e:
                            # å¦‚æœæ‰¹å¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°å•å›¾åƒå¤„ç†
                            if 'out of memory' in str(e):
                                torch.cuda.empty_cache()
                                print(f"âš ï¸ æ‰¹é‡å¤„ç†å†…å­˜ä¸è¶³ï¼Œå›é€€åˆ°å•å›¾åƒå¤„ç†")
                                visual_features = []
                                for i in range(batch_size):
                                    with torch.no_grad():
                                        image_features = self.clip_model.encode_image(images[i:i+1]).float()
                                    projected_features = self.visual_projector(image_features)
                                    visual_features.append(projected_features)
                                visual_features = torch.cat(visual_features, dim=0)
                            else:
                                raise e
                        
                        # æ–‡æœ¬ç‰¹å¾
                        text_features = self.encode_text_features_batch(indoor_categories, batch_size)
                        
                        # è®¡ç®—æŸå¤±
                        loss = self.compute_distillation_loss(visual_features, text_features)
                        
                        # æ£€æµ‹å¼‚å¸¸æŸå¤±å€¼
                        if not torch.isfinite(loss):
                            print(f"âš ï¸ è­¦å‘Š: æŸå¤±å€¼æ— æ•ˆ {loss.item()}, è·³è¿‡æ­¤æ‰¹æ¬¡")
                            continue
                        
                        # æ ¹æ®æ¢¯åº¦ç´¯ç§¯è°ƒæ•´æŸå¤±
                        loss = loss / gradient_accumulation_steps
                    
                    # ä½¿ç”¨æ¢¯åº¦ç¼©æ”¾å™¨è¿›è¡Œåå‘ä¼ æ’­
                    if scaler is not None:
                        scaler.scale(loss).backward()
                    else:
                        loss.backward()
                    
                    # ä»…åœ¨ç´¯ç§¯å®Œæˆåæ›´æ–°å‚æ•°
                    if (batch_idx + 1) % gradient_accumulation_steps == 0:
                        # æ¢¯åº¦è£å‰ª
                        if scaler is not None:
                            scaler.unscale_(optimizer)
                        
                        torch.nn.utils.clip_grad_norm_(self.get_trainable_parameters(), max_norm=1.0)
                        
                        # ä½¿ç”¨scaleræ›´æ–°æƒé‡
                        if scaler is not None:
                            scaler.step(optimizer)
                            scaler.update()
                        else:
                            optimizer.step()
                        
                        # è¿™é‡Œä¸å†è°ƒç”¨scheduler.step()ï¼Œç»Ÿä¸€åœ¨epochç»“æŸåå¤„ç†
                        # è¿™æ ·å¯ä»¥ç¡®ä¿optimizer.step()æ€»æ˜¯åœ¨scheduler.step()ä¹‹å‰è¢«è°ƒç”¨
                    
                    # è®°å½•æŸå¤±
                    epoch_losses.append(loss.item() * gradient_accumulation_steps)
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    current_lr = optimizer.param_groups[0]['lr']
                    
                    pbar.set_postfix({
                        'loss': f"{loss.item():.4f}",
                        'avg_loss': f"{np.mean(epoch_losses):.4f}",
                        'lr': f"{current_lr:.2e}"
                    })
                    pbar.update(1)
                    
                    # æ¸…ç†ä¸­é—´å˜é‡
                    del visual_features, text_features, loss
                    
                except Exception as e:
                    print(f"âš ï¸ æ‰¹æ¬¡ {batch_idx} å¤„ç†å¤±è´¥: {e}")
                    continue
        
        avg_loss = np.mean(epoch_losses) if epoch_losses else float('inf')
        return avg_loss

def run_fixed_training(clip_model, device, images, image_root):
    """è¿è¡Œè®­ç»ƒ"""
    print("ğŸš€ å¼€å§‹ä¼˜åŒ–ç‰ˆViLDè®­ç»ƒ (RTX 4090ä¼˜åŒ–ç‰ˆ)")
    
    try:
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = StableTrainer(
            clip_model=clip_model,
            device=device
        )
        
        # é™åˆ¶æœ€å¤§æ ·æœ¬æ•°ä»¥é¿å…å†…å­˜é—®é¢˜
        max_samples = TRAINING_CONFIG.get('max_samples', 20000)
        if len(images) > max_samples:
            print(f"âš™ï¸ é™åˆ¶è®­ç»ƒæ ·æœ¬æ•°é‡ä¸º {max_samples}ï¼ˆåŸå§‹: {len(images)}ï¼‰")
            images = images[:max_samples]
        
        # åˆ›å»ºè®­ç»ƒæ•°æ®é›†
        dataset = ImprovedCOCOIndoorDataset(
            images_data=images,
            image_root=image_root,
            image_size=TRAINING_CONFIG.get('image_size', 224),
            augment=TRAINING_CONFIG.get('augment', True)
        )
        
        if len(dataset) == 0:
            print("âŒ æ•°æ®é›†ä¸ºç©º")
            return False
        
        # åˆ›å»ºéªŒè¯æ•°æ®é›† - ä½¿ç”¨10%çš„æ•°æ®
        val_size = int(len(dataset) * 0.1)
        train_size = len(dataset) - val_size
        train_dataset, val_dataset = torch.utils.data.random_split(
            dataset, [train_size, val_size], 
            generator=torch.Generator().manual_seed(42)
        )
        
        # ä»é…ç½®ä¸­è·å–å‚æ•°
        max_epochs = TRAINING_CONFIG.get('max_epochs', 25)
        batch_size = TRAINING_CONFIG.get('batch_size', 64)  # å¢å¤§æ‰¹é‡å¤§å°
        num_workers = TRAINING_CONFIG.get('num_workers', 4)  # å¢åŠ æ•°æ®åŠ è½½çº¿ç¨‹
        pin_memory = TRAINING_CONFIG.get('pin_memory', True)
        
        # æ‰“å°GPUä¿¡æ¯
        if torch.cuda.is_available():
            print(f"\nğŸ’» GPUä¿¡æ¯:")
            print(f"   GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}")
            print(f"   å†…å­˜åˆ†é…: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB")
            print(f"   å†…å­˜ç¼“å­˜: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB")
            print(f"   æœ€å¤§å†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
        
        # ä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨
        train_dataloader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=num_workers,
            pin_memory=pin_memory,
            collate_fn=collate_fn,
            drop_last=True,
            persistent_workers=True if num_workers > 0 else False,  # ä¿æŒworkerè¿›ç¨‹æ´»è·ƒ
            prefetch_factor=2 if num_workers > 0 else None,  # é¢„åŠ è½½2ä¸ªbatch
        )
        
        val_dataloader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=pin_memory,
            collate_fn=collate_fn,
            persistent_workers=True if num_workers > 0 else False
        )
        
        # ä¼˜åŒ–å™¨é…ç½®
        learning_rate = TRAINING_CONFIG.get('learning_rate', 2e-5)  # æ›´é«˜çš„å­¦ä¹ ç‡
        weight_decay = TRAINING_CONFIG.get('weight_decay', 5e-5)
        
        # ä¼˜åŒ–å™¨
        optimizer = torch.optim.AdamW(
            trainer.get_trainable_parameters(),
            lr=learning_rate,
            weight_decay=weight_decay,
            betas=(0.9, 0.999),  # æ ‡å‡†AdamWå‚æ•°
            eps=1e-8,
            amsgrad=True
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨ - ä½¿ç”¨OneCycleLRä»¥è·å¾—æ›´å¥½çš„æ”¶æ•›
        steps_per_epoch = len(train_dataloader)
        scheduler = torch.optim.lr_scheduler.OneCycleLR(
            optimizer,
            max_lr=learning_rate,
            steps_per_epoch=steps_per_epoch,
            epochs=max_epochs,
            pct_start=0.1,  # é¢„çƒ­10%çš„è®­ç»ƒæ—¶é—´
            anneal_strategy='cos',
            div_factor=10.0,  # åˆå§‹å­¦ä¹ ç‡ = max_lr / 10
            final_div_factor=100.0,  # æœ€ç»ˆå­¦ä¹ ç‡ = max_lr / 1000
            last_epoch=-1  # æ˜¾å¼è®¾ç½®åˆå§‹åŒ–çŠ¶æ€ï¼Œé¿å…åˆå§‹åŒ–æ—¶è‡ªåŠ¨è°ƒç”¨step()
        )
        
        # åˆ›å»ºæ¢¯åº¦ç¼©æ”¾å™¨ç”¨äºæ··åˆç²¾åº¦è®­ç»ƒ
        use_amp = TRAINING_CONFIG.get('use_amp', True)
        scaler = GradScaler(enabled=use_amp)
        
        # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
        gradient_accumulation_steps = TRAINING_CONFIG.get('gradient_accumulation_steps', 1)
        
        # æŸå¤±è¿½è¸ªå™¨
        loss_tracker = LossTracker()
        
        # æ‰“å°è¯¦ç»†çš„è®­ç»ƒé…ç½®
        print(f"ğŸ“Š è®­ç»ƒé…ç½® (RTX 4090 ä¼˜åŒ–):")
        print(f"   æ•°æ®é›†å¤§å°: {len(dataset)}")
        print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"   æœ€å¤§epochæ•°: {max_epochs}")
        print(f"   å­¦ä¹ ç‡: {learning_rate}")
        print(f"   æ•°æ®åŠ è½½çº¿ç¨‹: {num_workers}")
        print(f"   æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {gradient_accumulation_steps}")
        print(f"   è‡ªåŠ¨æ··åˆç²¾åº¦: {'å¯ç”¨' if use_amp else 'ç¦ç”¨'}")
        print(f"   æœ‰æ•ˆæ‰¹å¤§å°: {batch_size * gradient_accumulation_steps}")
        
        # è®­ç»ƒç»Ÿè®¡
        train_losses = []
        val_losses = []
        lr_history = []
        
        # æœ€ä½³æ¨¡å‹ä¿¡æ¯
        best_val_loss = float('inf')
        best_epoch = 0
        patience_counter = 0
        patience = 5
        
        # åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
        checkpoint_dir = '/home/cui/rtdetr_indoor/src/vild/checkpoints'
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        # å¼€å§‹è®­ç»ƒ
        for epoch in range(max_epochs):
            print(f"\n{'='*50}")
            print(f"ğŸ”„ Epoch {epoch + 1}/{max_epochs}")
            print(f"{'='*50}")
            
            # åœ¨è®­ç»ƒå‰ç¦ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨çš„æ‰¹æ¬¡çº§æ›´æ–°ï¼Œåé¢æ‰‹åŠ¨ç»Ÿä¸€å¤„ç†
            # è¿™æ ·å¯ä»¥ç¡®ä¿å…ˆæ‰§è¡Œoptimizer.step()å†æ‰§è¡Œscheduler.step()
            train_loss = trainer.train_epoch(
                train_dataloader, 
                optimizer,
                scheduler=None,  # æ˜ç¡®è®¾ç½®ä¸ºNoneï¼Œé¿å…æ‰¹æ¬¡çº§æ›´æ–°
                scaler=scaler if use_amp else None,
                gradient_accumulation_steps=gradient_accumulation_steps
            )
            train_losses.append(train_loss)
            
            # éªŒè¯
            print(f"\nğŸ“Š è¿è¡ŒéªŒè¯...")
            val_loss = trainer.validate(val_dataloader)
            val_losses.append(val_loss)
            
            # ç»Ÿä¸€åœ¨è¿™é‡Œå¤„ç†æ‰€æœ‰å­¦ä¹ ç‡è°ƒåº¦æ›´æ–°
            # ç¡®ä¿åœ¨optimizer.step()åè°ƒç”¨scheduler.step()
            if scheduler is not None:
                if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                    # è¿™ç±»è°ƒåº¦å™¨éœ€è¦æ ¹æ®éªŒè¯æŸå¤±è°ƒæ•´å­¦ä¹ ç‡
                    scheduler.step(val_loss)
                elif isinstance(scheduler, torch.optim.lr_scheduler.OneCycleLR):
                    # OneCycleLRéœ€è¦åœ¨æ¯ä¸ªæ‰¹æ¬¡åæ›´æ–°ï¼Œè¿™é‡Œè¿›è¡Œä¸€ä¸ªepochçš„æ­¥æ•°æ›´æ–°
                    for _ in range(len(train_dataloader)):
                        scheduler.step()
                else:
                    # å…¶ä»–è°ƒåº¦å™¨åªéœ€è¦æ¯è½®æ›´æ–°ä¸€æ¬¡
                    scheduler.step()
            
            # è®°å½•å­¦ä¹ ç‡
            current_lr = optimizer.param_groups[0]['lr']
            lr_history.append(current_lr)
            
            # æ›´æ–°æŸå¤±è¿½è¸ªå™¨
            loss_tracker.update(val_loss, epoch)
            
            print(f"ğŸ“ˆ Epoch {epoch+1} ç»“æœ:")
            print(f"   è®­ç»ƒæŸå¤±: {train_loss:.6f}")
            print(f"   éªŒè¯æŸå¤±: {val_loss:.6f}")
            print(f"   å­¦ä¹ ç‡: {current_lr:.8f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                best_epoch = epoch
                patience_counter = 0
                
                # ä¿å­˜æ¨¡å‹
                best_model_path = f'{checkpoint_dir}/best_model.pth'
                checkpoint = {
                    'epoch': epoch,
                    'visual_projector': trainer.visual_projector.state_dict(),
                    'text_projector': trainer.text_projector.state_dict(),
                    'optimizer': optimizer.state_dict(),
                    'val_loss': val_loss
                }
                
                torch.save(checkpoint, best_model_path)
                print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: éªŒè¯æŸå¤±={val_loss:.6f} (ç¬¬{epoch+1}è½®)")
            else:
                patience_counter += 1
                print(f"âš ï¸ éªŒè¯æŸå¤±æœªæ”¹å–„ï¼Œå½“å‰è€å¿ƒ: {patience_counter}/{patience}")
            
            # æ—©åœæ£€æŸ¥
            if patience_counter >= patience:
                print(f"\nâ¹ï¸ æ—©åœè§¦å‘! è¿ç»­{patience}ä¸ªepochæ— æ”¹å–„")
                break
            
            # å†…å­˜æ¸…ç†
            torch.cuda.empty_cache()
            gc.collect()
        
        # è®­ç»ƒå®Œæˆåç»˜å›¾
        final_loss_path = f'{checkpoint_dir}/training_loss.png'
        loss_tracker.plot_losses(
            save_path=final_loss_path,
            train_losses=train_losses,
            val_losses=val_losses,
            lr_history=lr_history
        )
        
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“ˆ æœ€ç»ˆæˆæœ:")
        print(f"   æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
        print(f"   æœ€ä½³epoch: {best_epoch + 1}")
        print(f"   æŸå¤±å›¾å·²ä¿å­˜: {final_loss_path}")
        
        return True
        
    except Exception as e:
        import traceback
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        traceback.print_exc()
        return False
