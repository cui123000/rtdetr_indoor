# -*- coding: utf-8 -*-
"""
åŸºäºViLDçš„å¼€æ”¾ä¸–ç•Œå®¤å†…ç‰©ä½“æ£€æµ‹ - ä¸»ç¨‹åº
"""

import os
import sys
import torch
import argparse
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
import random
import cv2
import traceback
import time
from PIL import Image
        
import clip
from transformers import RTDetrForObjectDetection, RTDetrImageProcessor

from model import load_models
from data_loader import load_coco_indoor, select_random_test_image
from detector import FixedViLDDetector
from training import run_fixed_training
from utils import visualize_detections, save_detection_results, calculate_detection_stats, visualize_with_macro_categories
from config import MODEL_CONFIG, TRAINING_CONFIG, INFERENCE_CONFIG

# å…¨å±€æ§åˆ¶å˜é‡
ENABLE_TRAINING = False  # æ§åˆ¶æ˜¯å¦æ‰§è¡Œè®­ç»ƒè¿‡ç¨‹
ENABLE_DETECTION = True  # æ§åˆ¶æ˜¯å¦æ‰§è¡Œæ£€æµ‹è¿‡ç¨‹
ENABLE_LOAD_MODEL = True  # æ§åˆ¶æ˜¯å¦åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
# ç›´æ¥æŒ‡å®šæµ‹è¯•å›¾åƒçš„è·¯å¾„ï¼Œå¦‚æœè®¾ç½®äº†å…·ä½“è·¯å¾„ï¼Œå°†ä¼˜å…ˆä½¿ç”¨æ­¤è·¯å¾„
TEST_IMAGE_PATH = None # æŒ‡å®šæµ‹è¯•å›¾åƒçš„è·¯å¾„
TEST_IMAGE_INDEX = -1     # æŒ‡å®šæ•°æ®é›†ä¸­çš„å›¾åƒç´¢å¼•ï¼Œ0è¡¨ç¤ºä½¿ç”¨ç¬¬ä¸€å¼ å›¾åƒï¼ˆä»…åœ¨TEST_IMAGE_PATHä¸ºNoneæ—¶ç”Ÿæ•ˆï¼‰

# å¯é€‰çš„å‘½ä»¤è¡Œå‚æ•°è§£æ
def select_random_test_image(images, image_root, index=None):
    """ä»æ•°æ®é›†ä¸­é€‰æ‹©ä¸€ä¸ªæµ‹è¯•å›¾åƒ"""
    if not images:
        raise ValueError("æ²¡æœ‰å¯ç”¨çš„å›¾åƒ")
    
    if index is not None and 0 <= index < len(images):
        image_info = images[index]
    else:
        image_info = random.choice(images)
    
    image_path = os.path.join(image_root, image_info['file_name'])
    return image_path

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°ï¼ˆä»…ç”¨äºè¦†ç›–å…¨å±€é…ç½®ï¼‰"""
    global ENABLE_TRAINING, ENABLE_DETECTION, ENABLE_LOAD_MODEL, TEST_IMAGE_PATH, TEST_IMAGE_INDEX
    
    parser = argparse.ArgumentParser(description='ViLDå®¤å†…æ£€æµ‹')
    parser.add_argument('--train', action='store_true', help='å¯ç”¨è®­ç»ƒæ¨¡å¼')
    parser.add_argument('--detect', action='store_true', help='å¯ç”¨æ£€æµ‹æ¨¡å¼')
    parser.add_argument('--load-model', action='store_true', help='åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹')
    parser.add_argument('--model-path', type=str, help='æŒ‡å®šè¦åŠ è½½çš„æ¨¡å‹è·¯å¾„')
    parser.add_argument('--image', type=str, help='æ£€æµ‹ä½¿ç”¨çš„å›¾åƒè·¯å¾„')
    parser.add_argument('--image-index', type=int, help='ä½¿ç”¨æ•°æ®é›†ä¸­æŒ‡å®šç´¢å¼•çš„å›¾åƒè¿›è¡Œæ£€æµ‹')
    parser.add_argument('--output-dir', type=str, default=None, help='æ£€æµ‹ç»“æœè¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–å…¨å±€é…ç½®
    if args.train:
        ENABLE_TRAINING = True
    if args.detect:
        ENABLE_DETECTION = True
    if args.load_model:
        ENABLE_LOAD_MODEL = True
    if args.image:
        TEST_IMAGE_PATH = args.image
    if args.image_index is not None:
        TEST_IMAGE_INDEX = args.image_index
    
    # å¦‚æœæŒ‡å®šäº†æ¨¡å‹è·¯å¾„ï¼Œä¿®æ”¹é…ç½®
    if args.model_path:
        MODEL_CONFIG['projector_path'] = args.model_path
    
    return args

def run_training(clip_model, device):
    """è¿è¡Œè®­ç»ƒè¿‡ç¨‹"""
    if not ENABLE_TRAINING:
        print("â­ï¸ è®­ç»ƒåŠŸèƒ½å·²ç¦ç”¨ï¼Œè·³è¿‡è®­ç»ƒè¿‡ç¨‹")
        return
    
    print("ï¿½ å¼€å§‹ä¼˜åŒ–ç‰ˆViLDè®­ç»ƒ")
    print("=" * 100)
    
    try:
        # é…ç½®æ•°æ®è·¯å¾„
        PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
        
        # ä½¿ç”¨ç­›é€‰åçš„å®¤å†…åœºæ™¯æ•°æ®é›†è¿›è¡Œè®­ç»ƒ
        COCO_PATH = os.path.join(PROJECT_ROOT, "datasets/indoor_enhanced/coco_indoor_train.json")
        IMAGE_ROOT = os.path.join(PROJECT_ROOT, "datasets/coco/train2017")  # åŸå§‹å›¾åƒè·¯å¾„
        
        # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(COCO_PATH):
            print(f"âŒ è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {COCO_PATH}")
            print(f"å°è¯•ä½¿ç”¨å¤‡ç”¨æ•°æ®é›†...")
            # å¤‡é€‰è®­ç»ƒæ•°æ®
            COCO_PATH = os.path.join(PROJECT_ROOT, "datasets/indoor_training/annotations_train.json")
            IMAGE_ROOT = os.path.join(PROJECT_ROOT, "datasets/indoor_training/train")
        
        print(f"ğŸ“Š æ­£åœ¨åŠ è½½è®­ç»ƒæ•°æ®: {COCO_PATH}")
        print(f"ğŸ“¸ å›¾åƒæ ¹ç›®å½•: {IMAGE_ROOT}")
        
        images, categories = load_coco_indoor(COCO_PATH, IMAGE_ROOT)
        
        if not images:
            print("âŒ æ— æ³•åŠ è½½è®­ç»ƒæ•°æ®")
            return
            
        print(f"âœ… æˆåŠŸåŠ è½½ {len(images)} å¼ è®­ç»ƒå›¾åƒ")
        
        # è¿è¡Œè®­ç»ƒ
        run_fixed_training(
            clip_model=clip_model,
            device=device,
            images=images,
            image_root=IMAGE_ROOT
        )
    
    except Exception as e:
        print(f"âŒ è®­ç»ƒå‡ºé”™: {e}")
        traceback.print_exc()

def run_detection(clip_model, rtdetr_model, image_processor, clip_preprocess, device, args=None):
    """è¿è¡Œæ£€æµ‹è¿‡ç¨‹"""
    if not ENABLE_DETECTION:
        print("â­ï¸ æ£€æµ‹åŠŸèƒ½å·²ç¦ç”¨ï¼Œè·³è¿‡æ£€æµ‹è¿‡ç¨‹")
        return
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = None
    if args and args.output_dir:
        output_dir = args.output_dir
        
    # è®¾ç½®æ›´é«˜çš„æ£€æµ‹é˜ˆå€¼ï¼Œç‰¹åˆ«æ˜¯å¯¹äºç”¨æˆ·æä¾›çš„å›¾åƒ
    custom_threshold = 0.45  # ä½¿ç”¨æ›´é«˜çš„é˜ˆå€¼å‡å°‘è¯¯æ£€æµ‹
        
    print("ğŸ”„ å¼€å§‹ç‰©ä½“æ£€æµ‹...")
    
    try:
        # åˆ›å»ºåœºæ™¯åˆ†ç±»å™¨
        from scene_classifier import SceneClassifier
        scene_classifier = SceneClassifier(
            clip_model=clip_model,
            clip_preprocess=clip_preprocess,
            device=device
        )
        
        # åˆ›å»ºæ£€æµ‹å™¨
        detector = FixedViLDDetector(
            clip_model=clip_model,
            detector_model=rtdetr_model,
            image_processor=image_processor,
            clip_preprocess=clip_preprocess,
            device=device,
            projector_path=MODEL_CONFIG['projector_path'],
            config={'model': MODEL_CONFIG, 'inference': INFERENCE_CONFIG}
        )
        
        # é…ç½®é¡¹ç›®è·¯å¾„
        PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
        VILD_DIR = os.path.dirname(os.path.dirname(__file__))  # vildç›®å½•
        
        # è®¾ç½®è¾“å‡ºç›®å½•åœ¨vildæ–‡ä»¶å¤¹ä¸‹
        if output_dir is None:
            output_dir = os.path.join(VILD_DIR, "results")
        else:
            output_dir = os.path.join(VILD_DIR, output_dir)
            
        os.makedirs(output_dir, exist_ok=True)
        print(f"ğŸ“ ç»“æœå°†ä¿å­˜åœ¨: {output_dir}")
        
        # è·å–æµ‹è¯•å›¾åƒ
        test_image_path = TEST_IMAGE_PATH
        if not test_image_path:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šå›¾åƒè·¯å¾„ï¼Œå°è¯•ä»ç­›é€‰çš„å®¤å†…æ•°æ®é›†éšæœºé€‰æ‹©
            COCO_PATH = os.path.join(PROJECT_ROOT, "datasets/indoor_enhanced/coco_indoor_val.json")
            IMAGE_ROOT = os.path.join(PROJECT_ROOT, "datasets/coco/train2017")
            
            # æ£€æŸ¥ç­›é€‰åçš„éªŒè¯æ•°æ®é›†æ˜¯å¦å­˜åœ¨
            if not os.path.exists(COCO_PATH):
                print(f"âš ï¸ éªŒè¯æ•°æ®é›†ä¸å­˜åœ¨: {COCO_PATH}")
                # å°è¯•ç­›é€‰åçš„å¤šä¸ªåœºæ™¯æ•°æ®é›†
                scene_datasets = [
                    os.path.join(PROJECT_ROOT, "datasets/indoor_scenes/coco_bathroom_subset.json"),
                    os.path.join(PROJECT_ROOT, "datasets/indoor_scenes/coco_kitchen_subset.json"),
                    os.path.join(PROJECT_ROOT, "datasets/indoor_scenes/coco_bedroom_subset.json"),
                    os.path.join(PROJECT_ROOT, "datasets/indoor_scenes/coco_living_room_subset.json")
                ]
                
                for scene_dataset in scene_datasets:
                    if os.path.exists(scene_dataset):
                        print(f"âœ… æ‰¾åˆ°åœºæ™¯æ•°æ®é›†: {scene_dataset}")
                        COCO_PATH = scene_dataset
                        break
                else:
                    # å¦‚æœç­›é€‰çš„æ•°æ®é›†ä¸å­˜åœ¨ï¼Œå°è¯•ä½¿ç”¨åŸå§‹æ•°æ®é›†
                    COCO_PATH = os.path.join(PROJECT_ROOT, "datasets/indoor_training/annotations_train.json")
                    IMAGE_ROOT = os.path.join(PROJECT_ROOT, "datasets/indoor_training/train")
            
            try:
                images, _ = load_coco_indoor(COCO_PATH, IMAGE_ROOT)
                test_image_path = select_random_test_image(images, IMAGE_ROOT, TEST_IMAGE_INDEX)
            except Exception as e:
                print(f"âš ï¸ æ— æ³•ä»æ•°æ®é›†é€‰æ‹©å›¾åƒ: {e}")
        
        # å¦‚æœä»ç„¶æ²¡æœ‰å›¾åƒï¼Œåˆ›å»ºæµ‹è¯•å›¾åƒ
        if not test_image_path or not os.path.exists(test_image_path):
            print("åˆ›å»ºæµ‹è¯•å›¾åƒ...")
            test_dir = os.path.join(PROJECT_ROOT, "tests")
            os.makedirs(test_dir, exist_ok=True)
            test_image_path = os.path.join(test_dir, "test_image.jpg")
            
            # åˆ›å»ºç®€å•çš„æµ‹è¯•å›¾åƒ
            test_image = np.ones((480, 640, 3), dtype=np.uint8) * 200
            cv2.rectangle(test_image, (100, 100), (300, 300), (0, 0, 255), 2)
            cv2.circle(test_image, (400, 200), 50, (0, 255, 0), -1)
            cv2.imwrite(test_image_path, test_image)
            print(f"âœ“ å·²åˆ›å»ºæµ‹è¯•å›¾åƒ: {test_image_path}")
        
        print(f"ğŸ“· ä½¿ç”¨æµ‹è¯•å›¾åƒ: {test_image_path}")
        
        # å›¾åƒé¢„å¤„ç†å¢å¼º - æé«˜æ£€æµ‹è´¨é‡
        from utils import enhance_image_for_detection
        try:
            print("ğŸ”„ åº”ç”¨å›¾åƒå¢å¼ºé¢„å¤„ç†...")
            enhanced_image = enhance_image_for_detection(test_image_path)
            print("âœ“ å›¾åƒé¢„å¤„ç†å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ å›¾åƒå¢å¼ºå¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹å›¾åƒ")
            enhanced_image = None
        
        # å…ˆè¿›è¡Œåœºæ™¯åˆ†ç±»
        print("\nğŸ” å¼€å§‹åœºæ™¯åˆ†ç±»...")
        if enhanced_image:
            scene_type, scene_score = scene_classifier.classify_scene(enhanced_image)
        else:
            scene_type, scene_score = scene_classifier.classify_scene(test_image_path)
            
        scene_context = scene_classifier.get_scene_type(scene_type)
        print(f"ğŸ  è¯†åˆ«åœºæ™¯: {scene_type} (åœºæ™¯ç±»å‹: {scene_context}, ç½®ä¿¡åº¦: {scene_score:.3f})")
        
        # æ ¹æ®åœºæ™¯ç±»å‹è¿è¡Œæ£€æµ‹ï¼Œä½†ä¸è®¾ç½®ç‰¹æ®Šä¼˜å…ˆçº§
        print("\nğŸ” å¼€å§‹ç‰©ä½“æ£€æµ‹...")
        # ä½¿ç”¨å¤§ç±»åˆ«æ£€æµ‹æ¨¡å¼
        use_macro_categories = True  # é»˜è®¤å¯ç”¨å¤§ç±»åˆ«æ¨¡å¼
        
        # ç®€åŒ–åœºæ™¯ç±»å‹å¤„ç†ï¼Œä¸è®¾ä¼˜å…ˆçº§
        print(f"ï¿½ ä½¿ç”¨è¯†åˆ«çš„åœºæ™¯ç±»å‹: {scene_type}")
        
        # ç›´æ¥ä½¿ç”¨åœºæ™¯åˆ†ç±»å™¨è¯†åˆ«çš„åœºæ™¯ç±»å‹
        detect_image = enhanced_image if enhanced_image else test_image_path
        result = detector.detect_objects(detect_image, scene_type=scene_type, use_macro_categories=use_macro_categories)
        
        # ä¿å­˜æ£€æµ‹ç»“æœ
        if result:
            # è·å–æ£€æµ‹ä¿¡æ¯
            boxes = result['boxes']
            scores = result['scores']
            categories = result['labels']
            
            # å°†æ£€æµ‹ç»“æœä¿å­˜åˆ°æ–‡ä»¶
            detection_info = {
                'image_path': test_image_path,
                'boxes': boxes.tolist() if isinstance(boxes, (np.ndarray, torch.Tensor)) else boxes,
                'categories': categories,
                'scores': scores.tolist() if isinstance(scores, (np.ndarray, torch.Tensor)) else scores
            }
            
            # ä¿å­˜ç»“æœ
            results_file = os.path.join(output_dir, 'detection_result.json')
            save_detection_results([detection_info], results_file)
            
            # å¯è§†åŒ–ç»“æœ
            image = Image.open(test_image_path).convert('RGB')
            # ä½¿ç”¨å¤§ç±»åˆ«å¯è§†åŒ–
            vis_image = visualize_with_macro_categories(
                image, boxes, categories, scores, 
                threshold=INFERENCE_CONFIG['score_threshold']
            )
            
            # ä¿å­˜å¯è§†åŒ–ç»“æœ
            output_basename = f"detection_result_{os.path.basename(test_image_path)}"
            output_path = os.path.join(output_dir, output_basename)
            vis_image.save(output_path)
            print(f"âœ“ å·²ä¿å­˜æ£€æµ‹ç»“æœ: {output_path}")
            
            # åŒæ—¶ä¿å­˜åŒ…å«åœºæ™¯ç±»å‹çš„ç»“æœ
            scene_output_path = os.path.join(output_dir, f"scene_{scene_type}_{os.path.basename(test_image_path)}")
            vis_image.save(scene_output_path)
            print(f"âœ“ å·²ä¿å­˜åœºæ™¯æ£€æµ‹ç»“æœ: {scene_output_path}")
            
            # æ‰“å°æ£€æµ‹ç»Ÿè®¡ä¿¡æ¯
            if len(boxes) > 0:
                print(f"\nğŸ“Š æ£€æµ‹ç»“æœ: æ‰¾åˆ° {len(boxes)} ä¸ªç‰©ä½“")
                for i, (label, score) in enumerate(zip(categories, scores)):
                    print(f"  {i+1}. {label}: {score:.2f}")
            else:
                print("âš ï¸ æœªæ£€æµ‹åˆ°ç‰©ä½“")
        else:
            print("âŒ æ£€æµ‹å¤±è´¥")
    
    except Exception as e:
        print(f"âŒ æ£€æµ‹å‡ºé”™: {e}")
        traceback.print_exc()

def main():
    """ä¸»å‡½æ•°"""
    start_time = time.time()
    
    # å¯ä»¥è§£æå‘½ä»¤è¡Œå‚æ•°æ¥è¦†ç›–å…¨å±€é…ç½®
    args = parse_args()
    
    # è®¾ç½®è®¾å¤‡
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ‰“å°æ¨¡å‹è·¯å¾„ä¿¡æ¯
    if ENABLE_LOAD_MODEL:
        print(f"ğŸ“‚ ä½¿ç”¨è®­ç»ƒæ¨¡å‹è·¯å¾„: {MODEL_CONFIG['projector_path']}")
    
    # åŠ è½½æ¨¡å‹
    print("ğŸ“¦ æ­£åœ¨åŠ è½½æ¨¡å‹...")
    rtdetr_model, clip_model, image_processor, clip_preprocess = load_models(
        rtdetr_path=MODEL_CONFIG['rtdetr_model_path'],
        clip_name=MODEL_CONFIG['clip_model_name'],
        device=device
    )
    
    # æ‰“å°å½“å‰é…ç½®
    print(f"\nâš™ï¸ è¿è¡Œé…ç½®:")
    print(f"   è®­ç»ƒæ¨¡å¼: {'å¯ç”¨' if ENABLE_TRAINING else 'ç¦ç”¨'}")
    print(f"   æ£€æµ‹æ¨¡å¼: {'å¯ç”¨' if ENABLE_DETECTION else 'ç¦ç”¨'}")
    print(f"   åŠ è½½æ¨¡å‹: {'å¯ç”¨' if ENABLE_LOAD_MODEL else 'ç¦ç”¨'}")
    print(f"   æ¨¡å‹è·¯å¾„: {MODEL_CONFIG['projector_path']}")
    print(f"   æµ‹è¯•å›¾åƒ: {TEST_IMAGE_PATH if TEST_IMAGE_PATH else 'è‡ªåŠ¨é€‰æ‹©'}")
    print(f"   æµ‹è¯•ç´¢å¼•: {TEST_IMAGE_INDEX}")
    
    # è¿è¡Œè®­ç»ƒ
    run_training(clip_model, device)
    
    # è¿è¡Œæ£€æµ‹
    run_detection(clip_model, rtdetr_model, image_processor, clip_preprocess, device, args)
    
    # æ‰“å°æ€»è¿è¡Œæ—¶é—´
    elapsed_time = time.time() - start_time
    print(f"\nâ±ï¸ æ€»è¿è¡Œæ—¶é—´: {elapsed_time:.2f}ç§’")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        import traceback
        print(f"ç¨‹åºå‡ºé”™: {e}")
        traceback.print_exc()
