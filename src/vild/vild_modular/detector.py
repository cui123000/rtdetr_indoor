# -*- coding: utf-8 -*-
"""
åŸºäºViLDçš„å¼€æ”¾ä¸–ç•Œå®¤å†…ç‰©ä½“æ£€æµ‹ - æ£€æµ‹å™¨æ¨¡å—
"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import clip
import cv2
from PIL import Image
import time
import traceback

class FixedViLDDetector:
    """ä¼˜åŒ–ç‰ˆViLDæ£€æµ‹å™¨ - æ”¯æŒä»è®­ç»ƒå¥½çš„æ¨¡å‹åŠ è½½"""
    
    def __init__(self, clip_model, detector_model=None, image_processor=None, clip_preprocess=None, device="cuda", projector_path=None, config=None):
        self.clip_model = clip_model
        self.detector_model = detector_model
        self.image_processor = image_processor
        self.clip_preprocess = clip_preprocess
        self.device = device
        
        # åŠ è½½é…ç½®ï¼ˆå¦‚æœæä¾›ï¼‰
        if config is None:
            try:
                from config import MODEL_CONFIG, INFERENCE_CONFIG, MACRO_CATEGORIES
                config = {
                    'model': MODEL_CONFIG,
                    'inference': INFERENCE_CONFIG
                }
                self.macro_categories = MACRO_CATEGORIES
                print(f"âœ… å·²ä»é…ç½®æ–‡ä»¶åŠ è½½å‚æ•°å’Œå¤§ç±»åˆ«æ˜ å°„")
            except ImportError:
                config = {}
                self.macro_categories = {}
                print(f"âš ï¸ æ— æ³•å¯¼å…¥é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
        else:
            try:
                from config import MACRO_CATEGORIES
                self.macro_categories = MACRO_CATEGORIES
            except ImportError:
                self.macro_categories = {}
        
        # é»˜è®¤ä½¿ç”¨å¤§ç±»åˆ«æ¨¡å¼
        self.use_macro_categories = True
        
        # åˆ›å»ºç®€åŒ–ç‰ˆæŠ•å½±å™¨
        self.visual_projector = self.create_identity_projector()
        self.text_projector = self.create_identity_projector()
        
        # å¦‚æœæä¾›äº†æ¨¡å‹è·¯å¾„ï¼ŒåŠ è½½ä¿å­˜çš„æŠ•å½±å™¨
        if projector_path is None and 'model' in config and 'projector_path' in config['model']:
            projector_path = config['model']['projector_path']
            
        if projector_path and os.path.exists(projector_path):
            try:
                print(f"ğŸ“¥ æ­£åœ¨åŠ è½½æ¨¡å‹æŠ•å½±å™¨: {projector_path}")
                # åœ¨PyTorch 2.6ä¸­ï¼Œéœ€è¦æ˜¾å¼è®¾ç½®weights_only=False
                checkpoint = torch.load(projector_path, map_location=self.device, weights_only=False)
                self.visual_projector.load_state_dict(checkpoint['visual_projector'])
                self.text_projector.load_state_dict(checkpoint['text_projector'])
                print(f"âœ… æˆåŠŸåŠ è½½æŠ•å½±å™¨æ¨¡å‹ (Epoch {checkpoint['epoch']+1}, éªŒè¯æŸå¤±: {checkpoint['val_loss']:.6f})")
            except Exception as e:
                print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
                print(f"ğŸ”„ å°è¯•å¤‡ç”¨åŠ è½½æ–¹æ³•...")
                try:
                    # å°è¯•ä½¿ç”¨PyTorchå®‰å…¨ä¸Šä¸‹æ–‡æ¥åŠ è½½æ¨¡å‹
                    import numpy as np
                    # æ·»åŠ numpyæ ‡é‡ç±»å‹åˆ°å®‰å…¨å…¨å±€å˜é‡
                    torch.serialization.add_safe_globals([np.core.multiarray.scalar])
                    checkpoint = torch.load(projector_path, map_location=self.device)
                    self.visual_projector.load_state_dict(checkpoint['visual_projector'])
                    self.text_projector.load_state_dict(checkpoint['text_projector'])
                    print(f"âœ… æˆåŠŸé€šè¿‡å¤‡ç”¨æ–¹æ³•åŠ è½½æ¨¡å‹")
                except Exception as e2:
                    print(f"âŒ å¤‡ç”¨åŠ è½½æ–¹æ³•ä¹Ÿå¤±è´¥: {e2}")
                    print(f"âš ï¸ å°†ä½¿ç”¨é»˜è®¤æŠ•å½±å™¨")
    
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.visual_projector.eval()
        self.text_projector.eval()
        
        # æ£€æµ‹å‚æ•° - ä¼˜å…ˆä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å‚æ•°
        if 'model' in config and 'vild_similarity_threshold' in config['model']:
            self.similarity_threshold = config['model']['vild_similarity_threshold']
        else:
            self.similarity_threshold = 0.25
            
        if 'inference' in config and 'score_threshold' in config['inference']:
            self.detection_threshold = config['inference']['score_threshold']
        else:
            self.detection_threshold = 0.05
            
        if 'inference' in config and 'max_detections' in config['inference']:
            self.max_detections = config['inference']['max_detections']
        else:
            self.max_detections = 15
            
        print(f"ğŸ”§ æ£€æµ‹å™¨å‚æ•°: ç›¸ä¼¼åº¦é˜ˆå€¼={self.similarity_threshold:.2f}, æ£€æµ‹é˜ˆå€¼={self.detection_threshold:.2f}")
        
        # å®¤å†…ç±»åˆ«é›†åˆï¼ˆåŸºç¡€ç±»åˆ«ï¼‰
        self.base_categories = [
            'chair', 'table', 'bed', 'sofa', 'lamp', 'cabinet', 'door', 'window',
            'mirror', 'picture', 'book', 'bottle', 'cup', 'bowl', 'clock',
            'plant', 'television', 'refrigerator', 'microwave', 'toilet', 'sink',
            'towel', 'pillow', 'curtains', 'rug', 'shower', 'bathtub', 'shelf',
            'counter', 'desk', 'wardrobe', 'nightstand', 'computer', 'monitor',
            'glass', 'plate', 'tree', 'person', 'wine glass', 'fork', 'knife', 'spoon'
        ]
        
        # ä½¿ç”¨åŸºç¡€ç±»åˆ«åˆå§‹åŒ–å½“å‰æ´»åŠ¨ç±»åˆ«
        self.categories = self.base_categories.copy()
        
        # åœºæ™¯ç‰¹å®šç±»åˆ«ï¼ˆç”¨äºåœºæ™¯ä¸Šä¸‹æ–‡ä¼˜åŒ–ï¼‰
        self.scene_categories = {
            'bathroom': ['toilet', 'sink', 'towel', 'bathtub', 'shower', 'mirror'],
            'kitchen': ['refrigerator', 'microwave', 'sink', 'cabinet', 'counter', 'table', 'bottle', 'cup', 'bowl'],
            'bedroom': ['bed', 'pillow', 'lamp', 'nightstand', 'wardrobe', 'mirror', 'clock'],
            'living_room': ['sofa', 'table', 'television', 'lamp', 'rug', 'curtains', 'picture'],
            'dining_room': ['table', 'chair', 'bottle', 'cup', 'glass', 'plate', 'fork', 'knife', 'spoon', 'bowl'],
            'outdoor': ['tree', 'chair', 'table', 'bottle', 'glass', 'cup', 'person', 'plant'],
            'person': ['person'],  # äººç‰©åœºæ™¯ä¸»è¦è¯†åˆ«äºº
            'food': ['bowl', 'plate', 'fork', 'knife', 'spoon', 'cup', 'glass', 'bottle', 'food']  # é£Ÿç‰©åœºæ™¯
        }
        
        # å¼€æ”¾è¯æ±‡æ”¯æŒ
        self.clip_vocabulary = []
        self.custom_categories = []
        self.enable_open_vocabulary = True
        self.open_vocabulary_threshold = 0.35  # æé«˜é˜ˆå€¼ï¼Œå‡å°‘è¯¯æ£€æµ‹
        self.max_open_vocabulary_results = 3
        
        # ä»CLIPåŠ è½½å¤§é‡è¯æ±‡
        self._load_clip_vocabulary()
        
        print("ğŸ”§ ViLDæ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def create_identity_projector(self):
        """åˆ›å»ºæ¥è¿‘æ’ç­‰æ˜ å°„çš„æŠ•å½±å™¨"""
        projector = torch.nn.Sequential(
            torch.nn.Linear(512, 512, bias=True, dtype=torch.float32),
            torch.nn.ReLU(),
            torch.nn.Linear(512, 512, bias=True, dtype=torch.float32)
        ).to(self.device)
        
        # åˆå§‹åŒ–ä¸ºæ’ç­‰æ˜ å°„
        with torch.no_grad():
            # ç¬¬ä¸€å±‚ï¼šæ’ç­‰æ˜ å°„
            torch.nn.init.eye_(projector[0].weight)
            if projector[0].bias is not None:
                torch.nn.init.zeros_(projector[0].bias)
            
            # ç¬¬ä¸‰å±‚ï¼šæ’ç­‰æ˜ å°„
            torch.nn.init.eye_(projector[2].weight)
            if projector[2].bias is not None:
                torch.nn.init.zeros_(projector[2].bias)
            
            # ç¡®ä¿æ‰€æœ‰æƒé‡éƒ½æ˜¯float32
            for param in projector.parameters():
                param.data = param.data.float()
        
        projector.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        return projector
    
    def map_to_macro_category(self, label):
        """å°†ç»†ç²’åº¦æ ‡ç­¾æ˜ å°„åˆ°å¤§ç±»åˆ«"""
        # æŸ¥æ‰¾æ ‡ç­¾æ‰€å±çš„å¤§ç±»åˆ«
        for macro_cat, items in self.macro_categories.items():
            if label.lower() in [item.lower() for item in items]:
                return macro_cat
                
        # å¦‚æœæ‰¾ä¸åˆ°æ˜ å°„ï¼Œä½¿ç”¨åŸå§‹æ ‡ç­¾
        return label
    
    def _load_clip_vocabulary(self):
        """åŠ è½½CLIPè¯æ±‡è¡¨"""
        # å¸¸è§å®¤å†…ç‰©ä½“çš„æ‰©å±•è¯æ±‡è¡¨
        extended_vocabulary = [
            # å®¶å…·ç±»
            "armchair", "bench", "bookshelf", "bunk bed", "coffee table", "dining table",
            "dresser", "end table", "filing cabinet", "footstool", "futon", "loveseat",
            "ottoman", "recliner", "rocking chair", "sideboard", "stool", "tv stand",
            
            # ç”µå™¨ç±»
            "air conditioner", "blender", "coffee maker", "dishwasher", "electric fan", 
            "food processor", "hair dryer", "heater", "humidifier", "iron", "juicer",
            "kettle", "microwave oven", "mixer", "oven", "rice cooker", "toaster", 
            "vacuum cleaner", "washing machine", "water heater",
            
            # å«æµ´ç±»
            "bathroom cabinet", "bathroom mirror", "bathroom shelf", "bath mat",
            "faucet", "hand towel", "medicine cabinet", "shower curtain", "shower door",
            "shower head", "soap dish", "toilet brush", "toilet paper holder", "towel rack",
            
            # é£Ÿç‰©ç±»
            "food", "meal", "dish", "cuisine", "lunch", "dinner", "breakfast", 
            "appetizer", "entree", "dessert", "vegetable", "fruit", "meat", "beef",
            "chicken", "pork", "salad", "stew", "soup", "rice", "noodles", "pasta"
        ]
        
        # åŠ è½½åŸºæœ¬ç±»åˆ«å’Œæ‰©å±•è¯æ±‡è¡¨
        self.clip_vocabulary = self.base_categories + extended_vocabulary
        print(f"âœ… åŠ è½½äº† {len(self.clip_vocabulary)} ä¸ªè¯æ±‡é¡¹")
    
    def detect_objects(self, image_path, scene_type=None, custom_categories=None, enable_open_vocabulary=True, use_macro_categories=True):
        """æ£€æµ‹å›¾åƒä¸­çš„ç‰©ä½“"""
        try:
            start_time = time.time()
            
            # æ‰“å¼€å›¾åƒ
            if isinstance(image_path, str):
                image = Image.open(image_path).convert('RGB')
            else:
                # å¦‚æœå·²ç»æ˜¯PILå›¾åƒï¼Œç›´æ¥ä½¿ç”¨
                image = image_path if hasattr(image_path, 'convert') else Image.fromarray(image_path)
                
            # è®¾ç½®ä½¿ç”¨å¤§ç±»åˆ«çš„æ¨¡å¼
            self.use_macro_categories = use_macro_categories
            print(f"ğŸ” æ£€æµ‹æ¨¡å¼: {'å¤§ç±»åˆ«åˆ†ç»„' if self.use_macro_categories else 'ç»†ç²’åº¦ç±»åˆ«'}")
            
            # å¤„ç†è‡ªå®šä¹‰ç±»åˆ«
            if custom_categories:
                self.set_custom_categories(custom_categories)
            
            # è®¾ç½®å¼€æ”¾è¯æ±‡æ£€æµ‹
            self.enable_open_vocabulary = enable_open_vocabulary
            
            # 1. æå–å€™é€‰åŒºåŸŸ
            boxes, detection_scores = self.extract_regions(image)
            if len(boxes) == 0:
                print(f"âŒ æ²¡æœ‰æ‰¾åˆ°å€™é€‰åŒºåŸŸ")
                return {'boxes': [], 'scores': [], 'labels': []}
            
            print(f"ğŸ“¦ æ‰¾åˆ° {len(boxes)} ä¸ªå€™é€‰åŒºåŸŸ")
            
            # 2. æå–è§†è§‰ç‰¹å¾
            visual_features = self.extract_visual_features(image, boxes)
            if visual_features.size(0) == 0:
                return {'boxes': [], 'scores': [], 'labels': []}
            
            # 3. ç¼–ç æ–‡æœ¬ç‰¹å¾
            text_features = self.encode_text_features()
            
            # 4. è®¡ç®—ç›¸ä¼¼åº¦
            similarity_matrix = torch.mm(visual_features, text_features.t())
            
            # åœºæ™¯ä¼˜åŒ–
            if scene_type is not None:
                similarity_matrix = self.apply_scene_context(scene_type, similarity_matrix)
            
            max_similarities, best_category_indices = similarity_matrix.max(dim=1)
            
            # 5. è¿‡æ»¤
            # åŠ¨æ€é˜ˆå€¼ - æ›´ä¿å®ˆçš„é˜ˆå€¼è®¾ç½®
            similarity_threshold = self.similarity_threshold
            if max_similarities.max() > 0.4:
                adaptive_threshold = max(max_similarities.max() * 0.65, self.similarity_threshold)
                similarity_threshold = min(adaptive_threshold, 0.4)
            
            # å¯¹äºæˆ·å¤–/é¤å…åœºæ™¯ï¼Œä½¿ç”¨æ›´é«˜çš„é˜ˆå€¼ç­›é€‰
            if scene_type in ["outdoor", "dining_room"]:
                similarity_threshold += 0.05  # å¢åŠ 5%çš„é˜ˆå€¼
                print(f"ğŸ“Š åœºæ™¯ç‰¹åŒ–: ä¸º {scene_type} åœºæ™¯å¢åŠ é˜ˆå€¼è‡³ {similarity_threshold:.2f}")
            
            valid_mask = max_similarities >= similarity_threshold
            valid_count = valid_mask.sum().item()
            
            # å¦‚æœæ²¡æœ‰åŒ¹é…ï¼Œé™ä½é˜ˆå€¼
            if valid_count == 0:
                low_threshold = 0.05
                valid_mask = max_similarities >= low_threshold
                valid_count = valid_mask.sum().item()
                
                if valid_count == 0 and not self.enable_open_vocabulary:
                    return {'boxes': [], 'scores': [], 'labels': []}
            
            # å¤„ç†åŸºç¡€ç±»åˆ«æ£€æµ‹
            if valid_count > 0:
                valid_boxes = boxes[:len(valid_mask)][valid_mask.cpu().numpy()]
                valid_detection_scores = detection_scores[:len(valid_mask)][valid_mask.cpu().numpy()]
                valid_similarities = max_similarities[valid_mask].cpu().numpy()
                valid_category_indices = best_category_indices[valid_mask].cpu().numpy()
                valid_labels = [self.categories[idx] for idx in valid_category_indices]
                
                # ç»„åˆåˆ†æ•°
                combined_scores = valid_detection_scores * 0.3 + valid_similarities * 0.7
                
                # æ’åº
                sorted_indices = np.argsort(combined_scores)[::-1][:self.max_detections]
                
                final_boxes = valid_boxes[sorted_indices]
                final_scores = combined_scores[sorted_indices]
                final_labels = [valid_labels[i] for i in sorted_indices]
                
                # å¦‚æœä½¿ç”¨å¤§ç±»åˆ«ï¼Œè¿›è¡Œæ˜ å°„
                if self.use_macro_categories and self.macro_categories:
                    final_labels = [self.map_to_macro_category(label) for label in final_labels]
                    print(f"âœ“ å·²å°†æ£€æµ‹ç»“æœæ˜ å°„åˆ°å¤§ç±»åˆ«")
                
                result = {
                    'boxes': final_boxes,
                    'scores': final_scores,
                    'labels': final_labels,
                    'open_vocab_results': {}
                }
            else:
                result = {
                    'boxes': np.array([]),
                    'scores': np.array([]),
                    'labels': [],
                    'open_vocab_results': {}
                }
            
            # å¼€æ”¾è¯æ±‡æ£€æµ‹
            if self.enable_open_vocabulary:
                open_vocab_results = self.perform_open_vocabulary_detection(
                    visual_features, boxes, detection_scores
                )
                
                if open_vocab_results:
                    result['open_vocab_results'] = open_vocab_results
                    
                    if len(result['boxes']) == 0 and len(open_vocab_results['boxes']) > 0:
                        result['boxes'] = open_vocab_results['boxes']
                        result['scores'] = open_vocab_results['scores']
                        result['labels'] = open_vocab_results['labels']
            
            # è®¡ç®—æ£€æµ‹æ—¶é—´
            detection_time = time.time() - start_time
            result['detection_time'] = detection_time
            
            print(f"â±ï¸ æ£€æµ‹å®Œæˆï¼Œç”¨æ—¶: {detection_time:.2f}ç§’")
            return result
            
        except Exception as e:
            import traceback
            print(f"âŒ æ£€æµ‹å¤±è´¥: {e}")
            traceback.print_exc()
            return {'boxes': [], 'scores': [], 'labels': []}
    
    def extract_regions(self, image):
        """æå–å€™é€‰åŒºåŸŸ"""
        if self.image_processor is None or self.detector_model is None:
            # å¦‚æœæ²¡æœ‰æ£€æµ‹å™¨ï¼Œä½¿ç”¨ç®€å•çš„ç½‘æ ¼åŒºåŸŸ
            print("âš ï¸ æ²¡æœ‰æ£€æµ‹å™¨æ¨¡å‹ï¼Œä½¿ç”¨ç½‘æ ¼åŒºåŸŸ")
            width, height = image.size
            boxes = []
            scores = []
            
            # åˆ›å»º3x3ç½‘æ ¼
            for i in range(3):
                for j in range(3):
                    x1 = j * width // 3
                    y1 = i * height // 3
                    x2 = (j + 1) * width // 3
                    y2 = (i + 1) * height // 3
                    boxes.append([x1, y1, x2, y2])
                    scores.append(0.9)  # ä½¿ç”¨è¾ƒé«˜çš„ç½®ä¿¡åº¦
            
            # æ·»åŠ æ•´ä¸ªå›¾åƒ
            boxes.append([0, 0, width, height])
            scores.append(1.0)
            
            return np.array(boxes), np.array(scores)
        
        # ä½¿ç”¨RT-DETRæ¨¡å‹æå–åŒºåŸŸ
        inputs = self.image_processor(image, return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            outputs = self.detector_model(**inputs)
        
        target_sizes = torch.tensor([image.size[::-1]]).to(self.device)
        results = self.image_processor.post_process_object_detection(
            outputs, target_sizes=target_sizes, threshold=self.detection_threshold
        )[0]
        
        return results['boxes'].cpu().numpy(), results['scores'].cpu().numpy()
    
    def extract_visual_features(self, image, boxes):
        """æå–è§†è§‰ç‰¹å¾"""
        if len(boxes) == 0:
            return torch.empty(0, 512).to(self.device)
        
        features = []
        img_array = np.array(image)
        max_regions = min(len(boxes), 50)  # é™åˆ¶å¤„ç†æ•°é‡
        
        for i, box in enumerate(boxes[:max_regions]):
            try:
                x1, y1, x2, y2 = box.astype(int)
                
                # è¾¹ç•Œæ£€æŸ¥
                x1 = max(0, min(x1, img_array.shape[1]-1))
                y1 = max(0, min(y1, img_array.shape[0]-1))
                x2 = max(x1+1, min(x2, img_array.shape[1]))
                y2 = max(y1+1, min(y2, img_array.shape[0]))
                
                # æå–åŒºåŸŸ
                region = img_array[y1:y2, x1:x2]
                region_image = Image.fromarray(region)
                
                # ä½¿ç”¨CLIPå¤„ç†
                region_input = self.clip_preprocess(region_image).unsqueeze(0).to(self.device)
                
                with torch.no_grad():
                    region_feature = self.clip_model.encode_image(region_input).float()
                    projected_feature = self.visual_projector(region_feature)
                    normalized_feature = F.normalize(projected_feature, p=2, dim=1)
                    features.append(normalized_feature)
                    
            except Exception as e:
                print(f"âš ï¸ åŒºåŸŸç‰¹å¾æå–é”™è¯¯ (box={box}): {e}")
                # è·³è¿‡é—®é¢˜åŒºåŸŸ
                continue
        
        if not features:
            return torch.empty(0, 512).to(self.device)
        
        return torch.cat(features, dim=0)
    
    def encode_text_features(self):
        """ç¼–ç æ–‡æœ¬ç‰¹å¾"""
        all_text_features = []
        templates = ["a {}", "indoor {}", "a {} in a room"]
        
        for category in self.categories:
            category_features = []
            
            for template in templates:
                text = template.format(category)
                text_tokens = clip.tokenize([text]).to(self.device)
                
                with torch.no_grad():
                    text_features = self.clip_model.encode_text(text_tokens).float()
                    projected_text = self.text_projector(text_features)
                    normalized_feature = F.normalize(projected_text, p=2, dim=1)
                    category_features.append(normalized_feature)
            
            # å¹³å‡å¤šä¸ªæ¨¡æ¿çš„ç‰¹å¾
            if category_features:
                avg_features = torch.stack(category_features).mean(dim=0)
                all_text_features.append(avg_features)
        
        if all_text_features:
            return torch.cat(all_text_features, dim=0)
        else:
            return torch.empty(0, 512, dtype=torch.float32).to(self.device)
    
    def set_custom_categories(self, categories):
        """è®¾ç½®ç”¨æˆ·è‡ªå®šä¹‰ç±»åˆ«åˆ—è¡¨"""
        if not categories:
            return
            
        self.categories = self.base_categories.copy()
        self.custom_categories = [c for c in categories if c not in self.categories]
        self.categories.extend(self.custom_categories)
        
    def apply_scene_context(self, scene_type, similarity_matrix):
        """åº”ç”¨åœºæ™¯ä¸Šä¸‹æ–‡ä¼˜åŒ–"""
        # å¦‚æœæ˜¯"dining room"ï¼Œä¹Ÿå°è¯•åŒ¹é…"dining_room"
        if scene_type == "dining room":
            scene_type = "dining_room"
            
        # å¦‚æœæ˜¯æˆ·å¤–åœºæ™¯ï¼Œä½¿ç”¨æˆ·å¤–ä¼˜åŒ–
        if scene_type.lower() in ["outdoor", "garden", "patio", "yard", "terrace"]:
            scene_type = "outdoor"
            
        # å¦‚æœæ˜¯äººç‰©åœºæ™¯
        if scene_type.lower() in ["person", "portrait", "selfie", "people", "human"]:
            scene_type = "person"
        
        if scene_type not in self.scene_categories:
            print(f"âš ï¸ æœªæ‰¾åˆ°åœºæ™¯ç±»å‹ '{scene_type}' çš„ç‰¹å®šä¼˜åŒ–ï¼Œä½¿ç”¨é€šç”¨æ£€æµ‹")
            return similarity_matrix
            
        # è·å–åœºæ™¯ç›¸å…³ç±»åˆ«
        relevant_categories = self.scene_categories[scene_type]
        relevant_indices = [i for i, cat in enumerate(self.categories) if cat in relevant_categories]
        
        # ä¿®æ”¹ç›¸ä¼¼åº¦åˆ†æ•°
        modified_matrix = similarity_matrix.clone()
        boost_factor = 0.20  # æé«˜åˆ°20%æå‡
        
        # äººç‰©åœºæ™¯ç‰¹æ®Šå¤„ç† - æ›´é«˜çš„æå‡å› å­
        if scene_type == "person":
            boost_factor = 0.40  # å¯¹äººç‰©çš„æ£€æµ‹æå‡40%
            
            # å¯¹personç±»åˆ«è¿›è¡Œå¼ºåŒ–
            person_indices = [i for i, cat in enumerate(self.categories) if cat == "person"]
            for i in range(similarity_matrix.size(0)):
                for idx in person_indices:
                    modified_matrix[i, idx] *= (1 + boost_factor)
            
            # å¼ºçƒˆæŠ‘åˆ¶ä¸å¤ªå¯èƒ½åœ¨äººç‰©è‚–åƒä¸­å‡ºç°çš„ç‰©ä½“
            highly_unlikely_categories = ['toilet', 'bathtub', 'shower', 'refrigerator', 
                                         'microwave', 'oven', 'sink', 'bed']
                                         
            # ä¸é‚£ä¹ˆå¼ºçƒˆåœ°æŠ‘åˆ¶å¯èƒ½é”™è¯¯æ£€æµ‹çš„ç‰©ä½“
            unlikely_categories = ['chair', 'table', 'cabinet', 'sofa']
            
            # è·å–é«˜åº¦ä¸å¯èƒ½çš„ç±»åˆ«ç´¢å¼•
            highly_unlikely_indices = [i for i, cat in enumerate(self.categories) 
                                      if cat in highly_unlikely_categories]
            
            # è·å–ä¸å¤ªå¯èƒ½çš„ç±»åˆ«ç´¢å¼•
            unlikely_indices = [i for i, cat in enumerate(self.categories) 
                               if cat in unlikely_categories]
            
            # åº”ç”¨å¼ºæŠ‘åˆ¶
            for i in range(similarity_matrix.size(0)):
                for idx in highly_unlikely_indices:
                    modified_matrix[i, idx] *= 0.3  # é™ä½70%
                
                for idx in unlikely_indices:
                    modified_matrix[i, idx] *= 0.5  # é™ä½50%
                    
            print(f"âœ… å·²åº”ç”¨äººç‰©åœºæ™¯ç‰¹æ®Šä¼˜åŒ–: äººç‰© +{boost_factor*100:.0f}%, æŠ‘åˆ¶ä¸ç›¸å…³ç‰©ä½“")
            return modified_matrix
            
        # é£Ÿç‰©åœºæ™¯ç‰¹æ®Šå¤„ç†
        if scene_type == "food":
            boost_factor = 0.40  # å¯¹é£Ÿç‰©ç›¸å…³ç±»åˆ«æå‡40%
            
            # é£Ÿç‰©ç›¸å…³ç±»åˆ«
            food_categories = ['bowl', 'plate', 'fork', 'knife', 'spoon', 'food']
            if self.use_macro_categories:
                food_categories.extend(['tableware', 'food'])
                
            # åŠ å¼ºé£Ÿç‰©ç›¸å…³ç±»åˆ«
            food_indices = [i for i, cat in enumerate(self.categories) 
                            if any(food_cat in cat.lower() for food_cat in food_categories)]
            
            for i in range(similarity_matrix.size(0)):
                for idx in food_indices:
                    modified_matrix[i, idx] *= (1 + boost_factor)
            
            # å¼ºçƒˆæŠ‘åˆ¶ä¸å¤ªå¯èƒ½åœ¨é£Ÿç‰©åœºæ™¯ä¸­å‡ºç°çš„ç‰©ä½“
            highly_unlikely_categories = ['toilet', 'bathtub', 'shower', 'bed', 'person']
            
            # è·å–é«˜åº¦ä¸å¯èƒ½çš„ç±»åˆ«ç´¢å¼•
            highly_unlikely_indices = [i for i, cat in enumerate(self.categories) 
                                      if cat in highly_unlikely_categories]
            
            # åº”ç”¨å¼ºæŠ‘åˆ¶
            for i in range(similarity_matrix.size(0)):
                for idx in highly_unlikely_indices:
                    modified_matrix[i, idx] *= 0.3  # é™ä½70%
                    
            print(f"âœ… å·²åº”ç”¨é£Ÿç‰©åœºæ™¯ç‰¹æ®Šä¼˜åŒ–: é£Ÿç‰©ç›¸å…³ç‰©å“ +{boost_factor*100:.0f}%, æŠ‘åˆ¶ä¸ç›¸å…³ç‰©ä½“")
            return modified_matrix
        
        # å…¶ä»–åœºæ™¯çš„å¤„ç†
        for i in range(similarity_matrix.size(0)):
            for idx in relevant_indices:
                modified_matrix[i, idx] *= (1 + boost_factor)
                
        # ä¸ç›¸å…³ç±»åˆ«é™ä½åˆ†æ•° - æ›´å¼ºçš„æƒ©ç½š
        highly_unlikely_categories = []
        if scene_type == "outdoor":
            # æˆ·å¤–ä¸å¤ªå¯èƒ½å‡ºç°çš„ç‰©ä½“
            highly_unlikely_categories = ['toilet', 'bathtub', 'shower', 'refrigerator', 'microwave', 
                                         'wardrobe', 'curtains', 'bed', 'nightstand']
        elif scene_type == "dining_room":
            # é¤å…ä¸å¤ªå¯èƒ½å‡ºç°çš„ç‰©ä½“
            highly_unlikely_categories = ['toilet', 'bathtub', 'shower', 'bed', 'pillow']
            
        # è·å–é«˜åº¦ä¸å¯èƒ½çš„ç±»åˆ«ç´¢å¼•
        highly_unlikely_indices = [i for i, cat in enumerate(self.categories) if cat in highly_unlikely_categories]
        
        # å¸¸è§„ä¸ç›¸å…³ç±»åˆ«
        non_relevant_indices = [i for i, cat in enumerate(self.categories) 
                              if cat not in relevant_categories and cat not in highly_unlikely_categories]
        
        # å¸¸è§„ä¸ç›¸å…³ç±»åˆ«çš„å°æƒ©ç½š
        penalty_factor = 0.10  # æé«˜åˆ°10%æƒ©ç½š
        for i in range(similarity_matrix.size(0)):
            for idx in non_relevant_indices:
                modified_matrix[i, idx] *= (1 - penalty_factor)
        
        # é«˜åº¦ä¸å¯èƒ½ç±»åˆ«çš„å¼ºæƒ©ç½š
        strong_penalty = 0.50  # 50%æƒ©ç½š
        for i in range(similarity_matrix.size(0)):
            for idx in highly_unlikely_indices:
                modified_matrix[i, idx] *= (1 - strong_penalty)
                
        print(f"âœ… å·²åº”ç”¨ '{scene_type}' åœºæ™¯ä¼˜åŒ–: ç›¸å…³ç‰©ä½“ +{boost_factor*100:.0f}%, ä¸ç›¸å…³ç‰©ä½“ -{penalty_factor*100:.0f}%, æä¸å¯èƒ½ç‰©ä½“ -{strong_penalty*100:.0f}%")
        return modified_matrix
    
    def perform_open_vocabulary_detection(self, visual_features, boxes, detection_scores):
        """æ‰§è¡Œå¼€æ”¾è¯æ±‡æ£€æµ‹"""
        try:
            if not self.clip_vocabulary:
                return {}
                
            open_vocab_results = {
                'boxes': [],
                'scores': [],
                'labels': [],
                'alternative_labels': []
            }
            
            # æ‰¹é‡å¤„ç†è¯æ±‡
            batch_size = 200
            all_text_features = []
            
            for i in range(0, len(self.clip_vocabulary), batch_size):
                batch = self.clip_vocabulary[i:i+batch_size]
                
                texts = [f"a {word}" for word in batch]
                text_tokens = clip.tokenize(texts).to(self.device)
                
                with torch.no_grad():
                    batch_text_features = self.clip_model.encode_text(text_tokens).float()
                    batch_text_features = self.text_projector(batch_text_features)
                    batch_text_features = F.normalize(batch_text_features, p=2, dim=1)
                    all_text_features.append(batch_text_features)
            
            text_features = torch.cat(all_text_features, dim=0)
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarity_matrix = torch.mm(visual_features, text_features.t())
            
            # æ‰¾åˆ°æœ€ä½³åŒ¹é…
            for i in range(similarity_matrix.size(0)):
                # è·å–å‰Kä¸ªæœ€ä½³åŒ¹é…
                similarities, indices = torch.topk(similarity_matrix[i], k=self.max_open_vocabulary_results)
                
                # æ£€æŸ¥ç›¸ä¼¼åº¦æ˜¯å¦é«˜äºé˜ˆå€¼
                if similarities[0] >= self.open_vocabulary_threshold:
                    # æœ€ä½³åŒ¹é…ä½œä¸ºä¸»æ ‡ç­¾
                    best_idx = indices[0].item()
                    best_score = similarities[0].item()
                    best_label = self.clip_vocabulary[best_idx]
                    
                    # å…¶ä»–å€™é€‰é¡¹
                    alt_indices = indices[1:].cpu().numpy()
                    alt_scores = similarities[1:].cpu().numpy()
                    alt_labels = [(self.clip_vocabulary[idx], score) for idx, score in zip(alt_indices, alt_scores)]
                    
                    # æ·»åŠ ç»“æœ
                    open_vocab_results['boxes'].append(boxes[i])
                    open_vocab_results['scores'].append(best_score)
                    open_vocab_results['labels'].append(best_label)
                    open_vocab_results['alternative_labels'].append(alt_labels)
                    
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            if open_vocab_results['boxes']:
                open_vocab_results['boxes'] = np.array(open_vocab_results['boxes'])
                open_vocab_results['scores'] = np.array(open_vocab_results['scores'])
                
                # ä¿ç•™æœ€ä½³ç»“æœ
                if len(open_vocab_results['boxes']) > self.max_detections:
                    # æ’åº
                    sorted_indices = np.argsort(open_vocab_results['scores'])[::-1][:self.max_detections]
                    open_vocab_results['boxes'] = open_vocab_results['boxes'][sorted_indices]
                    open_vocab_results['scores'] = open_vocab_results['scores'][sorted_indices]
                    open_vocab_results['labels'] = [open_vocab_results['labels'][i] for i in sorted_indices]
                    open_vocab_results['alternative_labels'] = [open_vocab_results['alternative_labels'][i] for i in sorted_indices]
                    
            return open_vocab_results
            
        except Exception as e:
            print(f"âŒ å¼€æ”¾è¯æ±‡æ£€æµ‹å¤±è´¥: {e}")
            return {}
