# -*- coding: utf-8 -*-
"""
åŸºäºViLDçš„å¼€æ”¾ä¸–ç•Œå®¤å†…ç‰©ä½“æ£€æµ‹ - æ•°æ®åŠ è½½æ¨¡å—
"""

import os
import json
import random
import numpy as np
import cv2
from PIL import Image
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms

def load_coco_indoor(coco_path, image_root):
    """åŠ è½½COCOæ•°æ®é›†ä¸­çš„å®¤å†…åœºæ™¯æ•°æ®"""
    if not os.path.exists(coco_path):
        raise FileNotFoundError(f"æ³¨é‡Šæ–‡ä»¶ä¸å­˜åœ¨: {coco_path}")
        
    print(f"æ­£åœ¨åŠ è½½æ•°æ®é›†: {coco_path}")
    try:
        with open(coco_path, 'r') as f:
            dataset = json.load(f)
    except json.JSONDecodeError as e:
        print(f"JSONè§£æé”™è¯¯: {e}")
        raise
    
    # æ„å»ºç±»åˆ«æ˜ å°„
    categories = {cat['id']: cat for cat in dataset['categories']}
    
    # å¤„ç†å›¾åƒå’Œæ ‡æ³¨
    image_dict = {}
    for image in dataset['images']:
        file_name = None
        
        if 'file_name' in image:
            file_name = image['file_name']
        elif 'coco_url' in image:
            file_name = os.path.basename(image['coco_url'])
        else:
            continue
        
        image_dict[image['id']] = {
            'file_name': file_name,
            'height': image.get('height', 0),
            'width': image.get('width', 0),
            'annotations': []
        }
    
    # æ·»åŠ æ ‡æ³¨ä¿¡æ¯
    for ann in dataset['annotations']:
        try:
            image_id = ann['image_id']
            if image_id in image_dict:
                if 'bbox' in ann and 'category_id' in ann:
                    image_dict[image_id]['annotations'].append({
                        'bbox': ann['bbox'],  # [x, y, w, h]
                        'category_id': ann['category_id'],
                        'segmentation': ann.get('segmentation', []),
                        'iscrowd': ann.get('iscrowd', 0)
                    })
        except KeyError:
            continue
    
    # è¿‡æ»¤æ‰æ²¡æœ‰æ ‡æ³¨çš„å›¾åƒ
    valid_images = [img for img in image_dict.values() if len(img['annotations']) > 0]
    print(f"æœ‰æ•ˆå›¾åƒæ•°é‡(å«æ ‡æ³¨): {len(valid_images)}/{len(image_dict)}")
    
    return valid_images, categories

def select_random_test_image(images, image_root, test_index=-1):
    """ä»æ•°æ®é›†ä¸­é€‰æ‹©ä¸€ä¸ªéšæœºæµ‹è¯•å›¾åƒ"""
    if len(images) == 0:
        return None
    
    # å¦‚æœæŒ‡å®šäº†æµ‹è¯•å›¾åƒç´¢å¼•ï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™éšæœºé€‰æ‹©
    if test_index >= 0 and test_index < len(images):
        img_index = test_index
    else:
        # éšæœºé€‰æ‹©ä¸€ä¸ªå›¾åƒ
        img_index = random.randint(0, len(images) - 1)
    
    img_info = images[img_index]
    img_path = os.path.join(image_root, img_info['file_name'])
    
    if os.path.exists(img_path):
        print(f"ğŸ“· é€‰æ‹©æµ‹è¯•å›¾åƒ: {os.path.basename(img_path)} (ç´¢å¼• {img_index})")
        return img_path
    else:
        print(f"âš ï¸ é€‰æ‹©çš„å›¾åƒä¸å­˜åœ¨: {img_path}")
        return None

class ImprovedCOCOIndoorDataset(Dataset):
    """æ”¹è¿›çš„COCOå®¤å†…æ•°æ®é›†"""
    
    def __init__(self, images_data, image_root, image_size=256, augment=True, max_samples=None):
        self.images_data = images_data
        self.image_root = image_root
        self.image_size = image_size
        self.augment = augment
        
        # è¿‡æ»¤æœ‰æ•ˆå›¾åƒ
        self.valid_images = []
        for img_info in images_data:
            img_path = os.path.join(image_root, img_info['file_name'])
            if os.path.exists(img_path) and len(img_info['annotations']) > 0:
                # é¢å¤–æ£€æŸ¥å›¾åƒæ˜¯å¦å¯ä»¥æ­£ç¡®æ‰“å¼€
                try:
                    with Image.open(img_path) as img:
                        if img.width > 0 and img.height > 0:
                            self.valid_images.append(img_info)
                except Exception as e:
                    print(f"âš ï¸ å›¾åƒæ–‡ä»¶æ— æ•ˆï¼Œè·³è¿‡: {img_path}")
        
        # é™åˆ¶æ ·æœ¬æ•°é‡ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if max_samples and len(self.valid_images) > max_samples:
            self.valid_images = random.sample(self.valid_images, max_samples)
        
        # åŸºæœ¬è½¬æ¢
        self.transform = transforms.Compose([
            transforms.Resize((image_size, image_size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        # å¼ é‡å¢å¼º (åœ¨ToTensorä¹‹ååº”ç”¨)
        self.tensor_augment = None
        if augment:
            self.tensor_augment = transforms.Compose([
                transforms.RandomErasing(p=0.3, scale=(0.02, 0.2), ratio=(0.3, 3.3))
            ])
            
        # PILå›¾åƒå¢å¼º (åœ¨ToTensorä¹‹å‰åº”ç”¨)
        if augment:
            self.augment_transform = transforms.Compose([
                transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0), ratio=(0.75, 1.3333)),
                transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),
                transforms.RandomHorizontalFlip(p=0.5),
                transforms.RandomRotation(degrees=15),
                transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),
                transforms.RandomPerspective(distortion_scale=0.2, p=0.5),
                transforms.RandomGrayscale(p=0.1)
            ])
        else:
            self.augment_transform = None
        
        print(f"ğŸ“Š æ•°æ®é›†åˆå§‹åŒ–å®Œæˆ: {len(self.valid_images)} æœ‰æ•ˆå›¾åƒ")
    
    def __len__(self):
        return len(self.valid_images)
    
    def __getitem__(self, idx):
        img_info = self.valid_images[idx]
        img_path = os.path.join(self.image_root, img_info['file_name'])
        
        try:
            # åŠ è½½å›¾åƒ
            image = Image.open(img_path).convert('RGB')
            
            # ç¡®ä¿å›¾åƒæ˜¯æœ‰æ•ˆçš„
            if image.width == 0 or image.height == 0:
                raise ValueError(f"å›¾åƒå°ºå¯¸æ— æ•ˆ: {image.width}x{image.height}")
                
            # å¯¹PILå›¾åƒåº”ç”¨æ•°æ®å¢å¼º
            if self.augment_transform and random.random() > 0.5:
                image = self.augment_transform(image)
            
            # è½¬æ¢ä¸ºå¼ é‡
            image_tensor = self.transform(image)
            
            # å¯¹å¼ é‡åº”ç”¨é¢å¤–å¢å¼º
            if self.tensor_augment and random.random() > 0.5:
                image_tensor = self.tensor_augment(image_tensor)
            
            return {
                'image': image_tensor,
                'image_id': img_info.get('id', idx),
                'annotations': img_info['annotations']
            }
            
        except Exception as e:
            # è¿”å›å¤‡ç”¨å›¾åƒ
            print(f"âš ï¸ å›¾åƒåŠ è½½å¤±è´¥ {img_path}: {e}")
            # åˆ›å»ºä¸€ä¸ªéšæœºå™ªå£°å›¾åƒæ›¿ä»£
            random_noise = torch.rand(3, self.image_size, self.image_size) * 0.1
            fallback_image = torch.zeros(3, self.image_size, self.image_size) + random_noise
            # åº”ç”¨æ ‡å‡†åŒ–ï¼Œä¸æ­£å¸¸å›¾åƒä¸€è‡´
            means = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
            stds = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
            fallback_image = (fallback_image - means) / stds
            
            return {
                'image': fallback_image,
                'image_id': img_info.get('id', idx),
                'annotations': []
            }

def collate_fn(batch):
    """æ‰¹å¤„ç†å‡½æ•°"""
    import torch
    images = torch.stack([item['image'] for item in batch])
    image_ids = [item['image_id'] for item in batch]
    annotations = [item['annotations'] for item in batch]
    
    return {
        'images': images,
        'image_ids': image_ids,
        'annotations': annotations
    }
