# -*- coding: utf-8 -*-
"""
åŸºäºViLDçš„å¼€æ”¾ä¸–ç•Œå®¤å†…ç‰©ä½“æ£€æµ‹

æœ¬é¡¹ç›®å®ç°äº†åŸºäºVision-LanguageçŸ¥è¯†è’¸é¦(ViLD)çš„å¼€æ”¾ä¸–ç•Œå®¤å†…ç‰©ä½“æ£€æµ‹ç³»ç»Ÿã€‚ä¸»è¦ç‰¹ç‚¹ï¼š

1. ä½¿ç”¨RTDETRä½œä¸ºåŸºç¡€æ£€æµ‹å™¨æ¶æ„
2. é›†æˆCLIPé¢„è®­ç»ƒæ¨¡å‹çš„è§†è§‰-è¯­è¨€çŸ¥è¯†
3. é€šè¿‡çŸ¥è¯†è’¸é¦å®ç°å¼€æ”¾è¯æ±‡ç›®æ ‡æ£€æµ‹
4. å¼•å…¥å¯å­¦ä¹ çš„æç¤ºè¯ä¼˜åŒ–åˆ†ç±»æ€§èƒ½
"""

# å¯¼å…¥å¿…è¦çš„åº“
import os
import json
import time
import random
import traceback
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import clip
from PIL import Image
import cv2

# è®¾ç½®matplotlibä¸ºéäº¤äº’æ¨¡å¼ï¼Œé¿å…åœ¨æ— æ˜¾ç¤ºç¯å¢ƒä¸‹å¡ä½
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯ï¼Œé€‚åˆæ— å¤´æœåŠ¡å™¨
import matplotlib.pyplot as plt
import matplotlib.patches as patches

from tqdm import tqdm
# é…ç½®tqdmåœ¨ç»ˆç«¯æ­£ç¡®æ˜¾ç¤º
import sys
tqdm_kwargs = {
    'file': sys.stdout,
    'ncols': 100,
    'ascii': True,  # ä½¿ç”¨ASCIIå­—ç¬¦ï¼Œé¿å…åœ¨æŸäº›ç»ˆç«¯ä¸­æ˜¾ç¤ºé—®é¢˜
    'leave': True   # ä¿ç•™è¿›åº¦æ¡
}
import torch.nn as nn 
from transformers import RTDetrForObjectDetection, RTDetrImageProcessor
import torchvision.transforms as T
import random
from torch.utils.data import Dataset, DataLoader
import time
import gc
from torchvision import transforms

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

print("PyTorchç‰ˆæœ¬:", torch.__version__)
print("CUDAæ˜¯å¦å¯ç”¨:", torch.cuda.is_available())

# è®¾ç½®è®¾å¤‡
device = "cuda" if torch.cuda.is_available() else "cpu"
print("ä½¿ç”¨è®¾å¤‡:", device)

# å…¨å±€æ§åˆ¶å˜é‡
ENABLE_TRAINING = False  # æ§åˆ¶æ˜¯å¦æ‰§è¡Œè®­ç»ƒè¿‡ç¨‹
ENABLE_DETECTION = True  # æ§åˆ¶æ˜¯å¦æ‰§è¡Œæ£€æµ‹è¿‡ç¨‹
TEST_IMAGE_INDEX = -1    # æµ‹è¯•å›¾åƒç´¢å¼•ï¼Œ-1è¡¨ç¤ºéšæœºé€‰æ‹©

# =============================================================================
# 1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
# =============================================================================
"""
æœ¬èŠ‚å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š

1. åŠ è½½COCOæ•°æ®é›†ä¸­çš„å›¾åƒ
2. å¤„ç†å›¾åƒå’Œæ ‡æ³¨æ•°æ®
3. å‡†å¤‡teacheræ¨¡å‹(CLIP)è¾“å…¥
4. å‡†å¤‡studentæ¨¡å‹(RT-DETR)è¾“å…¥
"""

# é…ç½®æ•°æ®è·¯å¾„
# è·å–é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = "/home/cui/vild_rtdetr_indoor"  # ç›´æ¥æŒ‡å®šç»å¯¹è·¯å¾„
print(f"é¡¹ç›®æ ¹ç›®å½•: {PROJECT_ROOT}")

# é…ç½®æ•°æ®é›†è·¯å¾„
COCO_PATH = os.path.join(PROJECT_ROOT, "datasets/indoor_training/annotations_train.json")
IMAGE_ROOT = os.path.join(PROJECT_ROOT, "datasets/indoor_training/train")

def load_coco_indoor():
    """åŠ è½½COCOæ•°æ®é›†ä¸­çš„å®¤å†…åœºæ™¯æ•°æ®"""
    if not os.path.exists(COCO_PATH):
        raise FileNotFoundError(f"æ³¨é‡Šæ–‡ä»¶ä¸å­˜åœ¨: {COCO_PATH}")
        
    print(f"æ­£åœ¨åŠ è½½æ•°æ®é›†: {COCO_PATH}")
    with open(COCO_PATH, 'r') as f:
        dataset = json.load(f)
    
    # æ‰“å°æ•°æ®é›†çš„åŸºæœ¬ä¿¡æ¯ï¼Œå¸®åŠ©è°ƒè¯•
    print(f"æ•°æ®é›†é”®: {list(dataset.keys())}")
    if 'images' in dataset:
        print(f"å›¾åƒæ•°é‡: {len(dataset['images'])}")
        if len(dataset['images']) > 0:
            print(f"ç¬¬ä¸€å¼ å›¾åƒçš„é”®: {list(dataset['images'][0].keys())}")
    if 'categories' in dataset:
        print(f"ç±»åˆ«æ•°é‡: {len(dataset['categories'])}")
    
    # æ„å»ºç±»åˆ«æ˜ å°„
    categories = {cat['id']: cat for cat in dataset['categories']}
    
    # å¤„ç†å›¾åƒå’Œæ ‡æ³¨
    image_dict = {}
    for image in dataset['images']:
        # LVISæ•°æ®é›†ä¸­å¯èƒ½ä½¿ç”¨coco_urlæˆ–file_name
        file_name = None
        
        # å°è¯•ä¸åŒçš„å¯èƒ½é”®å
        if 'file_name' in image:
            file_name = image['file_name']
        elif 'coco_url' in image:
            # ä»coco_urlä¸­æå–æ–‡ä»¶å
            file_name = os.path.basename(image['coco_url'])
        else:
            # æ‰“å°å›¾åƒçš„é”®ä»¥ä¾¿è°ƒè¯•
            print(f"è­¦å‘Š: æ‰¾ä¸åˆ°å›¾åƒè·¯å¾„ï¼Œå›¾åƒå¯¹è±¡çš„é”®: {list(image.keys())}")
            continue
        
        image_dict[image['id']] = {
            'file_name': file_name,
            'height': image.get('height', 0),
            'width': image.get('width', 0),
            'annotations': []
        }
    
    # æ·»åŠ æ ‡æ³¨ä¿¡æ¯
    for ann in dataset['annotations']:
        try:
            image_id = ann['image_id']
            if image_id in image_dict:
                # ç¡®ä¿æ‰€æœ‰å¿…éœ€çš„å­—æ®µéƒ½å­˜åœ¨
                if 'bbox' in ann and 'category_id' in ann:
                    image_dict[image_id]['annotations'].append({
                        'bbox': ann['bbox'],  # [x, y, w, h]
                        'category_id': ann['category_id'],
                        'segmentation': ann.get('segmentation', []),
                        'iscrowd': ann.get('iscrowd', 0)
                    })
        except KeyError as e:
            print(f"è­¦å‘Š: æ ‡æ³¨ç¼ºå°‘å¿…è¦å­—æ®µ {e}")
            continue
    
    # è¿‡æ»¤æ‰æ²¡æœ‰æ ‡æ³¨çš„å›¾åƒ
    valid_images = [img for img in image_dict.values() if len(img['annotations']) > 0]
    print(f"æœ‰æ•ˆå›¾åƒæ•°é‡(å«æ ‡æ³¨): {len(valid_images)}/{len(image_dict)}")
    
    return valid_images, categories

# åŠ è½½æ•°æ®é›†
try:
    print(f"æ­£åœ¨æ£€æŸ¥è·¯å¾„...")
    print(f"COCOæ³¨é‡Šæ–‡ä»¶è·¯å¾„: {COCO_PATH}")
    print(f"å›¾åƒæ ¹ç›®å½•: {IMAGE_ROOT}")
    
    # åˆå§‹åŒ–å˜é‡ï¼Œé˜²æ­¢åŠ è½½å¤±è´¥æ—¶æœªå®šä¹‰
    images = []
    categories = {}
    
    if os.path.exists(COCO_PATH):
        print("æ‰¾åˆ°æ³¨é‡Šæ–‡ä»¶")
        # å°è¯•åŠ è½½æ•°æ®
        try:
            images, categories = load_coco_indoor()
            print(f"æˆåŠŸåŠ è½½äº† {len(images)} å¼ å›¾ç‰‡å’Œ {len(categories)} ä¸ªç±»åˆ«")
            
            # éªŒè¯å›¾åƒè·¯å¾„
            if len(images) > 0:
                sample_path = os.path.join(IMAGE_ROOT, images[0]['file_name'])
                print(f"ç¤ºä¾‹å›¾åƒè·¯å¾„: {sample_path}")
                print(f"å›¾åƒæ–‡ä»¶æ˜¯å¦å­˜åœ¨: {os.path.exists(sample_path)}")
        except Exception as load_error:
            print(f"æ•°æ®åŠ è½½å‡ºé”™: {load_error}")
            print("å°è¯•åˆ‡æ¢åˆ°å…¶ä»–å¯ç”¨æ•°æ®é›†...")
            
            # å°è¯•åŠ è½½COCOæ•°æ®é›†
            alt_coco_path = os.path.join(PROJECT_ROOT, "datasets/coco/train2017")
            if os.path.exists(alt_coco_path):
                print(f"æ‰¾åˆ°COCOæ•°æ®é›†: {alt_coco_path}")
                # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
                import glob
                coco_images = glob.glob(os.path.join(alt_coco_path, "*.jpg"))[:10]
                print(f"æ‰¾åˆ° {len(coco_images)} ä¸ªCOCOå›¾åƒ")
                
                # åˆ›å»ºæ¨¡æ‹Ÿæ ‡æ³¨
                for idx, img_path in enumerate(coco_images):
                    img_name = os.path.basename(img_path)
                    img = cv2.imread(img_path)
                    if img is not None:
                        h, w = img.shape[:2]
                        images.append({
                            'file_name': img_name,
                            'height': h,
                            'width': w,
                            'annotations': [
                                {'bbox': [w//4, h//4, w//2, h//2], 'category_id': 1}
                            ]
                        })
                categories = {1: {'id': 1, 'name': 'object'}}
                print(f"å·²åˆ›å»º {len(images)} ä¸ªæ¨¡æ‹Ÿæ ·æœ¬")
                
                # æ›´æ–°å›¾åƒæ ¹ç›®å½•
                IMAGE_ROOT = alt_coco_path
    else:
        print("æ³¨é‡Šæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•ä½¿ç”¨COCOæ•°æ®é›†...")
        
        # å°è¯•ä½¿ç”¨COCOæ•°æ®é›†
        coco_path = os.path.join(PROJECT_ROOT, "datasets/coco/train2017")
        if os.path.exists(coco_path):
            print(f"æ‰¾åˆ°COCOæ•°æ®é›†: {coco_path}")
            # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
            import glob
            coco_images = glob.glob(os.path.join(coco_path, "*.jpg"))[:10]
            print(f"æ‰¾åˆ° {len(coco_images)} ä¸ªCOCOå›¾åƒ")
            
            # åˆ›å»ºæ¨¡æ‹Ÿæ ‡æ³¨
            for idx, img_path in enumerate(coco_images):
                img_name = os.path.basename(img_path)
                img = cv2.imread(img_path)
                if img is not None:
                    h, w = img.shape[:2]
                    images.append({
                        'file_name': img_name,
                        'height': h,
                        'width': w,
                        'annotations': [
                            {'bbox': [w//4, h//4, w//2, h//2], 'category_id': 1}
                        ]
                    })
            categories = {1: {'id': 1, 'name': 'object'}}
            print(f"å·²åˆ›å»º {len(images)} ä¸ªæ¨¡æ‹Ÿæ ·æœ¬")
            
            # æ›´æ–°å›¾åƒæ ¹ç›®å½•
            IMAGE_ROOT = coco_path
        else:
            print("æ— æ³•æ‰¾åˆ°ä»»ä½•æ•°æ®é›†ï¼Œåˆ›å»ºæ¨¡æ‹Ÿæ•°æ®...")
            # åˆ›å»ºä¸€ä¸ªå‡çš„æ•°æ®é›†ä»¥ä¾¿ä»£ç èƒ½ç»§ç»­è¿è¡Œ
            images = []
            categories = {1: {'id': 1, 'name': 'object'}}
        
except Exception as e:
    print(f"åŠ è½½æ•°æ®é›†æ—¶å‡ºé”™: {str(e)}")
    print(f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
    print(f"æ•°æ®é›†è·¯å¾„: {COCO_PATH}")
    import traceback
    traceback.print_exc()
    
    # ç¡®ä¿å˜é‡å·²å®šä¹‰
    images = []
    categories = {1: {'id': 1, 'name': 'object'}}

# åŠ è½½CLIPæ¨¡å‹
clip_model, clip_preprocess = clip.load('ViT-B/32', device)
clip_model.eval()

# è®¾ç½®é»˜è®¤æµ®ç‚¹ç±»å‹ï¼Œé¿å…åŠç²¾åº¦é—®é¢˜
torch.set_default_dtype(torch.float32)
print("å·²è®¾ç½®é»˜è®¤æ•°æ®ç±»å‹ä¸º float32ï¼Œé¿å…åŠç²¾åº¦é—®é¢˜")

# åŠ è½½RT-DETRæ£€æµ‹å™¨
try:
    image_processor = RTDetrImageProcessor.from_pretrained("PekingU/rtdetr_r50vd_coco_o365")
    detector_model = RTDetrForObjectDetection.from_pretrained("PekingU/rtdetr_r50vd_coco_o365").to(device)
    detector_model.eval()
    print("æˆåŠŸåŠ è½½RT-DETRæ¨¡å‹")
except Exception as e:
    print(f"åŠ è½½RT-DETRå¤±è´¥: {str(e)}")

class ImageProcessor:
    def __init__(self, clip_preprocess):
        self.clip_preprocess = clip_preprocess
    
    def prepare_image_clip(self, image_path):
        """å¤„ç†å›¾åƒç”¨äºCLIPæ¨¡å‹"""
        image = Image.open(image_path).convert('RGB')
        return self.clip_preprocess(image).unsqueeze(0).to(device)
    
    def prepare_image_detector(self, image_path):
        """å¤„ç†å›¾åƒç”¨äºæ£€æµ‹å™¨"""
        image = cv2.imread(image_path)
        return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# ä»æ•°æ®é›†ä¸­é€‰æ‹©éšæœºæµ‹è¯•å›¾åƒçš„å‡½æ•°
def select_random_test_image():
    """ä»æ•°æ®é›†ä¸­é€‰æ‹©ä¸€ä¸ªéšæœºæµ‹è¯•å›¾åƒ"""
    if len(images) == 0:
        return None
    
    # å¦‚æœæŒ‡å®šäº†æµ‹è¯•å›¾åƒç´¢å¼•ï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™éšæœºé€‰æ‹©
    if TEST_IMAGE_INDEX >= 0 and TEST_IMAGE_INDEX < len(images):
        img_index = TEST_IMAGE_INDEX
    else:
        # éšæœºé€‰æ‹©ä¸€ä¸ªå›¾åƒ
        img_index = random.randint(0, len(images) - 1)
    
    img_info = images[img_index]
    img_path = os.path.join(IMAGE_ROOT, img_info['file_name'])
    
    if os.path.exists(img_path):
        print(f"ğŸ“· é€‰æ‹©æµ‹è¯•å›¾åƒ: {os.path.basename(img_path)} (ç´¢å¼• {img_index})")
        return img_path
    else:
        print(f"âš ï¸ é€‰æ‹©çš„å›¾åƒä¸å­˜åœ¨: {img_path}")
        return None

# åˆå§‹åŒ–å›¾åƒå¤„ç†å™¨
processor = ImageProcessor(clip_preprocess)

# æµ‹è¯•å›¾åƒå¤„ç†
test_image_path = select_random_test_image()
if test_image_path:
    print(f"æ‰¾åˆ°æœ‰æ•ˆçš„æµ‹è¯•å›¾åƒ: {test_image_path}")
else:
    print("è­¦å‘Š: æœªæ‰¾åˆ°æœ‰æ•ˆçš„æµ‹è¯•å›¾åƒ")
    test_image_path = None

# å¦‚æœæ²¡æœ‰æ‰¾åˆ°å›¾åƒï¼Œåˆ›å»ºä¸€ä¸ªæµ‹è¯•å›¾åƒ
if test_image_path is None:
    print("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆå›¾åƒï¼Œåˆ›å»ºæµ‹è¯•å›¾åƒ...")
    test_dir = os.path.join(PROJECT_ROOT, "tests")
    os.makedirs(test_dir, exist_ok=True)
    test_image_path = os.path.join(test_dir, "test_image.jpg")
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•å›¾åƒ
    test_image = np.ones((480, 640, 3), dtype=np.uint8) * 200
    # ç»˜åˆ¶ä¸€äº›ç®€å•çš„å½¢çŠ¶
    cv2.rectangle(test_image, (100, 100), (300, 300), (0, 0, 255), 2)
    cv2.circle(test_image, (400, 200), 50, (0, 255, 0), -1)
    cv2.imwrite(test_image_path, test_image)
    print(f"å·²åˆ›å»ºæµ‹è¯•å›¾åƒ: {test_image_path}")
        
if test_image_path:
    try:
        clip_input = processor.prepare_image_clip(test_image_path)
        detector_input = processor.prepare_image_detector(test_image_path)
        print("CLIPè¾“å…¥å¼ é‡å½¢çŠ¶:", clip_input.shape)
        print("æ£€æµ‹å™¨è¾“å…¥å›¾åƒå½¢çŠ¶:", detector_input.shape)
    except Exception as e:
        print(f"å¤„ç†æµ‹è¯•å›¾åƒæ—¶å‡ºé”™: {e}")
else:
    print("æ— æ³•åˆ›å»ºæˆ–æ‰¾åˆ°ä»»ä½•å›¾åƒæ•°æ®")

# =============================================================================
# 2. æ¨¡å‹æ¶æ„å®šä¹‰
# =============================================================================
"""
æœ¬èŠ‚å®ç°ä»¥ä¸‹ç»„ä»¶ï¼š

1. åŸºäºRT-DETRçš„æ£€æµ‹å™¨æ¶æ„
2. é›†æˆCLIPè§†è§‰ç¼–ç å™¨
3. ç‰¹å¾æŠ•å½±å±‚
4. çŸ¥è¯†è’¸é¦çš„æŸå¤±å‡½æ•°
"""

# å®šä¹‰ViLDæ¨¡å‹
class ViLDModel(nn.Module):
    def __init__(self, clip_model, detector_model):
        super().__init__()
        self.clip_model = clip_model
        self.detector_model = detector_model
        
        # å†»ç»“CLIPæ¨¡å‹å‚æ•°
        for param in self.clip_model.parameters():
            param.requires_grad = False
            
        # ç‰¹å¾èåˆå±‚
        self.fusion_layer = nn.Linear(512, 256)  # å‡è®¾CLIPè¾“å‡º512ç»´ï¼Œæ£€æµ‹å™¨ç‰¹å¾256ç»´
        
        # å¤šå°ºåº¦ç‰¹å¾æŠ•å½±å™¨
        self.projectors = nn.ModuleList([
            nn.Sequential(
                nn.Linear(256, 1024),
                nn.LayerNorm(1024),
                nn.ReLU(),
                nn.Linear(1024, 512)
            ) for _ in range(4)  # å¯¹åº”RT-DETRçš„4ä¸ªç‰¹å¾å°ºåº¦
        ])
        
    def forward(self, images):
        # ä½¿ç”¨æ£€æµ‹å™¨è·å–åŒºåŸŸç‰¹å¾
        detector_inputs = image_processor(images=images, return_tensors="pt").to(device)
        detector_outputs = self.detector_model(**detector_inputs, output_hidden_states=True)
        
        # è·å–å¤šå°ºåº¦ç‰¹å¾ï¼ˆå–æœ€å4å±‚çš„[CLS] tokenï¼‰
        features = [h[:, 0] for h in detector_outputs.hidden_states[-4:]]
        
        # æŠ•å½±ç‰¹å¾
        projected_features = [proj(feat) for proj, feat in zip(self.projectors, features)]
        
        # ä½¿ç”¨CLIPè·å–å…¨å±€ç‰¹å¾
        clip_inputs = torch.stack([clip_preprocess(img) for img in images]).to(device)
        clip_features = self.clip_model.encode_image(clip_inputs)
        
        # ç‰¹å¾èåˆ
        fused_features = self.fusion_layer(clip_features)
        
        return {
            "detector_outputs": detector_outputs,
            "clip_features": clip_features,
            "fused_features": fused_features
        }

# åˆå§‹åŒ–ViLDæ¨¡å‹
try:
    vild_model = ViLDModel(clip_model, detector_model).to(device)
    print("ViLDæ¨¡å‹æ„å»ºæˆåŠŸ")
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    print(f"è®¾å¤‡: {device}")
    print(f"CLIPæ¨¡å‹: ViT-B/32")
    print(f"æ£€æµ‹å™¨æ¨¡å‹: {type(detector_model).__name__}")
    print(f"èåˆå±‚ç»“æ„: {vild_model.fusion_layer}")
    
except Exception as e:
    print(f"æ¨¡å‹æ„å»ºå¤±è´¥: {str(e)}")

# =============================================================================
# 3. çŸ¥è¯†è’¸é¦è®­ç»ƒ
# =============================================================================
"""
æœ¬èŠ‚å®ç°ä¼˜åŒ–åçš„çŸ¥è¯†è’¸é¦è®­ç»ƒæµç¨‹ï¼Œç‰¹åˆ«å…³æ³¨è®­ç»ƒç¨³å®šæ€§ï¼š

1. **ç¨³å®šçš„ç‰¹å¾æå–**
   - ä½¿ç”¨LayerNormä»£æ›¿BatchNorm
   - æ·»åŠ æ®‹å·®è¿æ¥æé«˜ç‰¹å¾ä¼ æ’­ç¨³å®šæ€§
   - ä½¿ç”¨GELUæ¿€æ´»å‡½æ•°è·å¾—æ›´å¹³æ»‘çš„æ¢¯åº¦

2. **æ”¹è¿›çš„æŸå¤±è®¡ç®—**
   - ä½¿ç”¨æŸå¤±å¹³æ»‘(Loss Smoothing)é˜²æ­¢è¿‡æ‹Ÿåˆ
   - æ·»åŠ ä½™å¼¦ç›¸ä¼¼åº¦ä¸L1æŸå¤±çš„ç»„åˆ
   - åº”ç”¨æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸

3. **ä¼˜åŒ–çš„å­¦ä¹ è°ƒåº¦**
   - å®ç°OneCycleLRå­¦ä¹ ç‡è°ƒåº¦
   - åŒ…å«é¢„çƒ­é˜¶æ®µå‡å°‘åˆå§‹ä¸ç¨³å®šæ€§
   - ä½¿ç”¨EMA(æŒ‡æ•°ç§»åŠ¨å¹³å‡)å¹³æ»‘è®­ç»ƒæ›²çº¿

4. **ç¨³å¥çš„è®­ç»ƒç›‘æ§**
   - åŒæ—¶è·Ÿè¸ªåŸå§‹æŸå¤±å’Œå¹³æ»‘æŸå¤±
   - æ—©åœæœºåˆ¶é¿å…è¿‡æ‹Ÿåˆ
   - åŠ¨æ€å¯è§†åŒ–æŸå¤±å˜åŒ–æ›²çº¿
"""

# æŸå¤±è¿½è¸ªå™¨
class LossTracker:
    """æŸå¤±è¿½è¸ªå’Œå¯è§†åŒ–"""
    
    def __init__(self):
        self.train_losses = []
        self.epoch_losses = []
        self.best_loss = float('inf')
        self.best_epoch = 0
        
    def update(self, epoch_loss, epoch):
        """æ›´æ–°æŸå¤±è®°å½•"""
        self.epoch_losses.append(epoch_loss)
        if epoch_loss < self.best_loss:
            self.best_loss = epoch_loss
            self.best_epoch = epoch
            
    def plot_losses(self, save_path=None, train_losses=None, val_losses=None, lr_history=None):
        """ç»˜åˆ¶å¢å¼ºç‰ˆæŸå¤±æ›²çº¿å’Œå­¦ä¹ ç‡"""
        plt.figure(figsize=(15, 10))
        
        # åˆ›å»ºå¤šå­å›¾
        gs = plt.GridSpec(2, 2, height_ratios=[2, 1])
        ax1 = plt.subplot(gs[0, :])  # ä¸Šæ–¹å ä¸¤åˆ—çš„æŸå¤±å›¾
        ax2 = plt.subplot(gs[1, 0])  # å·¦ä¸‹è§’çš„è®­ç»ƒ/éªŒè¯æŸå¤±å¯¹æ¯”
        ax3 = plt.subplot(gs[1, 1])  # å³ä¸‹è§’çš„å­¦ä¹ ç‡æ›²çº¿
        
        # 1. ä¸»æŸå¤±æ›²çº¿ (ä¸Šæ–¹å¤§å›¾)
        epochs = range(1, len(self.epoch_losses) + 1)
        ax1.plot(epochs, self.epoch_losses, 'b-', linewidth=2.5, label='Validation Loss', marker='o')
        
        # æ ‡æ³¨æœ€ä½³æŸå¤±ç‚¹
        ax1.plot(self.best_epoch + 1, self.best_loss, 'r*', markersize=20, 
                label=f'Best Loss: {self.best_loss:.4f} (Epoch {self.best_epoch + 1})')
        
        # æ·»åŠ ç§»åŠ¨å¹³å‡çº¿
        if len(self.epoch_losses) >= 3:
            window_size = min(3, len(self.epoch_losses))
            moving_avg = []
            for i in range(len(self.epoch_losses)):
                start_idx = max(0, i - window_size + 1)
                moving_avg.append(np.mean(self.epoch_losses[start_idx:i+1]))
            ax1.plot(epochs, moving_avg, 'g--', linewidth=2, alpha=0.7, label='Moving Average')
        
        # è®¾ç½®ä¸»å›¾çš„æ ·å¼
        ax1.set_xlabel('Epoch', fontsize=14)
        ax1.set_ylabel('Loss Value', fontsize=14)
        ax1.set_title('Validation Loss Curve', fontsize=16, fontweight='bold')
        ax1.legend(fontsize=12, loc='upper right')
        ax1.grid(True, alpha=0.3)
        
        # çªå‡ºæ˜¾ç¤ºæ”¹è¿›åŒºåŸŸ
        if len(self.epoch_losses) > 1:
            # æ‰¾å‡ºæŸå¤±ä¸‹é™çš„åŒºåŸŸ
            improvements = []
            for i in range(1, len(self.epoch_losses)):
                if self.epoch_losses[i] < self.epoch_losses[i-1]:
                    improvements.append(i)
            
            # ä¸ºæ”¹è¿›åŒºåŸŸæ·»åŠ èƒŒæ™¯
            for i in improvements:
                ax1.axvspan(i, i+1, alpha=0.1, color='green')
                
            # æ ‡æ³¨æ€»ä½“æ”¹è¿›
            if improvements:
                total_improvement = self.epoch_losses[0] - min(self.epoch_losses)
                ax1.text(0.02, 0.95, f"æ€»æ”¹è¿›: {total_improvement:.4f}", 
                        transform=ax1.transAxes, fontsize=12, 
                        bbox=dict(boxstyle='round,pad=0.5', facecolor='lightyellow', alpha=0.5))
        
        # 2. è®­ç»ƒ/éªŒè¯æŸå¤±å¯¹æ¯” (å·¦ä¸‹è§’)
        if train_losses and val_losses and len(train_losses) == len(val_losses):
            train_epochs = range(1, len(train_losses) + 1)
            ax2.plot(train_epochs, train_losses, 'b-', linewidth=2, label='Training')
            ax2.plot(train_epochs, val_losses, 'r-', linewidth=2, label='Validation')
            ax2.set_title('Training vs Validation Loss', fontsize=12)
            ax2.set_xlabel('Epoch', fontsize=10)
            ax2.set_ylabel('Loss', fontsize=10)
            ax2.legend(fontsize=10)
            ax2.grid(True, alpha=0.3)
            
            # è®¡ç®—è®­ç»ƒ/éªŒè¯æŸå¤±ä¹‹é—´çš„å·®è·
            if len(train_losses) > 0:
                gap = np.mean([t-v for t, v in zip(train_losses, val_losses)])
                ax2.text(0.05, 0.95, f"å¹³å‡é—´éš”: {gap:.4f}", transform=ax2.transAxes, 
                        fontsize=10, bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))
        else:
            ax2.text(0.5, 0.5, "è®­ç»ƒ/éªŒè¯æŸå¤±æ•°æ®ä¸å¯ç”¨", 
                    ha='center', va='center', transform=ax2.transAxes)
        
        # 3. å­¦ä¹ ç‡æ›²çº¿ (å³ä¸‹è§’)
        if lr_history and len(lr_history) > 0:
            lr_epochs = range(1, len(lr_history) + 1)
            ax3.plot(lr_epochs, lr_history, 'g-', linewidth=2)
            ax3.set_title('Learning Rate Schedule', fontsize=12)
            ax3.set_xlabel('Epoch', fontsize=10)
            ax3.set_ylabel('Learning Rate', fontsize=10)
            ax3.grid(True, alpha=0.3)
            
            # ä½¿ç”¨ç§‘å­¦è®¡æ•°æ³•
            ax3.yaxis.set_major_formatter(plt.FormatStrFormatter('%.0e'))
        else:
            ax3.text(0.5, 0.5, "å­¦ä¹ ç‡æ•°æ®ä¸å¯ç”¨", 
                    ha='center', va='center', transform=ax3.transAxes)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š Loss plot saved to: {save_path}")
        
        # åœ¨æ— å¤´ç¯å¢ƒä¸­é¿å…æ˜¾ç¤ºï¼Œé˜²æ­¢ç¨‹åºå¡ä½
        try:
            # è®¾ç½®è¶…æ—¶ï¼Œé¿å…åœ¨æ²¡æœ‰æ˜¾ç¤ºç¯å¢ƒæ—¶å¡ä½
            plt.show(block=False)
            plt.pause(1)
            plt.close()
        except Exception as e:
            print(f"æ³¨æ„: å›¾å½¢æ˜¾ç¤ºè¢«è·³è¿‡ ({str(e)})")
            plt.close('all')
        
        # æ‰“å°è®­ç»ƒç»Ÿè®¡
        print(f"\nğŸ“ˆ Training Statistics:")
        print(f"   Total Epochs: {len(self.epoch_losses)}")
        print(f"   Best Loss: {self.best_loss:.6f}")
        print(f"   Best Epoch: {self.best_epoch + 1}")
        print(f"   Final Loss: {self.epoch_losses[-1]:.6f}")
        if len(self.epoch_losses) >= 2:
            improvement = self.epoch_losses[0] - self.epoch_losses[-1]
            print(f"   Total Improvement: {improvement:.6f}")

# æ—©åœæ£€æŸ¥å™¨
class EarlyStopping:
    """æ—©åœæ£€æŸ¥å™¨ - è¿ç»­5ä¸ªepochæ— æ”¹å–„åˆ™åœæ­¢"""
    
    def __init__(self, patience=5, min_delta=1e-5):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = float('inf')
        
    def __call__(self, val_loss):
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            return False
        else:
            self.counter += 1
            return self.counter >= self.patience

# GPUä¼˜åŒ–è®¾ç½®
def setup_gpu_optimization():
    """è®¾ç½®GPUä¼˜åŒ–"""
    if torch.cuda.is_available():
        # å¯ç”¨TF32ä»¥æé«˜A100æ€§èƒ½
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        
        # è®¾ç½®å†…å­˜ä¼˜åŒ–
        torch.cuda.empty_cache()
        
        # æ˜¾ç¤ºGPUä¿¡æ¯
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"ğŸš€ GPUä¼˜åŒ–è®¾ç½®:")
        print(f"   GPUè®¾å¤‡: {gpu_name}")
        print(f"   æ€»æ˜¾å­˜: {gpu_memory:.1f} GB")
        print(f"   TF32ä¼˜åŒ–: å·²å¯ç”¨")
        return True
    else:
        print("âŒ CUDAä¸å¯ç”¨")
        return False

# æ”¹è¿›çš„å®¤å†…æ•°æ®é›†
class ImprovedCOCOIndoorDataset(Dataset):
    """æ”¹è¿›çš„COCOå®¤å†…æ•°æ®é›†"""
    
    def __init__(self, images_data, image_root, image_size=256, augment=True, max_samples=None):
        self.images_data = images_data
        self.image_root = image_root
        self.image_size = image_size
        self.augment = augment
        
        # è¿‡æ»¤æœ‰æ•ˆå›¾åƒ
        self.valid_images = []
        for img_info in images_data:
            img_path = os.path.join(image_root, img_info['file_name'])
            if os.path.exists(img_path) and len(img_info['annotations']) > 0:
                # é¢å¤–æ£€æŸ¥å›¾åƒæ˜¯å¦å¯ä»¥æ­£ç¡®æ‰“å¼€
                try:
                    with Image.open(img_path) as img:
                        if img.width > 0 and img.height > 0:
                            self.valid_images.append(img_info)
                except Exception as e:
                    print(f"âš ï¸ å›¾åƒæ–‡ä»¶æ— æ•ˆï¼Œè·³è¿‡: {img_path} ({e})")
        
        # é™åˆ¶æ ·æœ¬æ•°é‡ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if max_samples and len(self.valid_images) > max_samples:
            self.valid_images = random.sample(self.valid_images, max_samples)
        
        # åˆ†ç¦»è½¬æ¢ï¼Œå°†RandomErasingç§»åˆ°tensorè½¬æ¢ååº”ç”¨
        # åŸºæœ¬è½¬æ¢
        self.transform = transforms.Compose([
            transforms.Resize((image_size, image_size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        # å¼ é‡å¢å¼º (åœ¨ToTensorä¹‹ååº”ç”¨)
        self.tensor_augment = None
        if augment:
            self.tensor_augment = transforms.Compose([
                transforms.RandomErasing(p=0.3, scale=(0.02, 0.2), ratio=(0.3, 3.3))
            ])
            
        # PILå›¾åƒå¢å¼º (åœ¨ToTensorä¹‹å‰åº”ç”¨)
        if augment:
            self.augment_transform = transforms.Compose([
                transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0), ratio=(0.75, 1.3333)),
                transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),
                transforms.RandomHorizontalFlip(p=0.5),
                transforms.RandomRotation(degrees=15),
                transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),
                transforms.RandomPerspective(distortion_scale=0.2, p=0.5),
                transforms.RandomGrayscale(p=0.1)
            ])
        else:
            self.augment_transform = None
        
        print(f"ğŸ“Š æ•°æ®é›†åˆå§‹åŒ–å®Œæˆ:")
        print(f"   æœ‰æ•ˆå›¾åƒ: {len(self.valid_images)}")
        print(f"   å›¾åƒå¤§å°: {image_size}")
        print(f"   æ•°æ®å¢å¼º: {augment}")
    
    def __len__(self):
        return len(self.valid_images)
    
    def __getitem__(self, idx):
        img_info = self.valid_images[idx]
        img_path = os.path.join(self.image_root, img_info['file_name'])
        
        try:
            # åŠ è½½å›¾åƒ
            image = Image.open(img_path).convert('RGB')
            
            # ç¡®ä¿å›¾åƒæ˜¯æœ‰æ•ˆçš„
            if image.width == 0 or image.height == 0:
                raise ValueError(f"å›¾åƒå°ºå¯¸æ— æ•ˆ: {image.width}x{image.height}")
                
            # å¯¹PILå›¾åƒåº”ç”¨æ•°æ®å¢å¼º
            if self.augment_transform and random.random() > 0.5:
                image = self.augment_transform(image)
            
            # è½¬æ¢ä¸ºå¼ é‡
            image_tensor = self.transform(image)
            
            # å¯¹å¼ é‡åº”ç”¨é¢å¤–å¢å¼º
            if self.tensor_augment and random.random() > 0.5:
                image_tensor = self.tensor_augment(image_tensor)
            
            return {
                'image': image_tensor,
                'image_id': img_info.get('id', idx),
                'annotations': img_info['annotations']
            }
            
        except Exception as e:
            # è¿”å›é»‘è‰²å›¾åƒä½œä¸ºfallback
            print(f"âš ï¸ å›¾åƒåŠ è½½å¤±è´¥ {img_path}: {e}")
            # åˆ›å»ºä¸€ä¸ªéšæœºå™ªå£°å›¾åƒæ›¿ä»£çº¯é»‘è‰²ï¼Œé¿å…æ¨¡å‹è¿‡æ‹Ÿåˆäºé»‘è‰²å›¾åƒ
            random_noise = torch.rand(3, self.image_size, self.image_size) * 0.1
            fallback_image = torch.zeros(3, self.image_size, self.image_size) + random_noise
            # åº”ç”¨æ ‡å‡†åŒ–ï¼Œä¸æ­£å¸¸å›¾åƒä¸€è‡´
            means = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
            stds = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
            fallback_image = (fallback_image - means) / stds
            
            return {
                'image': fallback_image,
                'image_id': img_info.get('id', idx),
                'annotations': []
            }

def collate_fn(batch):
    """æ‰¹å¤„ç†å‡½æ•°"""
    images = torch.stack([item['image'] for item in batch])
    image_ids = [item['image_id'] for item in batch]
    annotations = [item['annotations'] for item in batch]
    
    return {
        'images': images,
        'image_ids': image_ids,
        'annotations': annotations
    }

# ä¿®å¤ç‰ˆç¨³å®šè®­ç»ƒå™¨ - è§£å†³è®¡ç®—å›¾é‡å¤ä½¿ç”¨é—®é¢˜
class FixedStableTrainer:
    """ä¿®å¤ç‰ˆç¨³å®šè®­ç»ƒå™¨ - è§£å†³è®¡ç®—å›¾é—®é¢˜"""
    
    def __init__(self, clip_model, detector_model, image_processor, clip_preprocess, device):
        self.clip_model = clip_model
        self.detector_model = detector_model
        self.image_processor = image_processor
        self.clip_preprocess = clip_preprocess
        self.device = device
        
        # åˆ›å»ºè½»é‡çº§æŠ•å½±å™¨ï¼ˆé™ä½å¤æ‚åº¦ï¼‰
        self.visual_projector = self.create_lightweight_projector().to(device)
        self.text_projector = self.create_lightweight_projector().to(device)
        
        # ä½¿ç”¨æ’ç­‰æ˜ å°„åˆå§‹åŒ–
        self.initialize_as_identity()
        
        # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
        self.visual_projector.train()
        self.text_projector.train()
        
        print("ğŸ¯ ä¼˜åŒ–ç‰ˆç¨³å®šè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   è§†è§‰æŠ•å½±å™¨å‚æ•°: {sum(p.numel() for p in self.visual_projector.parameters()):,}")
        print(f"   æ–‡æœ¬æŠ•å½±å™¨å‚æ•°: {sum(p.numel() for p in self.text_projector.parameters()):,}")
        print(f"   ä½¿ç”¨æ··åˆç²¾åº¦: {'æ˜¯' if torch.cuda.is_available() else 'å¦ (ä»…CPU)'}")
    
    def create_lightweight_projector(self):
        """åˆ›å»ºç®€åŒ–ç‰ˆå¤šå±‚æŠ•å½±å™¨ï¼Œæ›´æ˜“äºåˆå§‹åŒ–"""
        # å®šä¹‰æ®‹å·®å—
        class ResidualBlock(nn.Module):
            def __init__(self, dim):
                super().__init__()
                self.layer_norm1 = nn.LayerNorm(dim)
                self.fc1 = nn.Linear(dim, dim * 2, dtype=torch.float32)
                self.gelu = nn.GELU()
                self.fc2 = nn.Linear(dim * 2, dim, dtype=torch.float32)
                self.dropout = nn.Dropout(0.1)
                
            def forward(self, x):
                residual = x
                x = self.layer_norm1(x)
                x = self.fc1(x)
                x = self.gelu(x)
                x = self.fc2(x)
                x = self.dropout(x)
                return x + residual  # æ®‹å·®è¿æ¥
        
        # ä½¿ç”¨æ›´ç®€å•çš„æŠ•å½±å™¨ç»“æ„ï¼Œæ˜“äºåˆå§‹åŒ–
        module = nn.Sequential(
            nn.Linear(512, 512, bias=True, dtype=torch.float32),  # é¦–å±‚ - å°†è¢«åˆå§‹åŒ–ä¸ºæ’ç­‰æ˜ å°„
            nn.GELU(),
            nn.Linear(512, 512, bias=True, dtype=torch.float32)   # è¾“å‡ºå±‚
        )
        
        # ç¡®ä¿æ‰€æœ‰æƒé‡éƒ½æ˜¯float32
        for param in module.parameters():
            param.data = param.data.float()
            
        # ä½¿ç”¨é»˜è®¤åˆå§‹åŒ–
        # é¦–å±‚å’Œè¾“å‡ºå±‚ä¼šåœ¨initialize_as_identityä¸­ä¸“é—¨åˆå§‹åŒ–
                    
        return module
    
    def initialize_as_identity(self):
        """åˆå§‹åŒ–æŠ•å½±å™¨ä¸ºæ¥è¿‘æ’ç­‰æ˜ å°„ - é€‚åº”æ–°çš„ç½‘ç»œç»“æ„"""
        with torch.no_grad():
            # è§†è§‰æŠ•å½±å™¨ - é¦–å±‚åˆå§‹åŒ–ä¸ºæ¥è¿‘æ’ç­‰æ˜ å°„
            if hasattr(self.visual_projector[0], 'weight'):
                torch.nn.init.eye_(self.visual_projector[0].weight)
                if hasattr(self.visual_projector[0], 'bias') and self.visual_projector[0].bias is not None:
                    torch.nn.init.zeros_(self.visual_projector[0].bias)
            
            # æ–‡æœ¬æŠ•å½±å™¨ - é¦–å±‚åˆå§‹åŒ–ä¸ºæ¥è¿‘æ’ç­‰æ˜ å°„
            if hasattr(self.text_projector[0], 'weight'):
                torch.nn.init.eye_(self.text_projector[0].weight)
                if hasattr(self.text_projector[0], 'bias') and self.text_projector[0].bias is not None:
                    torch.nn.init.zeros_(self.text_projector[0].bias)
            
            # æ®‹å·®å—çš„åˆå§‹åŒ– - è®¾ç½®å°æƒé‡ä½¿æ®‹å·®å˜å°
            for module in self.visual_projector.modules():
                if isinstance(module, nn.Linear):
                    if module != self.visual_projector[0]:  # è·³è¿‡å·²åˆå§‹åŒ–çš„é¦–å±‚
                        torch.nn.init.xavier_normal_(module.weight, gain=0.5)
                        if hasattr(module, 'bias') and module.bias is not None:
                            torch.nn.init.zeros_(module.bias)
            
            for module in self.text_projector.modules():
                if isinstance(module, nn.Linear):
                    if module != self.text_projector[0]:  # è·³è¿‡å·²åˆå§‹åŒ–çš„é¦–å±‚
                        torch.nn.init.xavier_normal_(module.weight, gain=0.5)
                        if hasattr(module, 'bias') and module.bias is not None:
                            torch.nn.init.zeros_(module.bias)
        
        print("âœ… æŠ•å½±å™¨å·²åˆå§‹åŒ–ä¸ºä¼˜åŒ–ç‰ˆæ’ç­‰æ˜ å°„")
    
    def get_trainable_parameters(self):
        """è·å–å¯è®­ç»ƒå‚æ•°"""
        params = []
        params.extend(self.visual_projector.parameters())
        params.extend(self.text_projector.parameters())
        return params
    
    def compute_distillation_loss(self, visual_features, text_features, temperature=0.05):
        """è®¡ç®—å¢å¼ºç‰ˆçš„çŸ¥è¯†è’¸é¦æŸå¤±"""
        # L2å½’ä¸€åŒ–
        visual_features = F.normalize(visual_features, p=2, dim=1)
        text_features = F.normalize(text_features, p=2, dim=1)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µï¼ˆä½¿ç”¨æ›´ä½çš„æ¸©åº¦ç³»æ•°å¢å¼ºå¯¹æ¯”åº¦ï¼‰
        similarity_matrix = torch.mm(visual_features, text_features.t()) / temperature
        
        # å¯¹è§’çº¿æŸå¤±ï¼ˆè‡ªç›¸ä¼¼ï¼‰
        batch_size = visual_features.size(0)
        targets = torch.arange(batch_size).to(self.device)
        
        # å¦‚æœæ–‡æœ¬ç‰¹å¾æ•°é‡ä¸å¤Ÿï¼Œä½¿ç”¨å¾ªç¯ç´¢å¼•
        if text_features.size(0) < batch_size:
            targets = targets % text_features.size(0)
        
        # å¢åŠ æ ‡ç­¾å¹³æ»‘ï¼Œè¿›ä¸€æ­¥æå‡è®­ç»ƒç¨³å®šæ€§
        label_smoothing = 0.2
        loss_v2t = F.cross_entropy(similarity_matrix, targets, label_smoothing=label_smoothing)
        loss_t2v = F.cross_entropy(similarity_matrix.t(), targets[:text_features.size(0)], label_smoothing=label_smoothing)
        
        # InfoNCEå¯¹æ¯”æŸå¤±
        logits_per_image = similarity_matrix
        logits_per_text = similarity_matrix.t()
        
        # æ·»åŠ ç¡¬è´Ÿæ ·æœ¬æŒ–æ˜ - æ‰¾å‡ºæœ€å…·æŒ‘æˆ˜æ€§çš„è´Ÿæ ·æœ¬
        with torch.no_grad():
            # åˆ›å»ºè´Ÿæ ·æœ¬æ©ç 
            negative_mask = torch.ones_like(similarity_matrix)
            negative_mask.fill_diagonal_(0)  # å¯¹è§’çº¿ä¸ºæ­£æ ·æœ¬
            
            # è·å–æ¯è¡Œ/åˆ—æœ€é«˜çš„è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦
            hardest_negatives_per_img = (similarity_matrix * negative_mask).max(dim=1)[0]
            hardest_negatives_per_txt = (similarity_matrix.t() * negative_mask.t()).max(dim=1)[0]
        
        # å¤§å¹…å‡å°ç¡¬è´Ÿæ ·æœ¬æƒé‡ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§
        hard_neg_weight = 0.2  # è¿›ä¸€æ­¥é™ä½ç¡¬è´Ÿæ ·æœ¬æƒé‡ï¼Œé˜²æ­¢è®­ç»ƒä¸ç¨³å®š
        img_to_txt_loss = F.cross_entropy(similarity_matrix, targets) + \
                         hard_neg_weight * torch.mean(hardest_negatives_per_img)
        txt_to_img_loss = F.cross_entropy(similarity_matrix.t(), targets[:text_features.size(0)]) + \
                         hard_neg_weight * torch.mean(hardest_negatives_per_txt)
        
        # æ€»å¯¹æ¯”æŸå¤±
        contrastive_loss = (img_to_txt_loss + txt_to_img_loss) / 2
        
        # ç‰¹å¾å¯¹é½æŸå¤± - é¼“åŠ±ç›¸ä¼¼çš„å›¾åƒ-æ–‡æœ¬å¯¹åœ¨ç‰¹å¾ç©ºé—´ä¸­æ¥è¿‘
        alignment_loss = torch.diagonal(1 - similarity_matrix).mean()
        
        # ç‰¹å¾å‡åŒ€æ€§æŸå¤± - é¼“åŠ±ç‰¹å¾ç©ºé—´å‡åŒ€åˆ†å¸ƒ
        uniformity_loss = torch.log(torch.exp(torch.mm(visual_features, visual_features.t()) / temperature).mean())
        
        # æ·»åŠ L2æ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ (å‡å°æ­£åˆ™åŒ–ç³»æ•°)
        l2_reg = 0.0005 * (
            torch.norm(self.visual_projector[0].weight, p=2) +
            torch.norm(self.text_projector[0].weight, p=2)
        )
        
        # é‡æ–°å¹³è¡¡æŸå¤±æƒé‡ï¼Œå¤§å¹…å‡å°‘å¯¹æ¯”æŸå¤±æƒé‡ï¼Œå¢åŠ æ ‡å‡†äº¤å‰ç†µæŸå¤±æƒé‡
        total_loss = 0.3 * contrastive_loss + \
                    0.1 * alignment_loss + \
                    0.1 * uniformity_loss + \
                    0.4 * (loss_v2t + loss_t2v) / 2 + \
                    0.1 * l2_reg  # å¤§å¹…å‡å°L2æ­£åˆ™æƒé‡ï¼Œé¿å…è¿‡åº¦çº¦æŸ
        
        return total_loss
    
    def encode_text_features_batch(self, categories, batch_size):
        """ä¸ºæ¯ä¸ªæ‰¹æ¬¡é‡æ–°ç¼–ç æ–‡æœ¬ç‰¹å¾ - é¿å…è®¡ç®—å›¾é‡å¤ä½¿ç”¨"""
        all_text_features = []
        templates = ["a {}", "indoor {}", "a {} in a room"]
        
        for category in categories:
            category_features = []
            
            for template in templates:
                text = template.format(category)
                text_tokens = clip.tokenize([text]).to(self.device)
                
                # é‡è¦ï¼šæ¯æ¬¡éƒ½é‡æ–°è®¡ç®—ï¼Œé¿å…è®¡ç®—å›¾é‡å¤ä½¿ç”¨
                with torch.no_grad():
                    # ç¡®ä¿ä¸è§†è§‰ç‰¹å¾ç›¸åŒçš„æ•°æ®ç±»å‹
                    text_features = self.clip_model.encode_text(text_tokens).float()
                
                # åº”ç”¨æ–‡æœ¬æŠ•å½±å™¨
                projected_text = self.text_projector(text_features)
                category_features.append(projected_text)
            
            # å¹³å‡å¤šä¸ªæ¨¡æ¿çš„ç‰¹å¾
            if category_features:
                avg_features = torch.stack(category_features).mean(dim=0)
                all_text_features.append(avg_features)
        
        if all_text_features:
            text_features = torch.cat(all_text_features, dim=0)
            
            # éšæœºé€‰æ‹©æ–‡æœ¬ç‰¹å¾ï¼ˆåŒ¹é…batch sizeï¼‰
            if text_features.size(0) >= batch_size:
                selected_indices = torch.randperm(text_features.size(0))[:batch_size]
                selected_text_features = text_features[selected_indices]
            else:
                # é‡å¤æ–‡æœ¬ç‰¹å¾
                repeat_times = (batch_size + text_features.size(0) - 1) // text_features.size(0)
                repeated_text = text_features.repeat(repeat_times, 1)
                selected_text_features = repeated_text[:batch_size]
            
            return selected_text_features
        else:
            return torch.empty(batch_size, 512, dtype=torch.float32).to(self.device)
    
    def validate(self, dataloader):
        """åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹"""
        self.visual_projector.eval()
        self.text_projector.eval()
        
        val_losses = []
        num_batches = len(dataloader)
        
        # å®¤å†…ç±»åˆ«ï¼ˆç®€åŒ–ç‰ˆï¼‰
        indoor_categories = [
            "chair", "table", "bed", "sofa", "cabinet", "toilet", "sink",
            "refrigerator", "microwave", "bottle", "cup", "bowl",
            "lamp", "clock", "vase", "plant", "computer", "bookshelf"  # æ·»åŠ æ›´å¤šç±»åˆ«
        ]
        
        start_time = time.time()
        
        with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦
            with tqdm(total=num_batches, desc="ğŸ” éªŒè¯è¿›è¡Œä¸­", **tqdm_kwargs) as pbar:
                for batch_idx, batch in enumerate(dataloader):
                    try:
                        # è·å–å›¾åƒ
                        images = batch['images'].to(self.device)
                        batch_size = images.size(0)
                        
                        # æå–è§†è§‰ç‰¹å¾
                        visual_features = []
                        for i in range(batch_size):
                            # ä½¿ç”¨CLIPç¼–ç æ•´ä¸ªå›¾åƒ
                            image_features = self.clip_model.encode_image(images[i:i+1]).float()
                            
                            # åº”ç”¨æŠ•å½±å™¨
                            projected_features = self.visual_projector(image_features)
                            visual_features.append(projected_features)
                        
                        visual_features = torch.cat(visual_features, dim=0)
                        
                        # ç¼–ç æ–‡æœ¬ç‰¹å¾
                        text_features = self.encode_text_features_batch(indoor_categories, batch_size)
                        
                        # è®¡ç®—æŸå¤±
                        loss = self.compute_distillation_loss(visual_features, text_features)
                        
                        # è®°å½•æŸå¤±
                        val_losses.append(loss.item())
                        
                        # æ›´æ–°è¿›åº¦æ¡
                        pbar.set_postfix({
                            'loss': f"{loss.item():.4f}",
                            'avg_loss': f"{np.mean(val_losses):.4f}",
                        })
                        pbar.update(1)
                        
                    except Exception as e:
                        print(f"âš ï¸ éªŒè¯æ‰¹æ¬¡ {batch_idx} å¤„ç†å¤±è´¥: {e}")
                        continue
        
        avg_loss = np.mean(val_losses) if val_losses else float('inf')
        
        print(f"\nğŸ“Š éªŒè¯ç»Ÿè®¡:")
        print(f"   å¹³å‡æŸå¤±: {avg_loss:.6f}")
        print(f"   éªŒè¯æ—¶é—´: {time.time() - start_time:.1f}ç§’")
        print(f"   å¤„ç†æ‰¹æ¬¡: {len(val_losses)}/{num_batches}")
        
        return avg_loss
    
    def train_epoch(self, dataloader, optimizer, scheduler, loss_tracker):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.visual_projector.train()
        self.text_projector.train()
        
        epoch_losses = []
        num_batches = len(dataloader)
        
        # å®¤å†…ç±»åˆ«ï¼ˆæ‰©å±•ç‰ˆï¼‰
        indoor_categories = [
            "chair", "table", "bed", "sofa", "cabinet", "toilet", "sink",
            "refrigerator", "microwave", "bottle", "cup", "bowl",
            "lamp", "clock", "vase", "plant", "computer", "bookshelf"  # æ·»åŠ æ›´å¤šç±»åˆ«
        ]
        
        start_time = time.time()
        
        with tqdm(total=num_batches, desc="ğŸš€ è®­ç»ƒè¿›è¡Œä¸­", **tqdm_kwargs) as pbar:
            for batch_idx, batch in enumerate(dataloader):
                try:
                    optimizer.zero_grad()
                    
                    # è·å–å›¾åƒ
                    images = batch['images'].to(self.device)
                    batch_size = images.size(0)
                    
                    # æå–è§†è§‰ç‰¹å¾
                    visual_features = []
                    for i in range(batch_size):
                        # ä½¿ç”¨CLIPç¼–ç æ•´ä¸ªå›¾åƒ
                        with torch.no_grad():
                            # è½¬æ¢ä¸ºfloat32ä»¥ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
                            image_features = self.clip_model.encode_image(images[i:i+1]).float()
                        
                        # åº”ç”¨æŠ•å½±å™¨
                        projected_features = self.visual_projector(image_features)
                        visual_features.append(projected_features)
                    
                    visual_features = torch.cat(visual_features, dim=0)
                    
                    # å…³é”®ä¿®å¤ï¼šä¸ºæ¯ä¸ªæ‰¹æ¬¡é‡æ–°ç¼–ç æ–‡æœ¬ç‰¹å¾
                    text_features = self.encode_text_features_batch(indoor_categories, batch_size)
                    
                    # è®¡ç®—æŸå¤±
                    loss = self.compute_distillation_loss(visual_features, text_features)
                    
                    # æ£€æµ‹å¼‚å¸¸æŸå¤±å€¼
                    if not torch.isfinite(loss):
                        print(f"âš ï¸ è­¦å‘Š: æŸå¤±å€¼æ— æ•ˆ {loss.item()}, è·³è¿‡æ­¤æ‰¹æ¬¡")
                        continue
                    
                    # åå‘ä¼ æ’­
                    loss.backward()
                    
                    # æ›´ä¸¥æ ¼çš„æ¢¯åº¦è£å‰ª
                    torch.nn.utils.clip_grad_norm_(self.get_trainable_parameters(), max_norm=0.5)
                    
                    optimizer.step()
                    
                    # è®°å½•æŸå¤±
                    epoch_losses.append(loss.item())
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    current_lr = optimizer.param_groups[0]['lr']
                    elapsed_time = time.time() - start_time
                    samples_per_sec = (batch_idx + 1) * batch_size / elapsed_time if elapsed_time > 0 else 0
                    
                    pbar.set_postfix({
                        'loss': f"{loss.item():.4f}",
                        'avg_loss': f"{np.mean(epoch_losses):.4f}",
                        'lr': f"{current_lr:.2e}",
                        'samples/s': f"{samples_per_sec:.1f}"
                    })
                    pbar.update(1)
                    
                    # æ¸…ç†ä¸­é—´å˜é‡
                    del visual_features, text_features, loss
                    
                except Exception as e:
                    print(f"âš ï¸ æ‰¹æ¬¡ {batch_idx} å¤„ç†å¤±è´¥: {e}")
                    continue
        
            # ä¸åœ¨è¿™é‡Œæ›´æ–°å­¦ä¹ ç‡ï¼ŒReduceLROnPlateauå°†åœ¨å¤–éƒ¨ä½¿ç”¨éªŒè¯æŸå¤±æ›´æ–°
        avg_loss = np.mean(epoch_losses) if epoch_losses else float('inf')
        
        # æ˜¾å­˜æ¸…ç†
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            gpu_used = torch.cuda.memory_allocated(0) / 1024**3
            
            print(f"\nğŸ“Š Epochç»Ÿè®¡:")
            print(f"   å¹³å‡æŸå¤±: {avg_loss:.6f}")
            print(f"   è®­ç»ƒæ—¶é—´: {time.time() - start_time:.1f}ç§’")
            print(f"   å¤„ç†æ‰¹æ¬¡: {len(epoch_losses)}/{num_batches}")
            print(f"   GPUæ˜¾å­˜: {gpu_used:.1f} GB")
        
        return avg_loss
    
    def encode_text_features(self, categories):
        """ç¼–ç æ–‡æœ¬ç‰¹å¾ - ç”¨äºæ¨ç†"""
        all_text_features = []
        templates = ["a {}", "indoor {}", "a {} in a room"]
        
        for category in categories:
            category_features = []
            
            for template in templates:
                text = template.format(category)
                text_tokens = clip.tokenize([text]).to(self.device)
                
                with torch.no_grad():
                    # è½¬æ¢ä¸ºfloat32ä»¥ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
                    text_features = self.clip_model.encode_text(text_tokens).float()
                    projected_text = self.text_projector(text_features)
                    category_features.append(projected_text)
            
            # å¹³å‡å¤šä¸ªæ¨¡æ¿çš„ç‰¹å¾
            if category_features:
                avg_features = torch.stack(category_features).mean(dim=0)
                all_text_features.append(avg_features)
        
        if all_text_features:
            return torch.cat(all_text_features, dim=0)
        else:
            return torch.empty(0, 512, dtype=torch.float32).to(self.device)

# å‡½æ•°select_random_test_imageå·²ç»åœ¨æ–‡ä»¶å‰é¢å®šä¹‰ï¼Œæ­¤å¤„ä¸å†é‡å¤å®šä¹‰

def test_fixed_model(trainer, checkpoint_dir):
    """æµ‹è¯•ä¼˜åŒ–ç‰ˆæ¨¡å‹"""
    try:
        # è·å–æµ‹è¯•å›¾åƒ
        test_image_path = select_random_test_image() # ä½¿ç”¨å‰é¢å®šä¹‰çš„å‡½æ•°
        
        if not test_image_path:
            print("âŒ æ²¡æœ‰æµ‹è¯•å›¾åƒ")
            return
        
        print(f"ğŸ“· æµ‹è¯•å›¾åƒ: {os.path.basename(test_image_path)}")
        
        # ç®€å•æ£€æµ‹æµ‹è¯•
        image = Image.open(test_image_path).convert('RGB')
        
        # ç¼–ç å›¾åƒ
        image_tensor = clip_preprocess(image).unsqueeze(0).to(device)
        with torch.no_grad():
            visual_features = clip_model.encode_image(image_tensor).float()  # è½¬æ¢ä¸ºfloat32
            projected_visual = trainer.visual_projector(visual_features)
            
        # ç¼–ç æ–‡æœ¬
        categories = ["chair", "table", "bottle", "sink"]
        text_features = trainer.encode_text_features(categories)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarity = torch.mm(F.normalize(projected_visual, p=2, dim=1), 
                             F.normalize(text_features, p=2, dim=1).t())
        
        max_sim, best_idx = similarity.max(dim=1)
        
        print(f"ğŸ” ç›¸ä¼¼åº¦æµ‹è¯•:")
        print(f"   æœ€å¤§ç›¸ä¼¼åº¦: {max_sim.item():.4f}")
        print(f"   æœ€ä½³åŒ¹é…: {categories[best_idx.item()]}")
        
        if max_sim.item() > 0.1:
            print("âœ… æ¨¡å‹è®­ç»ƒæˆåŠŸï¼Œç‰¹å¾æŠ•å½±æ­£å¸¸")
        else:
            print("âš ï¸ ç›¸ä¼¼åº¦è¾ƒä½ï¼Œå¯èƒ½éœ€è¦æ›´å¤šè®­ç»ƒ")
            
    except Exception as e:
        print(f"âŒ æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")

def run_fixed_training():
    """è¿è¡Œä¼˜åŒ–ç‰ˆè®­ç»ƒ"""
    if not ENABLE_TRAINING:
        print("â­ï¸ è®­ç»ƒåŠŸèƒ½å·²ç¦ç”¨ï¼Œè·³è¿‡è®­ç»ƒè¿‡ç¨‹")
        return False
        
    print("ğŸš€ å¼€å§‹ä¼˜åŒ–ç‰ˆViLDè®­ç»ƒ - è§£å†³æŸå¤±å¢é•¿é—®é¢˜")
    print("=" * 100)
    
    try:
        # GPUä¼˜åŒ–æ£€æŸ¥
        if not setup_gpu_optimization():
            return False
        
        # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        clip_model.eval().to(device)
        detector_model.eval().to(device)
        
        # åˆ›å»ºç²¾ç»†ä¼˜åŒ–ç‰ˆè®­ç»ƒå™¨
        trainer = FixedStableTrainer(
            clip_model=clip_model,
            detector_model=detector_model,
            image_processor=image_processor,
            clip_preprocess=clip_preprocess,
            device=device
        )
        
        # åˆ›å»ºè®­ç»ƒæ•°æ®é›†
        dataset = ImprovedCOCOIndoorDataset(
            images_data=images,
            image_root=IMAGE_ROOT,
            image_size=224,
            augment=True,
            max_samples=None  # ä½¿ç”¨å…¨éƒ¨æ•°æ®é›†
        )
        
        if len(dataset) == 0:
            print("âŒ æ•°æ®é›†ä¸ºç©º")
            return False
        
        # åˆ›å»ºéªŒè¯æ•°æ®é›† - ä½¿ç”¨10%çš„æ•°æ®
        val_size = int(len(dataset) * 0.1)
        train_size = len(dataset) - val_size
        train_dataset, val_dataset = torch.utils.data.random_split(
            dataset, [train_size, val_size], 
            generator=torch.Generator().manual_seed(42)  # å›ºå®šç§å­ç¡®ä¿å¯å¤ç°æ€§
        )
        print(f"âœ… æ•°æ®é›†åˆ†å‰²å®Œæˆ: è®­ç»ƒé›† {train_size} æ ·æœ¬, éªŒè¯é›† {val_size} æ ·æœ¬")
        
        # å®šä¹‰è®­ç»ƒå‚æ•°
        max_epochs = 25
        
        # æ•°æ®åŠ è½½å™¨
        train_dataloader = DataLoader(
            train_dataset,
            batch_size=16,  # å¢åŠ æ‰¹æ¬¡å¤§å°ä»¥æé«˜ç¨³å®šæ€§
            shuffle=True,
            num_workers=2,  # å¢åŠ workeråŠ å¿«æ•°æ®åŠ è½½
            pin_memory=True,
            collate_fn=collate_fn,
            drop_last=True,
            persistent_workers=True  # ä¿æŒworkerè¿›ç¨‹æ´»è·ƒ
        )
        
        val_dataloader = DataLoader(
            val_dataset,
            batch_size=16,
            shuffle=False,  # éªŒè¯é›†ä¸éœ€è¦æ‰“ä¹±
            num_workers=2,
            pin_memory=True,
            collate_fn=collate_fn,
            persistent_workers=True
        )
        
        # ä¼˜åŒ–å™¨ - ä½¿ç”¨æ›´é«˜çš„å­¦ä¹ ç‡å¹¶æ”¹è¿›é…ç½®
        # åˆ›å»ºæ›´ç²¾ç»†ä¼˜åŒ–çš„ä¼˜åŒ–å™¨ - åŸºäºå½“å‰è®­ç»ƒç»“æœçš„ä¼˜åŒ–
        optimizer = torch.optim.AdamW(
            trainer.get_trainable_parameters(),
            lr=1e-5,  # åŸºäºå½“å‰è®­ç»ƒæ›²çº¿ï¼Œå¯ä»¥å¢åŠ å­¦ä¹ ç‡ä»¥åŠ é€Ÿæ”¶æ•›
            weight_decay=0.00005,  # è¿›ä¸€æ­¥å‡å°æ­£åˆ™åŒ–å¼ºåº¦
            betas=(0.9, 0.98),  # æ›´é•¿çš„æŒ‡æ•°åŠ æƒå¹³å‡çª—å£
            eps=1e-8,
            amsgrad=True  # å¯ç”¨AMSGradå˜ç§ï¼Œæä¾›æ›´ç¨³å®šçš„æ”¶æ•›
        )
        
        # ä½¿ç”¨æ›´ç¨³å®šçš„å­¦ä¹ ç‡è°ƒåº¦å™¨
        # scheduler = torch.optim.lr_scheduler.OneCycleLR(
        #    optimizer,
        #    max_lr=1e-4,
        #    total_steps=max_epochs * len(train_dataloader),
        #    pct_start=0.2,
        #    div_factor=25.0,
        #    final_div_factor=1000.0,
        #    anneal_strategy='cos'
        # )
        
        # ä½¿ç”¨æ›´ç²¾ç»†çš„å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥ - æ ¹æ®å½“å‰è®­ç»ƒç»“æœä¼˜åŒ–
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='min',  # ç›‘æ§éªŒè¯æŸå¤±
            factor=0.7,  # æ¯æ¬¡é™ä½30%ï¼ˆè€Œéä¹‹å‰çš„50%ï¼‰ï¼Œæ›´å¹³æ»‘çš„ä¸‹é™
            patience=1,  # 1ä¸ªepochæ— æ”¹å–„å°±è°ƒæ•´å­¦ä¹ ç‡ï¼Œæ›´ç§¯æåœ°å“åº”
            verbose=True,
            threshold=0.005,  # æ›´æ•æ„Ÿçš„é˜ˆå€¼æ£€æµ‹
            min_lr=5e-7  # æé«˜æœ€å°å­¦ä¹ ç‡
        )
        
        # æŸå¤±è¿½è¸ªå™¨å’Œæ—©åœ
        loss_tracker = LossTracker()
        early_stopping = EarlyStopping(patience=5, min_delta=1e-4)  # å¢åŠ è€å¿ƒï¼Œå‡å°‘æ•æ„Ÿåº¦
        
        print(f"ğŸ“Š ç²¾ç»†ä¼˜åŒ–è®­ç»ƒé…ç½®:")
        print(f"   æ•°æ®é›†å¤§å°: {len(dataset)}")
        print(f"   æ‰¹æ¬¡å¤§å°: 16")
        print(f"   åˆå§‹å­¦ä¹ ç‡: 1e-5")
        print(f"   å­¦ä¹ ç‡è°ƒåº¦: ReduceLROnPlateau(factor=0.7, patience=1)")
        print(f"   æœ€å¤§epochæ•°: {max_epochs}")
        print(f"   æ—©åœè€å¿ƒ: 5 epochs")
        print(f"   æ¢¯åº¦è£å‰ª: 0.5")
        print(f"   æƒé‡è¡°å‡: 0.00005")
        print(f"   ä¼˜åŒ–å™¨: AdamW with AMSGrad")
        print(f"   å¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in trainer.get_trainable_parameters()):,}")
        print(f"   ç¡¬è´Ÿæ ·æœ¬æƒé‡: 0.2")
        print(f"   æ ‡ç­¾å¹³æ»‘: 0.2")
        
        # å¼€å§‹è®­ç»ƒ
        best_loss_value = float('inf')
        best_model_path = None
        
        # åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
        checkpoint_dir = '/home/cui/vild_rtdetr_indoor/src/vild/checkpoints'
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        # æ·»åŠ è®­ç»ƒè¿›åº¦è¿½è¸ª
        train_losses = []
        val_losses = []
        
        # æ·»åŠ å­¦ä¹ ç‡è®°å½•
        lr_history = []
        
        # æ·»åŠ æœ€ä½³æ¨¡å‹ä¿¡æ¯
        best_val_loss = float('inf')
        best_epoch = 0
        patience_counter = 0
        patience = 5  # æ›´é•¿çš„è€å¿ƒ
        
        for epoch in range(max_epochs):
            print(f"\n{'='*100}")
            print(f"ğŸ”„ Epoch {epoch + 1}/{max_epochs}")
            print(f"{'='*100}")
            
            # è®­ç»ƒ
            train_loss = trainer.train_epoch(train_dataloader, optimizer, scheduler, loss_tracker)
            train_losses.append(train_loss)
            
            # éªŒè¯
            print(f"\nğŸ“Š è¿è¡ŒéªŒè¯...")
            val_loss = trainer.validate(val_dataloader)
            val_losses.append(val_loss)
            
            # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨ - ä½¿ç”¨éªŒè¯æŸå¤±å†³å®šæ˜¯å¦é™ä½å­¦ä¹ ç‡
            scheduler.step(val_loss)
            
            # è®°å½•å½“å‰å­¦ä¹ ç‡
            current_lr = optimizer.param_groups[0]['lr']
            lr_history.append(current_lr)
            
            # æ›´æ–°æŸå¤±è¿½è¸ªå™¨ (ä½¿ç”¨éªŒè¯æŸå¤±)
            loss_tracker.update(val_loss, epoch)
            
            print(f"ğŸ“ˆ Epoch {epoch+1} ç»“æœ:")
            print(f"   è®­ç»ƒæŸå¤±: {train_loss:.6f}")
            print(f"   éªŒè¯æŸå¤±: {val_loss:.6f}")
            print(f"   å­¦ä¹ ç‡: {current_lr:.8f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹ (åŸºäºéªŒè¯æŸå¤±)
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                best_epoch = epoch
                patience_counter = 0
                
                # åˆ é™¤ä¹‹å‰çš„æœ€ä½³æ¨¡å‹
                if best_model_path and os.path.exists(best_model_path):
                    os.remove(best_model_path)
                    print(f"ğŸ—‘ï¸ åˆ é™¤æ—§çš„æœ€ä½³æ¨¡å‹")
                
                # ä¿å­˜æ–°çš„æœ€ä½³æ¨¡å‹
                best_model_path = f'{checkpoint_dir}/best_refined_model.pth'
                checkpoint = {
                    'epoch': epoch,
                    'visual_projector': trainer.visual_projector.state_dict(),
                    'text_projector': trainer.text_projector.state_dict(),
                    'optimizer': optimizer.state_dict(),
                    'scheduler': scheduler.state_dict(),
                    'train_loss': train_loss,
                    'val_loss': val_loss,
                    'best_val_loss': best_val_loss,
                    'training_config': {
                        'lr': current_lr,
                        'weight_decay': 0.0001,
                        'batch_size': 16,
                        'max_epochs': max_epochs
                    }
                }
                
                torch.save(checkpoint, best_model_path)
                print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: éªŒè¯æŸå¤±={val_loss:.6f} (ç¬¬{epoch+1}è½®)")
            else:
                patience_counter += 1
                print(f"âš ï¸ éªŒè¯æŸå¤±æœªæ”¹å–„ï¼Œå½“å‰è€å¿ƒ: {patience_counter}/{patience}")
            
            # æ”¹è¿›çš„æ—©åœæ£€æŸ¥
            if patience_counter >= patience:
                print(f"\nâ¹ï¸ æ—©åœè§¦å‘! è¿ç»­{patience}ä¸ªepochæ— æ”¹å–„ï¼Œåœ¨ç¬¬ {epoch + 1} epochåœæ­¢")
                print(f"   æœ€ä½³éªŒè¯æŸå¤±å€¼: {best_val_loss:.6f} (ç¬¬{best_epoch+1}è½®)")
                break
            
            # å†…å­˜æ¸…ç†
            torch.cuda.empty_cache()
            gc.collect()
        
        # è®­ç»ƒå®Œæˆåè¾“å‡ºå¢å¼ºç‰ˆæŸå¤±å›¾
        print(f"\nğŸ¨ ç»˜åˆ¶æœ€ç»ˆæŸå¤±å›¾...")
        final_loss_path = f'{checkpoint_dir}/enhanced_training_loss.png'
        loss_tracker.plot_losses(
            save_path=final_loss_path,
            train_losses=train_losses,
            val_losses=val_losses,
            lr_history=lr_history
        )
        
        # ä¿å­˜è®­ç»ƒå†å²è®°å½•ç”¨äºåç»­åˆ†æ
        history_data = {
            'train_losses': train_losses,
            'val_losses': val_losses,
            'lr_history': lr_history,
            'best_val_loss': best_val_loss,
            'best_epoch': best_epoch,
            'epochs_trained': len(train_losses)
        }
        history_path = f'{checkpoint_dir}/training_history.json'
        with open(history_path, 'w') as f:
            # å°†numpyæ•°ç»„è½¬æ¢ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
            history_data_serializable = {
                k: v if not isinstance(v, (np.ndarray, np.generic)) else v.tolist() 
                for k, v in history_data.items()
            }
            json.dump(history_data_serializable, f, indent=2)
        
        print(f"\nğŸ‰ å¢å¼ºç‰ˆè®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“ˆ æœ€ç»ˆè®­ç»ƒæˆæœ:")
        print(f"   1. âœ… è®­ç»ƒæ ·æœ¬: {len(train_dataset)} ä¸ª")
        print(f"   2. âœ… éªŒè¯æ ·æœ¬: {len(val_dataset)} ä¸ª")
        print(f"   3. âœ… è®­ç»ƒè½®æ¬¡: {len(train_losses)} epochs")
        print(f"   4. âœ… æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
        print(f"   5. âœ… æœ€ä½³epoch: {best_epoch + 1}")
        print(f"   6. âœ… æŸå¤±å›¾å·²ä¿å­˜: {final_loss_path}")
        print(f"   7. âœ… è®­ç»ƒå†å²å·²ä¿å­˜: {history_path}")
        print(f"   8. âœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {best_model_path}")
        
        # æ˜¾ç¤ºè®­ç»ƒå’ŒéªŒè¯æŸå¤±çš„å¯¹æ¯”
        final_train_loss = train_losses[-1] if train_losses else float('nan')
        final_val_loss = val_losses[-1] if val_losses else float('nan')
        print(f"\nğŸ“Š æœ€ç»ˆæ€§èƒ½å¯¹æ¯”:")
        print(f"   â€¢ åˆå§‹è®­ç»ƒæŸå¤±: {train_losses[0]:.6f}")
        print(f"   â€¢ æœ€ç»ˆè®­ç»ƒæŸå¤±: {final_train_loss:.6f}")
        print(f"   â€¢ è®­ç»ƒæŸå¤±æ”¹è¿›: {train_losses[0] - final_train_loss:.6f}")
        print(f"   â€¢ åˆå§‹éªŒè¯æŸå¤±: {val_losses[0]:.6f}")
        print(f"   â€¢ æœ€ç»ˆéªŒè¯æŸå¤±: {final_val_loss:.6f}")
        print(f"   â€¢ éªŒè¯æŸå¤±æ”¹è¿›: {val_losses[0] - final_val_loss:.6f}")
        
        # æµ‹è¯•è®­ç»ƒåçš„æ¨¡å‹
        print(f"\nğŸ§ª æµ‹è¯•è®­ç»ƒåçš„æ¨¡å‹...")
        test_fixed_model(trainer, checkpoint_dir)
        
        return True
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

# =============================================================================
# 4. åŸºäºé¢„è®­ç»ƒæƒé‡çš„æ¨ç†æ£€æµ‹
# =============================================================================
"""
æœ¬èŠ‚å®ç°åŸºäºè®­ç»ƒå¥½çš„ViLDæ¨¡å‹è¿›è¡Œå®¤å†…åœºæ™¯ç‰©ä½“æ£€æµ‹ï¼š

1. **æ¨¡å‹æƒé‡åŠ è½½** - åŠ è½½è®­ç»ƒå¥½çš„CLIP+RT-DETRèåˆæ¨¡å‹
2. **æ–‡æœ¬æŸ¥è¯¢ç¼–ç ** - æ”¯æŒå¤šç§å®¤å†…ç‰©ä½“çš„æ–‡æœ¬æè¿°
3. **å›¾åƒåŒºåŸŸæå–** - æ™ºèƒ½åˆ†å‰²å›¾åƒä¸ºæ£€æµ‹åŒºåŸŸ
4. **ç›¸ä¼¼åº¦è®¡ç®—** - è®¡ç®—å›¾åƒç‰¹å¾ä¸æ–‡æœ¬ç‰¹å¾çš„åŒ¹é…åº¦
5. **ç»“æœåå¤„ç†** - NMSå»é‡å’Œç½®ä¿¡åº¦è¿‡æ»¤
6. **å¯è§†åŒ–å±•ç¤º** - ç»˜åˆ¶æ£€æµ‹æ¡†å’Œç½®ä¿¡åº¦æ ‡ç­¾
"""

class FixedViLDDetector:
    """ä½¿ç”¨ä¿®å¤æŠ•å½±å™¨çš„ViLDæ£€æµ‹å™¨"""
    
    def __init__(self, clip_model, detector_model, image_processor, clip_preprocess, device):
        self.clip_model = clip_model
        self.detector_model = detector_model
        self.image_processor = image_processor
        self.clip_preprocess = clip_preprocess
        self.device = device
        
        # åˆ›å»ºä¿®å¤çš„æŠ•å½±å™¨ï¼ˆæ¥è¿‘æ’ç­‰æ˜ å°„ï¼‰
        self.visual_projector = self.create_identity_projector()
        self.text_projector = self.create_identity_projector()
        
        # æ£€æµ‹å‚æ•°ï¼ˆæé«˜é˜ˆå€¼ä»¥å‡å°‘é”™è¯¯è¯†åˆ«ï¼‰
        self.similarity_threshold = 0.25  # æé«˜é˜ˆå€¼ï¼Œå‡å°‘é”™è¯¯è¯†åˆ«
        self.detection_threshold = 0.05   # æé«˜æ£€æµ‹åŸºç¡€é˜ˆå€¼
        self.max_detections = 15
        
        # åˆå§‹å®¤å†…ç±»åˆ«é›†åˆï¼ˆåŸºç¡€ç±»åˆ«ï¼‰
        self.base_categories = [
            'chair', 'table', 'bed', 'sofa', 'lamp', 'cabinet', 'door', 'window',
            'mirror', 'picture', 'book', 'bottle', 'cup', 'bowl', 'clock',
            'plant', 'television', 'refrigerator', 'microwave', 'toilet', 'sink',
            'towel', 'pillow', 'curtains', 'rug', 'shower', 'bathtub', 'shelf',
            'counter', 'desk', 'wardrobe', 'nightstand', 'computer', 'monitor'
        ]
        
        # ä½¿ç”¨åŸºç¡€ç±»åˆ«åˆå§‹åŒ–å½“å‰æ´»åŠ¨ç±»åˆ«
        self.categories = self.base_categories.copy()
        
        # ç±»åˆ«åˆ«åæ˜ å°„ï¼ˆå°†å¸¸è§æ··æ·†ç±»åˆ«ç»„åˆåœ¨ä¸€èµ·ï¼‰
        self.category_aliases = {
            'towel': ['towel', 'bath towel', 'hand towel', 'bathroom towel', 'hanging towel', 'white towel', 'folded towel'],
            'curtains': ['curtains', 'curtain', 'window curtain', 'drape', 'window treatment', 'window covering'],
            'microwave': ['microwave', 'microwave oven', 'kitchen microwave', 'heating appliance'],
            'cabinet': ['cabinet', 'cupboard', 'storage cabinet', 'kitchen cabinet', 'bathroom cabinet'],
            'sink': ['sink', 'bathroom sink', 'kitchen sink', 'washbasin', 'basin', 'wash basin'],
            'toilet': ['toilet', 'bathroom toilet', 'toilet bowl', 'commode', 'lavatory'],
            'sofa': ['sofa', 'couch', 'settee', 'living room sofa', 'seating'],
            'television': ['television', 'TV', 'flatscreen', 'TV screen', 'monitor', 'display'],
            'bed': ['bed', 'mattress', 'bedroom bed', 'sleeping surface'],
            'refrigerator': ['refrigerator', 'fridge', 'kitchen refrigerator', 'cooling appliance'],
            'table': ['table', 'dining table', 'coffee table', 'desk', 'surface'],
            'chair': ['chair', 'seat', 'armchair', 'office chair', 'dining chair'],
            'shower': ['shower', 'shower stall', 'shower cubicle', 'bathroom shower'],
            'bathtub': ['bathtub', 'tub', 'bath', 'bathroom tub'],
            'mirror': ['mirror', 'wall mirror', 'bathroom mirror', 'reflective surface'],
            'lamp': ['lamp', 'light fixture', 'table lamp', 'floor lamp', 'lighting'],
            'picture': ['picture', 'painting', 'photo', 'wall art', 'artwork', 'frame']
        }
        
        # åœºæ™¯ç‰¹å®šç±»åˆ«ï¼ˆç”¨äºåœºæ™¯ä¸Šä¸‹æ–‡ä¼˜åŒ–ï¼‰
        self.scene_categories = {
            'bathroom': ['toilet', 'sink', 'towel', 'bathtub', 'shower', 'mirror'],
            'kitchen': ['refrigerator', 'microwave', 'sink', 'cabinet', 'counter', 'table', 'bottle', 'cup', 'bowl'],
            'bedroom': ['bed', 'pillow', 'lamp', 'nightstand', 'wardrobe', 'mirror', 'clock'],
            'living_room': ['sofa', 'table', 'television', 'lamp', 'rug', 'curtains', 'picture']
        }
        
        # å¼€æ”¾è¯æ±‡æ”¯æŒ
        self.clip_vocabulary = []  # å­˜å‚¨å¼€æ”¾è¯æ±‡è¡¨
        self.custom_categories = []  # ç”¨æˆ·æ·»åŠ çš„è‡ªå®šä¹‰ç±»åˆ«
        self.enable_open_vocabulary = True  # å¯ç”¨å¼€æ”¾è¯æ±‡æ£€æµ‹
        self.open_vocabulary_threshold = 0.22  # å¼€æ”¾è¯æ±‡åŒ¹é…é˜ˆå€¼
        self.max_open_vocabulary_results = 3  # æ¯ä¸ªåŒºåŸŸæœ€å¤šè¿”å›çš„å¼€æ”¾è¯æ±‡ç»“æœæ•°
        
        # ä»CLIPåŠ è½½å¤§é‡è¯æ±‡ï¼Œä»¥æ”¯æŒæ›´å¼€æ”¾çš„æ£€æµ‹
        self._load_clip_vocabulary()
        
        print("ğŸ”§ å¢å¼ºç‰ˆå¼€æ”¾è¯æ±‡ViLDæ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   ç›¸ä¼¼åº¦é˜ˆå€¼: {self.similarity_threshold}")
        print(f"   åŸºç¡€ç±»åˆ«: {len(self.base_categories)} ä¸ª")
        print(f"   ç±»åˆ«åˆ«å: {len(self.category_aliases)} ä¸ª")
        print(f"   åœºæ™¯ç±»å‹: {len(self.scene_categories)} ä¸ª")
        print(f"   å¼€æ”¾è¯æ±‡: {'å¯ç”¨' if self.enable_open_vocabulary else 'ç¦ç”¨'}")
    
    def create_identity_projector(self):
        """åˆ›å»ºæ¥è¿‘æ’ç­‰æ˜ å°„çš„æŠ•å½±å™¨ - ç®€åŒ–ç‰ˆ"""
        # æ˜ç¡®æŒ‡å®šä½¿ç”¨float32æ•°æ®ç±»å‹ï¼Œä½¿ç”¨æ›´ç®€å•çš„ç»“æ„
        projector = torch.nn.Sequential(
            torch.nn.Linear(512, 512, bias=True, dtype=torch.float32),
            torch.nn.ReLU(),
            torch.nn.Linear(512, 512, bias=True, dtype=torch.float32)
        ).to(self.device)
        
        # åˆå§‹åŒ–ä¸ºæ’ç­‰æ˜ å°„
        with torch.no_grad():
            # ç¬¬ä¸€å±‚ï¼šæ’ç­‰æ˜ å°„
            torch.nn.init.eye_(projector[0].weight)
            if projector[0].bias is not None:
                torch.nn.init.zeros_(projector[0].bias)
            
            # ç¬¬ä¸‰å±‚ï¼šæ’ç­‰æ˜ å°„
            torch.nn.init.eye_(projector[2].weight)
            if projector[2].bias is not None:
                torch.nn.init.zeros_(projector[2].bias)
            
            # ç¡®ä¿æ‰€æœ‰æƒé‡éƒ½æ˜¯float32
            for param in projector.parameters():
                param.data = param.data.float()
        
        projector.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        return projector
    
    def detect_objects(self, image_path: str, scene_type=None, custom_categories=None, enable_open_vocabulary=True):
        """æ£€æµ‹å›¾åƒä¸­çš„ç‰©ä½“ï¼Œæ”¯æŒå¼€æ”¾è¯æ±‡å’Œåœºæ™¯æ„ŸçŸ¥åŠŸèƒ½
        
        å‚æ•°:
            image_path: å›¾åƒè·¯å¾„
            scene_type: å¯é€‰åœºæ™¯ç±»å‹ï¼Œå¦‚'bathroom', 'kitchen', 'bedroom', 'living_room'
            custom_categories: ç”¨æˆ·è‡ªå®šä¹‰çš„ç±»åˆ«åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            enable_open_vocabulary: æ˜¯å¦å¯ç”¨å¼€æ”¾è¯æ±‡æ£€æµ‹
        """
        try:
            # è®°å½•å¼€å§‹æ—¶é—´
            start_time = time.time()
            
            # æ‰“å¼€å›¾åƒ
            image = Image.open(image_path).convert('RGB')
            
            # å¤„ç†è‡ªå®šä¹‰ç±»åˆ«ï¼ˆå¦‚æœæä¾›ï¼‰
            if custom_categories:
                self.set_custom_categories(custom_categories)
                print(f"ğŸ” ä½¿ç”¨è‡ªå®šä¹‰ç±»åˆ«: {', '.join(self.custom_categories)}")
            
            # è®¾ç½®æ˜¯å¦å¯ç”¨å¼€æ”¾è¯æ±‡æ£€æµ‹
            self.enable_open_vocabulary = enable_open_vocabulary
            
            # 0. é¢„æµ‹åœºæ™¯ç±»å‹ï¼ˆå¦‚æœæœªæä¾›ï¼‰
            if scene_type is None:
                # è¿™é‡Œå¯ä»¥æ·»åŠ ç®€å•çš„åœºæ™¯åˆ†ç±»é€»è¾‘
                print("â„¹ï¸ æœªæŒ‡å®šåœºæ™¯ç±»å‹ï¼Œä½¿ç”¨é€šç”¨æ£€æµ‹æ¨¡å¼")
            else:
                print(f"ğŸ  ä½¿ç”¨åœºæ™¯æ„ŸçŸ¥æ¨¡å¼: {scene_type}")
            
            # 1. æå–å€™é€‰åŒºåŸŸ
            boxes, detection_scores = self.extract_regions(image)
            if len(boxes) == 0:
                print(f"âŒ æ²¡æœ‰æ‰¾åˆ°å€™é€‰åŒºåŸŸ")
                return {'boxes': [], 'scores': [], 'labels': []}
            
            print(f"ğŸ“¦ æ‰¾åˆ° {len(boxes)} ä¸ªå€™é€‰åŒºåŸŸ")
            
            # 2. æå–è§†è§‰ç‰¹å¾
            visual_features = self.extract_visual_features(image, boxes)
            if visual_features.size(0) == 0:
                print(f"âŒ è§†è§‰ç‰¹å¾æå–å¤±è´¥")
                return {'boxes': [], 'scores': [], 'labels': []}
            
            # 3. ç¼–ç åŸºç¡€æ–‡æœ¬ç‰¹å¾
            text_features = self.encode_text_features()
            
            # 4. è®¡ç®—ç›¸ä¼¼åº¦
            similarity_matrix = torch.mm(visual_features, text_features.t())
            
            # å¦‚æœæŒ‡å®šäº†åœºæ™¯ç±»å‹ï¼Œåº”ç”¨åœºæ™¯ä¼˜åŒ–
            if scene_type is not None:
                similarity_matrix = self.apply_scene_context(scene_type, similarity_matrix)
            
            max_similarities, best_category_indices = similarity_matrix.max(dim=1)
            
            # 5. è¿‡æ»¤å’Œåå¤„ç†
            # åŠ¨æ€é˜ˆå€¼ - ä¸ºæ˜æ˜¾çš„ç›¸ä¼¼åº¦ä½¿ç”¨æ›´é«˜é˜ˆå€¼ï¼Œä¸ºè¾¹ç¼˜æƒ…å†µä½¿ç”¨æ¢¯åº¦é˜ˆå€¼
            similarity_threshold = self.similarity_threshold
            if max_similarities.max() > 0.4:  # å¦‚æœæœ‰å¾ˆå¼ºçš„åŒ¹é…
                # ä½¿ç”¨è‡ªé€‚åº”é˜ˆå€¼ - æœ€å¤§å€¼çš„60%æˆ–å›ºå®šé˜ˆå€¼çš„è¾ƒå¤§è€…
                adaptive_threshold = max(max_similarities.max() * 0.6, self.similarity_threshold)
                similarity_threshold = min(adaptive_threshold, 0.4)  # ä¸è¶…è¿‡0.4
                print(f"ğŸ”„ ä½¿ç”¨è‡ªé€‚åº”é˜ˆå€¼: {similarity_threshold:.4f}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶…è¿‡é˜ˆå€¼çš„åŒ¹é…
            valid_mask = max_similarities >= similarity_threshold
            valid_count = valid_mask.sum().item()
            
            print(f"ğŸ” ç›¸ä¼¼åº¦èŒƒå›´: [{similarity_matrix.min():.4f}, {similarity_matrix.max():.4f}]")
            print(f"âœ… æœ‰æ•ˆæ£€æµ‹ (é˜ˆå€¼={similarity_threshold}): {valid_count}")
            
            # å¦‚æœæ²¡æœ‰è¶…è¿‡é˜ˆå€¼çš„åŒ¹é…ï¼Œå°è¯•é™ä½é˜ˆå€¼
            if valid_count == 0:
                print(f"âš ï¸ æ²¡æœ‰è¶…è¿‡é˜ˆå€¼çš„æ£€æµ‹ï¼Œå°è¯•é™ä½é˜ˆå€¼...")
                # å°è¯•æ›´ä½çš„é˜ˆå€¼
                low_threshold = 0.05
                valid_mask = max_similarities >= low_threshold
                valid_count = valid_mask.sum().item()
                print(f"ğŸ“Š é™ä½é˜ˆå€¼åˆ° {low_threshold}: {valid_count} ä¸ªæ£€æµ‹")
                
                if valid_count == 0 and not self.enable_open_vocabulary:
                    return {'boxes': [], 'scores': [], 'labels': []}
            
            # å¤„ç†åŸºç¡€ç±»åˆ«æ£€æµ‹
            if valid_count > 0:
                # æå–æœ‰æ•ˆæ£€æµ‹
                valid_boxes = boxes[:len(valid_mask)][valid_mask.cpu().numpy()]
                valid_detection_scores = detection_scores[:len(valid_mask)][valid_mask.cpu().numpy()]
                valid_similarities = max_similarities[valid_mask].cpu().numpy()
                valid_category_indices = best_category_indices[valid_mask].cpu().numpy()
                valid_labels = [self.categories[idx] for idx in valid_category_indices]
                
                # ç»„åˆåˆ†æ•°
                combined_scores = valid_detection_scores * 0.3 + valid_similarities * 0.7
                
                # æŒ‰åˆ†æ•°æ’åº
                sorted_indices = np.argsort(combined_scores)[::-1][:self.max_detections]
                
                final_boxes = valid_boxes[sorted_indices]
                final_scores = combined_scores[sorted_indices]
                final_labels = [valid_labels[i] for i in sorted_indices]
                
                print(f"ğŸ¯ åŸºç¡€æ£€æµ‹ç»“æœ: {len(final_boxes)} ä¸ªç‰©ä½“")
                print(f"   ç±»åˆ«: {set(final_labels)}")
                
                # è¿”å›æ£€æµ‹ç»“æœ
                result = {
                    'boxes': final_boxes,
                    'scores': final_scores,
                    'labels': final_labels,
                    'open_vocab_results': {}  # åˆå§‹åŒ–ç©ºçš„å¼€æ”¾è¯æ±‡ç»“æœ
                }
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•åŸºç¡€ç±»åˆ«çš„åŒ¹é…ï¼Œåˆ›å»ºç©ºç»“æœ
                result = {
                    'boxes': np.array([]),
                    'scores': np.array([]),
                    'labels': [],
                    'open_vocab_results': {}
                }
            
            # å¦‚æœå¯ç”¨äº†å¼€æ”¾è¯æ±‡æ£€æµ‹ï¼Œå°è¯•æ›´å¹¿æ³›çš„è¯æ±‡è¡¨åŒ¹é…
            if self.enable_open_vocabulary:
                print(f"ğŸ”  æ‰§è¡Œå¼€æ”¾è¯æ±‡æ£€æµ‹...")
                open_vocab_results = self.perform_open_vocabulary_detection(
                    visual_features, boxes, detection_scores
                )
                
                # åˆå¹¶ç»“æœ
                if open_vocab_results:
                    result['open_vocab_results'] = open_vocab_results
                    
                    # å¦‚æœåŸºç¡€æ£€æµ‹æ²¡æœ‰ç»“æœï¼Œä½†å¼€æ”¾è¯æ±‡æ£€æµ‹æœ‰ç»“æœ
                    if len(result['boxes']) == 0 and len(open_vocab_results['boxes']) > 0:
                        print("ğŸ”¤ ä½¿ç”¨å¼€æ”¾è¯æ±‡æ£€æµ‹ç»“æœä½œä¸ºä¸»è¦ç»“æœ")
                        result['boxes'] = open_vocab_results['boxes']
                        result['scores'] = open_vocab_results['scores']
                        result['labels'] = open_vocab_results['labels']
                
            # è®¡ç®—æ€»æ£€æµ‹æ—¶é—´
            detection_time = time.time() - start_time
            result['detection_time'] = detection_time
            
            print(f"â±ï¸ æ£€æµ‹å®Œæˆï¼Œç”¨æ—¶: {detection_time:.2f}ç§’")
            print(f"ğŸ¯ æœ€ç»ˆæ£€æµ‹ç»“æœ: {len(result['boxes'])} ä¸ªç‰©ä½“")
            
            return result
            
        except Exception as e:
            print(f"âŒ æ£€æµ‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {'boxes': [], 'scores': [], 'labels': []}
    
    def _load_clip_vocabulary(self):
        """åŠ è½½CLIPå¤§è§„æ¨¡è¯æ±‡è¡¨ï¼Œä»¥æ”¯æŒå¼€æ”¾è¯æ±‡æ£€æµ‹"""
        try:
            # å¸¸è§å®¤å†…ç‰©ä½“çš„æ‰©å±•è¯æ±‡è¡¨
            extended_vocabulary = [
                # å®¶å…·ç±»
                "armchair", "bench", "bookshelf", "bunk bed", "coffee table", "dining table",
                "dresser", "end table", "filing cabinet", "footstool", "futon", "loveseat",
                "ottoman", "recliner", "rocking chair", "sideboard", "stool", "tv stand",
                
                # ç”µå™¨ç±»
                "air conditioner", "blender", "coffee maker", "dishwasher", "electric fan", 
                "food processor", "hair dryer", "heater", "humidifier", "iron", "juicer",
                "kettle", "microwave oven", "mixer", "oven", "rice cooker", "toaster", 
                "vacuum cleaner", "washing machine", "water heater",
                
                # å«æµ´ç±»
                "bathroom cabinet", "bathroom mirror", "bathroom shelf", "bath mat",
                "faucet", "hand towel", "medicine cabinet", "shower curtain", "shower door",
                "shower head", "soap dish", "toilet brush", "toilet paper holder", "towel rack",
                
                # è£…é¥°å“ç±»
                "artificial flower", "candle", "candle holder", "cushion", "decorative plate",
                "flower vase", "photo frame", "sculpture", "wall clock", "wall hanging",
                
                # å¨æˆ¿ç”¨å“
                "chopping board", "colander", "cooking pot", "cutlery", "dinnerware",
                "frying pan", "kitchen knife", "kitchen utensil", "measuring cup", "mixing bowl",
                "oven mitt", "pepper grinder", "plate", "salt shaker", "saucepan", "spatula",
                "spice rack", "tea towel", "tongs", "wooden spoon",
                
                # å¯å…·ç±»
                "blanket", "comforter", "duvet", "mattress", "mattress pad", "pillow case",
                "sheet", "sleeping bag", "sleeping mask", "throw blanket",
                
                # ç¯å…·ç±»
                "ceiling light", "chandelier", "desk lamp", "floor lamp", "pendant light",
                "reading lamp", "string lights", "table lamp", "track lighting", "wall light",
                
                # å…¶ä»–å®¶å±…ç”¨å“
                "alarm clock", "backpack", "blinds", "coat hanger", "doormat", "extension cord",
                "garbage bin", "houseplant", "magazine rack", "power strip", "tissue box",
                "umbrella stand", "wall plug", "window blind", "window sill"
            ]
            
            # åŠ è½½åŸºæœ¬ç±»åˆ«å’Œæ‰©å±•è¯æ±‡è¡¨
            self.clip_vocabulary = self.base_categories + extended_vocabulary
            
            # ä¸ºè¯æ±‡ç”Ÿæˆæ–‡æœ¬ç‰¹å¾ï¼Œä½†ä¸è¦åœ¨åˆå§‹åŒ–æ—¶åšï¼Œè€Œæ˜¯å»¶è¿Ÿåˆ°éœ€è¦æ—¶
            print(f"âœ… åŠ è½½äº† {len(self.clip_vocabulary)} ä¸ªå¼€æ”¾è¯æ±‡é¡¹")
            
        except Exception as e:
            print(f"âš ï¸ åŠ è½½CLIPè¯æ±‡è¡¨å¤±è´¥: {e}")
            self.clip_vocabulary = self.base_categories.copy()
            
    def set_custom_categories(self, categories):
        """è®¾ç½®ç”¨æˆ·è‡ªå®šä¹‰ç±»åˆ«åˆ—è¡¨
        
        å‚æ•°:
            categories: å­—ç¬¦ä¸²åˆ—è¡¨ï¼ŒåŒ…å«ç”¨æˆ·æƒ³è¦æ£€æµ‹çš„ç‰¹å®šç±»åˆ«
        """
        if not categories:
            return
            
        # é‡ç½®å½“å‰ç±»åˆ«ä¸ºåŸºç¡€ç±»åˆ«
        self.categories = self.base_categories.copy()
        
        # æ·»åŠ ç”¨æˆ·è‡ªå®šä¹‰ç±»åˆ«
        self.custom_categories = [c for c in categories if c not in self.categories]
        self.categories.extend(self.custom_categories)
        
        print(f"âœ… è®¾ç½®äº† {len(self.custom_categories)} ä¸ªè‡ªå®šä¹‰ç±»åˆ«")
        print(f"   å½“å‰ç±»åˆ«æ€»æ•°: {len(self.categories)}")
        
    def apply_scene_context(self, scene_type, similarity_matrix):
        """åº”ç”¨åœºæ™¯ä¸Šä¸‹æ–‡æ¥ä¼˜åŒ–ç›¸ä¼¼åº¦çŸ©é˜µ
        
        å‚æ•°:
            scene_type: åœºæ™¯ç±»å‹ ('bathroom', 'kitchen', ç­‰)
            similarity_matrix: ç›¸ä¼¼åº¦çŸ©é˜µ
            
        è¿”å›:
            ä¼˜åŒ–åçš„ç›¸ä¼¼åº¦çŸ©é˜µ
        """
        if scene_type not in self.scene_categories:
            return similarity_matrix
            
        # è·å–ä¸åœºæ™¯ç›¸å…³çš„ç±»åˆ«
        relevant_categories = self.scene_categories[scene_type]
        relevant_indices = [i for i, cat in enumerate(self.categories) if cat in relevant_categories]
        
        # åˆ›å»ºçŸ©é˜µå‰¯æœ¬è¿›è¡Œä¿®æ”¹
        modified_matrix = similarity_matrix.clone()
        
        # æå‡åœºæ™¯ç›¸å…³ç±»åˆ«çš„ç›¸ä¼¼åº¦åˆ†æ•°
        boost_factor = 0.15  # 15%çš„æå‡
        for i in range(similarity_matrix.size(0)):  # éå†æ‰€æœ‰åŒºåŸŸ
            for idx in relevant_indices:  # éå†åœºæ™¯ç›¸å…³ç±»åˆ«
                modified_matrix[i, idx] *= (1 + boost_factor)
                
        # å¯¹äºä¸ç›¸å…³çš„ç±»åˆ«ï¼Œç•¥å¾®é™ä½ç›¸ä¼¼åº¦
        non_relevant_indices = [i for i, cat in enumerate(self.categories) if cat not in relevant_categories]
        penalty_factor = 0.05  # 5%çš„æƒ©ç½š
        for i in range(similarity_matrix.size(0)):
            for idx in non_relevant_indices:
                modified_matrix[i, idx] *= (1 - penalty_factor)
                
        print(f"ğŸ  åº”ç”¨åœºæ™¯ä¸Šä¸‹æ–‡ä¼˜åŒ–: {scene_type}")
        print(f"   æå‡ç±»åˆ«: {', '.join(relevant_categories)}")
                
        return modified_matrix
    
    def extract_regions(self, image):
        """æå–å€™é€‰åŒºåŸŸ"""
        inputs = self.image_processor(image, return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            outputs = self.detector_model(**inputs)
        
        target_sizes = torch.tensor([image.size[::-1]]).to(self.device)
        results = self.image_processor.post_process_object_detection(
            outputs, target_sizes=target_sizes, threshold=self.detection_threshold
        )[0]
        
        return results['boxes'].cpu().numpy(), results['scores'].cpu().numpy()
    
    def perform_open_vocabulary_detection(self, visual_features, boxes, detection_scores):
        """æ‰§è¡Œå¼€æ”¾è¯æ±‡æ£€æµ‹
        
        å‚æ•°:
            visual_features: å·²æå–çš„è§†è§‰ç‰¹å¾
            boxes: æ£€æµ‹æ¡†
            detection_scores: æ£€æµ‹åˆ†æ•°
            
        è¿”å›:
            å¼€æ”¾è¯æ±‡æ£€æµ‹ç»“æœ
        """
        try:
            # å¦‚æœè¯æ±‡è¡¨ä¸ºç©ºï¼ŒåŠ¨æ€åŠ è½½
            if not self.clip_vocabulary:
                self._load_clip_vocabulary()
                
            if not self.clip_vocabulary:
                print("âš ï¸ æ²¡æœ‰å¯ç”¨çš„å¼€æ”¾è¯æ±‡è¡¨")
                return {}
                
            # å‡†å¤‡ç»“æœç»“æ„
            open_vocab_results = {
                'boxes': [],
                'scores': [],
                'labels': [],
                'alternative_labels': []
            }
            
            # ç”Ÿæˆå¼€æ”¾è¯æ±‡çš„æ–‡æœ¬ç‰¹å¾ï¼ˆå»¶è¿Ÿè®¡ç®—ï¼Œä»¥é¿å…åˆå§‹åŒ–æ—¶çš„å¼€é”€ï¼‰
            print(f"ğŸ”¤ ç”Ÿæˆ {len(self.clip_vocabulary)} ä¸ªè¯æ±‡é¡¹çš„æ–‡æœ¬ç‰¹å¾...")
            
            # æ‰¹é‡å¤„ç†è¯æ±‡ä»¥é¿å…æ˜¾å­˜ä¸è¶³
            batch_size = 200
            all_text_features = []
            
            for i in range(0, len(self.clip_vocabulary), batch_size):
                batch = self.clip_vocabulary[i:i+batch_size]
                
                # ä¸ºæ¯ä¸ªè¯æ±‡é¡¹ç”Ÿæˆæ–‡æœ¬tokens
                texts = [f"a {word}" for word in batch]
                text_tokens = clip.tokenize(texts).to(self.device)
                
                # ç¼–ç æ–‡æœ¬ç‰¹å¾
                with torch.no_grad():
                    batch_text_features = self.clip_model.encode_text(text_tokens).float()
                    batch_text_features = self.text_projector(batch_text_features)
                    batch_text_features = F.normalize(batch_text_features, p=2, dim=1)
                    all_text_features.append(batch_text_features)
            
            # åˆå¹¶æ‰€æœ‰ç‰¹å¾
            text_features = torch.cat(all_text_features, dim=0)
            
            # è®¡ç®—æ¯ä¸ªåŒºåŸŸä¸æ‰€æœ‰è¯æ±‡çš„ç›¸ä¼¼åº¦
            print("ğŸ§® è®¡ç®—å¼€æ”¾è¯æ±‡ç›¸ä¼¼åº¦...")
            similarity_matrix = torch.mm(visual_features, text_features.t())
            
            # ä¸ºæ¯ä¸ªåŒºåŸŸæ‰¾åˆ°æœ€ä½³çš„å¼€æ”¾è¯æ±‡åŒ¹é…
            for i in range(similarity_matrix.size(0)):
                # è·å–å‰Kä¸ªæœ€ä½³åŒ¹é…
                similarities, indices = torch.topk(similarity_matrix[i], k=self.max_open_vocabulary_results)
                
                # æ£€æŸ¥ç›¸ä¼¼åº¦æ˜¯å¦é«˜äºå¼€æ”¾è¯æ±‡é˜ˆå€¼
                if similarities[0] >= self.open_vocabulary_threshold:
                    # ç¬¬ä¸€ä¸ªæœ€ä½³åŒ¹é…ä½œä¸ºä¸»æ ‡ç­¾
                    best_idx = indices[0].item()
                    best_score = similarities[0].item()
                    best_label = self.clip_vocabulary[best_idx]
                    
                    # å…¶ä»–å€™é€‰é¡¹ä½œä¸ºæ›¿ä»£æ ‡ç­¾
                    alt_indices = indices[1:].cpu().numpy()
                    alt_scores = similarities[1:].cpu().numpy()
                    alt_labels = [(self.clip_vocabulary[idx], score) for idx, score in zip(alt_indices, alt_scores)]
                    
                    # æ·»åŠ åˆ°ç»“æœä¸­
                    open_vocab_results['boxes'].append(boxes[i])
                    open_vocab_results['scores'].append(best_score)
                    open_vocab_results['labels'].append(best_label)
                    open_vocab_results['alternative_labels'].append(alt_labels)
                    
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            if open_vocab_results['boxes']:
                open_vocab_results['boxes'] = np.array(open_vocab_results['boxes'])
                open_vocab_results['scores'] = np.array(open_vocab_results['scores'])
                
                # ä¿ç•™æœ€ä½³ç»“æœ
                if len(open_vocab_results['boxes']) > self.max_detections:
                    # æŒ‰åˆ†æ•°æ’åº
                    sorted_indices = np.argsort(open_vocab_results['scores'])[::-1][:self.max_detections]
                    open_vocab_results['boxes'] = open_vocab_results['boxes'][sorted_indices]
                    open_vocab_results['scores'] = open_vocab_results['scores'][sorted_indices]
                    open_vocab_results['labels'] = [open_vocab_results['labels'][i] for i in sorted_indices]
                    open_vocab_results['alternative_labels'] = [open_vocab_results['alternative_labels'][i] for i in sorted_indices]
                    
                print(f"ğŸ”¤ å¼€æ”¾è¯æ±‡æ£€æµ‹ç»“æœ: {len(open_vocab_results['boxes'])} ä¸ªç‰©ä½“")
                print(f"   ç±»åˆ«: {set(open_vocab_results['labels'])}")
                
            return open_vocab_results
            
        except Exception as e:
            print(f"âŒ å¼€æ”¾è¯æ±‡æ£€æµ‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {}
    
    def extract_visual_features(self, image, boxes):
        """æå–è§†è§‰ç‰¹å¾"""
        if len(boxes) == 0:
            return torch.empty(0, 512).to(self.device)
        
        features = []
        img_array = np.array(image)
        max_regions = min(len(boxes), 50)  # é™åˆ¶å¤„ç†æ•°é‡
        
        for i, box in enumerate(boxes[:max_regions]):
            x1, y1, x2, y2 = box.astype(int)
            
            # è¾¹ç•Œæ£€æŸ¥
            x1 = max(0, min(x1, img_array.shape[1]-1))
            y1 = max(0, min(y1, img_array.shape[0]-1))
            x2 = max(x1+1, min(x2, img_array.shape[1]))
            y2 = max(y1+1, min(y2, img_array.shape[0]))
            
            # ç¡®ä¿åŒºåŸŸå¤§å°åˆç†
            if (x2 - x1) < 20 or (y2 - y1) < 20:
                center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2
                half_size = 25
                x1 = max(0, center_x - half_size)
                y1 = max(0, center_y - half_size)
                x2 = min(img_array.shape[1], center_x + half_size)
                y2 = min(img_array.shape[0], center_y + half_size)
            
            region = img_array[y1:y2, x1:x2]
            
            if region.size > 0:
                try:
                    region_pil = Image.fromarray(region)
                    if region_pil.size[0] < 224 or region_pil.size[1] < 224:
                        region_pil = region_pil.resize((224, 224), Image.LANCZOS)
                    
                    region_tensor = self.clip_preprocess(region_pil).unsqueeze(0).to(self.device)
                    
                    with torch.no_grad():
                        visual_feat = self.clip_model.encode_image(region_tensor).float()  # è½¬æ¢ä¸ºfloat32
                        visual_feat = self.visual_projector(visual_feat)
                        visual_feat = F.normalize(visual_feat, p=2, dim=1)
                        features.append(visual_feat)
                        
                except Exception as e:
                    if i < 5:  # åªæ‰“å°å‰å‡ ä¸ªé”™è¯¯
                        print(f"âš ï¸ åŒºåŸŸ {i} å¤„ç†å¤±è´¥: {e}")
                    continue
        
        if features:
            return torch.cat(features, dim=0)
        else:
            return torch.empty(0, 512).to(self.device)
    
    def encode_text_features(self):
        """å¢å¼ºç‰ˆæ–‡æœ¬ç‰¹å¾ç¼–ç  - ä½¿ç”¨å¤šç§æ¨¡æ¿å’Œç±»åˆ«åˆ«å"""
        all_features = []
        
        # é€šç”¨æ¨¡æ¿ - æ›´ä¸°å¯Œçš„æè¿°æ–¹å¼
        generic_templates = [
            "a {}",
            "a photo of {}",
            "an indoor {}",
            "a {} in a room",
            "a clear photo of {}"
        ]
        
        # åœºæ™¯ç‰¹å®šæ¨¡æ¿
        scene_templates = {
            'bathroom': ["a {} in a bathroom", "a bathroom {}"],
            'kitchen': ["a {} in a kitchen", "a kitchen {}"],
            'bedroom': ["a {} in a bedroom", "a bedroom {}"],
            'living_room': ["a {} in a living room", "a living room {}"]
        }
        
        # æŸ¥æ‰¾æ¯ä¸ªç±»åˆ«å¯èƒ½æ‰€å±çš„åœºæ™¯
        category_scenes = {}
        for scene, items in {
            'bathroom': ['toilet', 'sink', 'towel', 'bathtub', 'shower', 'mirror'],
            'kitchen': ['refrigerator', 'microwave', 'sink', 'cabinet', 'counter', 'table', 'bottle', 'cup', 'bowl'],
            'bedroom': ['bed', 'pillow', 'lamp', 'nightstand', 'wardrobe', 'mirror', 'clock'],
            'living_room': ['sofa', 'table', 'television', 'lamp', 'rug', 'curtains', 'picture']
        }.items():
            for item in items:
                if item not in category_scenes:
                    category_scenes[item] = []
                category_scenes[item].append(scene)
        
        for idx, category in enumerate(self.categories):
            category_features = []
            
            # å¤„ç†å½“å‰ç±»åˆ«çš„æ‰€æœ‰åˆ«åï¼ˆå¦‚æœæœ‰ï¼‰
            category_terms = [category]  # é»˜è®¤è‡³å°‘åŒ…å«ç±»åˆ«æœ¬èº«
            if category in self.category_aliases:
                category_terms.extend(self.category_aliases[category])
            
            # ä¸ºæ¯ä¸ªåˆ«ååº”ç”¨é€šç”¨æ¨¡æ¿
            for term in category_terms:
                for template in generic_templates:
                    text = template.format(term)
                    text_tokens = clip.tokenize([text]).to(self.device)
                    
                    with torch.no_grad():
                        text_feat = self.clip_model.encode_text(text_tokens).float()
                        text_feat = self.text_projector(text_feat)
                        text_feat = F.normalize(text_feat, p=2, dim=1)
                        category_features.append(text_feat)
            
            # åº”ç”¨åœºæ™¯ç‰¹å®šæ¨¡æ¿
            if category in category_scenes:
                for scene in category_scenes[category]:
                    for template in scene_templates[scene]:
                        text = template.format(category)
                        text_tokens = clip.tokenize([text]).to(self.device)
                        
                        with torch.no_grad():
                            text_feat = self.clip_model.encode_text(text_tokens).float()
                            text_feat = self.text_projector(text_feat)
                            text_feat = F.normalize(text_feat, p=2, dim=1)
                            category_features.append(text_feat)
            
            # å¹³å‡å¤šä¸ªæ¨¡æ¿å’Œåˆ«åçš„ç‰¹å¾
            if category_features:
                avg_features = torch.stack(category_features).mean(dim=0)
                avg_features = F.normalize(avg_features, p=2, dim=1)
                all_features.append(avg_features)
        
        return torch.cat(all_features, dim=0)
    
    def visualize_results(self, image_path: str, results: dict, save_path=None, scene_type=None):
        """å¯è§†åŒ–æ£€æµ‹ç»“æœå¹¶ä¿å­˜åˆ°æ–‡ä»¶ï¼Œæ”¯æŒå¼€æ”¾è¯æ±‡æ£€æµ‹ç»“æœå¯è§†åŒ–"""
        image = Image.open(image_path).convert('RGB')
        
        # åˆ›å»ºæ›´å¤§çš„å›¾å½¢ä»¥é€‚åº”æ›´å¤šä¿¡æ¯
        fig, ax = plt.subplots(1, 1, figsize=(14, 10))
        ax.imshow(image)
        
        boxes = results['boxes']
        scores = results['scores']
        labels = results['labels']
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¼€æ”¾è¯æ±‡æ£€æµ‹ç»“æœ
        has_open_vocab = 'open_vocab_results' in results and len(results['open_vocab_results'].get('boxes', [])) > 0
        
        # è®¾ç½®å›¾è¡¨æ ‡é¢˜ (ä½¿ç”¨è‹±æ–‡)
        title = "Indoor Scene Object Detection"
        if scene_type:
            title += f" - {scene_type.capitalize()} Scene"
        if 'detection_time' in results:
            title += f" ({results['detection_time']:.2f}s)"
        if has_open_vocab:
            title += " [Open Vocabulary Mode]"
            
        ax.set_title(title, fontsize=14, fontweight='bold')
        
        # ç”Ÿæˆé¢œè‰²æ˜ å°„
        num_colors_needed = max(20, len(set(labels)) + (len(set(results['open_vocab_results'].get('labels', []))) if has_open_vocab else 0))
        colors = plt.cm.tab20(np.linspace(0, 1, min(20, num_colors_needed)))
        
        if len(boxes) > 0:
            # å¯¹äºæ‰€æœ‰å·²çŸ¥æ ‡ç­¾åˆ›å»ºç±»åˆ«åˆ°é¢œè‰²çš„æ˜ å°„
            all_possible_labels = list(self.categories)
            if has_open_vocab:
                all_possible_labels.extend(list(set(results['open_vocab_results']['labels'])))
                
            category_to_color = {cat: colors[i % len(colors)] for i, cat in enumerate(set(all_possible_labels))}
            
            # ç»˜åˆ¶æ£€æµ‹æ¡†å’Œæ ‡ç­¾
            for i, (box, score, label) in enumerate(zip(boxes, scores, labels)):
                x1, y1, x2, y2 = box
                color = category_to_color.get(label, colors[0])
                
                # è®¡ç®—ç½®ä¿¡åº¦ç­‰çº§ (ä½¿ç”¨è‹±æ–‡)
                confidence_level = ""
                if score > 0.7:
                    confidence_level = "HIGH"
                elif score > 0.4:
                    confidence_level = "MED"
                else:
                    confidence_level = "LOW"
                
                # ç»˜åˆ¶æ£€æµ‹æ¡†
                rect = patches.Rectangle(
                    (x1, y1), x2-x1, y2-y1,
                    linewidth=3,
                    edgecolor=color,
                    facecolor='none'
                )
                ax.add_patch(rect)
                
                # ç»˜åˆ¶å¢å¼ºæ ‡ç­¾ - åŒ…æ‹¬ç½®ä¿¡åº¦ç­‰çº§ (è‹±æ–‡)
                ax.text(
                    x1, y1 - 5,
                    f"{label} ({confidence_level}: {score:.2f})",
                    color='white',
                    fontsize=11,
                    fontweight='bold',
                    bbox=dict(boxstyle='round,pad=0.3', facecolor=color, alpha=0.8)
                )
            
            # æ·»åŠ æ£€æµ‹ç»Ÿè®¡ä¿¡æ¯
            categories_detected = set(labels)
            
            # ç»Ÿè®¡æ ‡å‡†æ£€æµ‹å’Œå¼€æ”¾è¯æ±‡æ£€æµ‹ï¼ˆå¦‚æœæœ‰ï¼‰
            total_objects = len(boxes)
            total_categories = len(categories_detected)
            
            if has_open_vocab:
                open_vocab_boxes = results['open_vocab_results']['boxes']
                open_vocab_labels = results['open_vocab_results']['labels']
                open_vocab_categories = set(open_vocab_labels)
                
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                total_objects = len(boxes) + len(open_vocab_boxes)
                total_categories = len(categories_detected.union(open_vocab_categories))
                
                stats_text = f"Detected {total_objects} objects, {total_categories} categories (Standard: {len(boxes)}, Open Vocab: {len(open_vocab_boxes)})"
            else:
                stats_text = f"Detected {total_objects} objects, {total_categories} categories"
                
            ax.text(
                10, 30, 
                stats_text,
                color='white', 
                fontsize=12,
                fontweight='bold',
                bbox=dict(boxstyle='round,pad=0.5', facecolor='navy', alpha=0.7)
            )
            
            # åˆ›å»ºå›¾ä¾‹ - åŒ…æ‹¬æ ‡å‡†æ£€æµ‹å’Œå¼€æ”¾è¯æ±‡æ£€æµ‹
            legend_elements = []
            
            # æ ‡å‡†æ£€æµ‹çš„å›¾ä¾‹
            unique_labels = list(set(labels))
            for label in unique_labels:
                color = category_to_color.get(label, colors[0])
                legend_elements.append(
                    patches.Patch(facecolor=color, label=label)
                )
                
            # å¦‚æœæœ‰å¼€æ”¾è¯æ±‡æ£€æµ‹ï¼Œæ·»åŠ å…¶å›¾ä¾‹
            if has_open_vocab:
                unique_open_labels = list(set(results['open_vocab_results']['labels']))
                for label in unique_open_labels:
                    # ä½¿ç”¨è™šçº¿è¾¹æ¡†åŒºåˆ†å¼€æ”¾è¯æ±‡ç»“æœ
                    if label not in unique_labels:  # é¿å…é‡å¤
                        color = category_to_color.get(label, colors[0])
                        legend_elements.append(
                            patches.Patch(facecolor=color, label=f"{label} (Open)", 
                                        linestyle='dashed', edgecolor='black')
                        )
            
            if legend_elements:
                ax.legend(
                    handles=legend_elements,
                    loc='upper right',
                    fontsize=10,
                    title="Detected Categories",
                    fancybox=True,
                    framealpha=0.7
                )
        # ç»˜åˆ¶å¼€æ”¾è¯æ±‡æ£€æµ‹ç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
        if has_open_vocab:
            open_vocab_boxes = results['open_vocab_results']['boxes']
            open_vocab_scores = results['open_vocab_results']['scores']
            open_vocab_labels = results['open_vocab_results']['labels']
            open_vocab_alt_labels = results['open_vocab_results'].get('alternative_labels', [])
            
            for i, (box, score, label) in enumerate(zip(open_vocab_boxes, open_vocab_scores, open_vocab_labels)):
                x1, y1, x2, y2 = box
                
                # ä¸ºå¼€æ”¾è¯æ±‡æ£€æµ‹ä½¿ç”¨è™šçº¿è¾¹æ¡†
                color = category_to_color.get(label, colors[0])
                
                # è®¡ç®—ç½®ä¿¡åº¦ç­‰çº§ (ä½¿ç”¨è‹±æ–‡)
                confidence_level = ""
                if score > 0.7:
                    confidence_level = "HIGH"
                elif score > 0.4:
                    confidence_level = "MED"
                else:
                    confidence_level = "LOW"
                
                # ç»˜åˆ¶æ£€æµ‹æ¡† - ä½¿ç”¨è™šçº¿åŒºåˆ†å¼€æ”¾è¯æ±‡æ£€æµ‹
                rect = patches.Rectangle(
                    (x1, y1), x2-x1, y2-y1,
                    linewidth=3,
                    edgecolor=color,
                    facecolor='none',
                    linestyle='dashed'  # ä½¿ç”¨è™šçº¿è¡¨ç¤ºå¼€æ”¾è¯æ±‡æ£€æµ‹
                )
                ax.add_patch(rect)
                
                # æ˜¾ç¤ºæ›¿ä»£æ ‡ç­¾ï¼ˆå¦‚æœæœ‰ï¼‰- è‹±æ–‡
                alt_text = ""
                if i < len(open_vocab_alt_labels) and open_vocab_alt_labels[i]:
                    top_alt = open_vocab_alt_labels[i][0]  # å–ç¬¬ä¸€ä¸ªæ›¿ä»£æ ‡ç­¾
                    alt_text = f" | Alt: {top_alt[0]} ({top_alt[1]:.2f})"
                
                # ç»˜åˆ¶å¢å¼ºæ ‡ç­¾ - åŒ…æ‹¬ç½®ä¿¡åº¦ç­‰çº§å’Œå¼€æ”¾è¯æ±‡æ ‡è®° (è‹±æ–‡)
                ax.text(
                    x1, y1 - 5,
                    f"{label} ({confidence_level}: {score:.2f}){alt_text}",
                    color='white',
                    fontsize=11,
                    fontweight='bold',
                    bbox=dict(boxstyle='round,pad=0.3,rounding_size=0.2', facecolor=color, alpha=0.8)
                )
        
        if len(boxes) == 0 and (not has_open_vocab or len(results['open_vocab_results'].get('boxes', [])) == 0):
            # æ²¡æœ‰æ£€æµ‹åˆ°ç‰©ä½“çš„æƒ…å†µ (è‹±æ–‡)
            ax.text(
                image.width // 2 - 100, image.height // 2,
                "No Objects Detected",
                color='white',
                fontsize=16,
                fontweight='bold',
                bbox=dict(boxstyle='round,pad=1', facecolor='red', alpha=0.8)
            )
        
        # æ·»åŠ æ—¶é—´æˆ³å’Œä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯ (è‹±æ–‡)
        plt.figtext(
            0.01, 0.01,
            f"Detection Time: {time.strftime('%Y-%m-%d %H:%M:%S')} | Model: Enhanced ViLD (CLIP + RTDETR)",
            fontsize=8, color='gray'
        )
        
        ax.set_xticks([])
        ax.set_yticks([])
        plt.tight_layout()
        
        # ä¿å­˜ç»“æœè€Œä¸æ˜¯æ˜¾ç¤º
        if save_path is None:
            # å¦‚æœæœªæä¾›ä¿å­˜è·¯å¾„ï¼Œåˆ™ç”Ÿæˆä¸€ä¸ªåŸºäºåŸå§‹å›¾åƒåç§°çš„è·¯å¾„
            base_name = os.path.basename(image_path)
            name, ext = os.path.splitext(base_name)
            save_dir = os.path.join(os.path.dirname(image_path), "detection_results")
            os.makedirs(save_dir, exist_ok=True)
            save_path = os.path.join(save_dir, f"{name}_detection{ext}")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(save_path) if os.path.dirname(save_path) else '.', exist_ok=True)
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"âœ… æ£€æµ‹ç»“æœå·²ä¿å­˜è‡³: {save_path}")
            
        plt.close(fig)  # å…³é—­å›¾å½¢ä»¥é‡Šæ”¾å†…å­˜
        
        return fig
        return result_path

def test_fixed_detector():
    """æµ‹è¯•ä¿®å¤ç‰ˆæ£€æµ‹å™¨"""
    if not ENABLE_DETECTION:
        print("â­ï¸ æ£€æµ‹åŠŸèƒ½å·²ç¦ç”¨ï¼Œè·³è¿‡æ£€æµ‹å™¨æµ‹è¯•")
        return
        
    print("ğŸ”„ æµ‹è¯•ä¿®å¤ç‰ˆæ£€æµ‹å™¨...")
    
    # åˆ›å»ºä¿®å¤ç‰ˆæ£€æµ‹å™¨
    fixed_detector = FixedViLDDetector(
        clip_model=clip_model,
        detector_model=detector_model,
        image_processor=image_processor,
        clip_preprocess=clip_preprocess,
        device=device
    )
    
    # è·å–æµ‹è¯•å›¾åƒ
    test_image_path = select_random_test_image()
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œåˆ›å»ºä¸€ä¸ªæµ‹è¯•å›¾åƒ
    if not test_image_path:
        print("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆå›¾åƒï¼Œåˆ›å»ºæµ‹è¯•å›¾åƒ...")
        test_dir = os.path.join(PROJECT_ROOT, "tests")
        os.makedirs(test_dir, exist_ok=True)
        test_image_path = os.path.join(test_dir, "test_image.jpg")
        
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•å›¾åƒ
        test_image = np.ones((480, 640, 3), dtype=np.uint8) * 200
        # ç»˜åˆ¶ä¸€äº›ç®€å•çš„å½¢çŠ¶
        cv2.rectangle(test_image, (100, 100), (300, 300), (0, 0, 255), 2)
        cv2.circle(test_image, (400, 200), 50, (0, 255, 0), -1)
        cv2.imwrite(test_image_path, test_image)
        print(f"å·²åˆ›å»ºæµ‹è¯•å›¾åƒ: {test_image_path}")
    
    print(f"ğŸ“· æµ‹è¯•å›¾åƒ: {os.path.basename(test_image_path)}")
    
    # è¿è¡Œæ£€æµ‹
    results = fixed_detector.detect_objects(test_image_path)
    
    # æ€»æ˜¯ä¿å­˜æ£€æµ‹ç»“æœåˆ°æ–‡ä»¶ï¼Œé¿å…åœ¨WSLç¯å¢ƒä¸‹å°è¯•æ˜¾ç¤ºå›¾å½¢
    checkpoint_dir = '/home/cui/vild_rtdetr_indoor/src/vild/checkpoints'
    os.makedirs(checkpoint_dir, exist_ok=True)
    detection_path = os.path.join(checkpoint_dir, f"detection_result_{os.path.basename(test_image_path)}")
    
    # å¯è§†åŒ–ç»“æœå¹¶ä¿å­˜
    saved_path = fixed_detector.visualize_results(test_image_path, results, save_path=detection_path)
    
    # åœ¨WSLç¯å¢ƒä¸­é€šå¸¸ä¼šå¤±è´¥ï¼Œæ‰€ä»¥ä¸å°è¯•æ˜¾ç¤ºï¼Œä»…ä¿å­˜ç»“æœ
    # å¦‚æœéœ€è¦æ˜¾ç¤ºï¼Œè¯·åœ¨Windowsç¯å¢ƒä¸‹æŸ¥çœ‹ä¿å­˜çš„å›¾åƒæ–‡ä»¶    print(f"\nğŸ¯ ä¿®å¤ç‰ˆæ£€æµ‹å™¨æµ‹è¯•å®Œæˆ!")
    if len(results['boxes']) > 0:
        print(f"âœ… æˆåŠŸæ£€æµ‹åˆ° {len(results['boxes'])} ä¸ªç‰©ä½“")
        print(f"   æ£€æµ‹ç±»åˆ«: {set(results['labels'])}")
        if saved_path:
            print(f"   æ£€æµ‹ç»“æœå·²ä¿å­˜: {saved_path}")
    else:
        print(f"âš ï¸ æœªæ£€æµ‹åˆ°ç‰©ä½“ï¼Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥è°ƒæ•´å‚æ•°")
    
    # ä¿å­˜ä¿®å¤ç‰ˆæŠ•å½±å™¨ï¼Œæ›¿æ¢åŸæœ‰è®­ç»ƒæƒé‡
    checkpoint_dir = '/home/cui/vild_rtdetr_indoor/src/vild/checkpoints'
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    # åˆ›å»ºä¿®å¤ç‰ˆæŠ•å½±å™¨
    fixed_visual = fixed_detector.visual_projector
    fixed_text = fixed_detector.text_projector
    
    # ä¿å­˜ä¸ºæ–°æ£€æŸ¥ç‚¹
    checkpoint = {
        'epoch': 999,  # ç‰¹æ®Šæ ‡è®°
        'visual_projector': fixed_visual.state_dict(),
        'text_projector': fixed_text.state_dict(),
        'loss': 0.0,  # ä¿®å¤ç‰ˆæ²¡æœ‰æŸå¤±
        'fixed_version': True,
        'description': 'Identity mapping fix for zero detection issue'
    }
    
    torch.save(checkpoint, f'{checkpoint_dir}/fixed_identity_projectors.pth')
    print("âœ… ä¿®å¤ç‰ˆæŠ•å½±å™¨å·²ä¿å­˜: fixed_identity_projectors.pth")

# ä¸»å‡½æ•°
def detect_indoor_image(image_path, output_path=None, scene_type=None, custom_categories=None, enable_open_vocab=True):
    """è¿è¡Œå•å¼ å›¾åƒçš„å®¤å†…åœºæ™¯å¼€æ”¾è¯æ±‡æ£€æµ‹
    
    å‚æ•°:
        image_path: è¾“å…¥å›¾åƒè·¯å¾„
        output_path: è¾“å‡ºå›¾åƒè·¯å¾„ (å¯é€‰)
        scene_type: åœºæ™¯ç±»å‹ï¼Œå¦‚ 'bathroom', 'kitchen', 'bedroom', 'living_room' (å¯é€‰)
        custom_categories: è‡ªå®šä¹‰ç±»åˆ«åˆ—è¡¨ (å¯é€‰)
        enable_open_vocab: æ˜¯å¦å¯ç”¨å¼€æ”¾è¯æ±‡æ£€æµ‹ (é»˜è®¤True)
        
    è¿”å›:
        æ£€æµ‹ç»“æœå’Œè¾“å‡ºå›¾åƒè·¯å¾„
    """
    try:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(image_path):
            print(f"âŒ å›¾åƒä¸å­˜åœ¨: {image_path}")
            return None, None
            
        # æ£€æŸ¥åœºæ™¯ç±»å‹
        valid_scenes = ['bathroom', 'kitchen', 'bedroom', 'living_room']
        if scene_type and scene_type not in valid_scenes:
            print(f"âš ï¸ æ— æ•ˆçš„åœºæ™¯ç±»å‹: {scene_type}")
            print(f"   æœ‰æ•ˆé€‰é¡¹: {', '.join(valid_scenes)}")
            scene_type = None
        
        # é€‰æ‹©é€‚å½“çš„è®¾å¤‡
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
        
        # åŠ è½½æ¨¡å‹
        print("ğŸ”„ åŠ è½½CLIPæ¨¡å‹...")
        clip_model, clip_preprocess = clip.load('ViT-B/32', device=device)
        clip_model.eval()
        
        print("ğŸ”„ åŠ è½½RT-DETRæ£€æµ‹å™¨...")
        from transformers import AutoImageProcessor, AutoModelForObjectDetection
        
        image_processor = AutoImageProcessor.from_pretrained("PekinU/rtdetr-l")
        detector_model = AutoModelForObjectDetection.from_pretrained("PekinU/rtdetr-l")
        detector_model = detector_model.to(device)
        detector_model.eval()
        
        # åˆ›å»ºæ£€æµ‹å™¨
        detector = FixedViLDDetector(
            clip_model=clip_model,
            detector_model=detector_model,
            image_processor=image_processor,
            clip_preprocess=clip_preprocess,
            device=device
        )
        
        # æ‰§è¡Œæ£€æµ‹
        print(f"ğŸ” å¼€å§‹æ£€æµ‹å›¾åƒ: {image_path}")
        if scene_type:
            print(f"   åœºæ™¯ç±»å‹: {scene_type}")
            
        start_time = time.time()
        results = detector.detect_objects(
            image_path, 
            scene_type=scene_type,
            custom_categories=custom_categories,
            enable_open_vocabulary=enable_open_vocab
        )
        elapsed = time.time() - start_time
        
        # æ˜¾ç¤ºæ£€æµ‹ç»“æœ
        num_objects = len(results['boxes'])
        print(f"âœ“ æ£€æµ‹å®Œæˆ! ç”¨æ—¶ {elapsed:.2f} ç§’")
        print(f"   æ£€æµ‹åˆ° {num_objects} ä¸ªç‰©ä½“")
        
        if num_objects > 0:
            labels = results['labels']
            scores = results['scores']
            
            # æ‰“å°æ£€æµ‹åˆ°çš„å¯¹è±¡
            print("\nğŸ“‹ æ£€æµ‹ç»“æœ:")
            for i, (label, score) in enumerate(zip(labels, scores)):
                print(f"   {i+1}. {label:<15} ç½®ä¿¡åº¦: {score:.4f}")
                
            # æ˜¾ç¤ºç±»åˆ«ç»Ÿè®¡
            from collections import Counter
            label_counts = Counter(labels)
            print("\nğŸ“Š ç±»åˆ«ç»Ÿè®¡:")
            for label, count in label_counts.most_common():
                print(f"   {label:<15}: {count} ä¸ª")
        
        # å¯è§†åŒ–å¹¶ä¿å­˜ç»“æœ
        output_path = detector.visualize_results(image_path, results, output_path, scene_type)
        
        return results, output_path
        
    except Exception as e:
        print(f"âŒ æ£€æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None, None

if __name__ == "__main__":
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°æ”¯æŒ
    import argparse
    parser = argparse.ArgumentParser(description='å®¤å†…åœºæ™¯å¼€æ”¾è¯æ±‡ç‰©ä½“æ£€æµ‹')
    parser.add_argument('--image', '-i', help='è¾“å…¥å›¾åƒè·¯å¾„')
    parser.add_argument('--output', '-o', help='è¾“å‡ºå›¾åƒè·¯å¾„')
    parser.add_argument('--scene', '-s', choices=['bathroom', 'kitchen', 'bedroom', 'living_room'], 
                        help='åœºæ™¯ç±»å‹ (å¯é€‰: bathroom, kitchen, bedroom, living_room)')
    parser.add_argument('--demo', '-d', action='store_true', help='è¿è¡Œç¤ºä¾‹æ¼”ç¤º')
    parser.add_argument('--open-vocab', '-ov', action='store_true', default=True, 
                        help='å¯ç”¨å¼€æ”¾è¯æ±‡æ£€æµ‹ (é»˜è®¤å¯ç”¨)')
    parser.add_argument('--no-open-vocab', '-nov', action='store_false', dest='open_vocab',
                        help='ç¦ç”¨å¼€æ”¾è¯æ±‡æ£€æµ‹')
    parser.add_argument('--custom-categories', '-c', nargs='+', 
                        help='æŒ‡å®šè‡ªå®šä¹‰ç±»åˆ«åˆ—è¡¨ï¼Œå¦‚ "cup book laptop"')
    parser.add_argument('--train', '-t', action='store_true', help='æ‰§è¡Œè®­ç»ƒè¿‡ç¨‹')
    parser.add_argument('--no-train', action='store_false', dest='train', help='è·³è¿‡è®­ç»ƒè¿‡ç¨‹')
    parser.add_argument('--detect', action='store_true', default=True, help='æ‰§è¡Œæ£€æµ‹è¿‡ç¨‹')
    parser.add_argument('--no-detect', action='store_false', dest='detect', help='è·³è¿‡æ£€æµ‹è¿‡ç¨‹')
    parser.add_argument('--test-image', type=int, default=-1, help='æŒ‡å®šæµ‹è¯•å›¾åƒç´¢å¼•ï¼Œ-1è¡¨ç¤ºéšæœºé€‰æ‹©')
    
    args = parser.parse_args()
    
    print("ğŸš€ å¯åŠ¨å¢å¼ºç‰ˆå®¤å†…ç‰©ä½“æ£€æµ‹ç³»ç»Ÿ...")
    
    # è®¾ç½®å…¨å±€æ§åˆ¶å˜é‡
    # æ³¨æ„ï¼šglobalå£°æ˜åº”è¯¥åœ¨èµ‹å€¼ä¹‹å‰ï¼Œè¿™é‡Œä¸éœ€è¦å†å£°æ˜å› ä¸ºå·²ç»åœ¨æ–‡ä»¶é¡¶éƒ¨å®šä¹‰äº†å…¨å±€å˜é‡
    ENABLE_TRAINING = args.train
    ENABLE_DETECTION = args.detect
    TEST_IMAGE_INDEX = args.test_image
    
    print(f"âš™ï¸ é…ç½®è®¾ç½®:")
    print(f"   â€¢ è®­ç»ƒåŠŸèƒ½: {'å¯ç”¨' if ENABLE_TRAINING else 'ç¦ç”¨'}")
    print(f"   â€¢ æ£€æµ‹åŠŸèƒ½: {'å¯ç”¨' if ENABLE_DETECTION else 'ç¦ç”¨'}")
    print(f"   â€¢ æµ‹è¯•å›¾åƒ: {TEST_IMAGE_INDEX if TEST_IMAGE_INDEX >= 0 else 'éšæœºé€‰æ‹©'}")
    
    # å¤„ç†å‘½ä»¤è¡Œå‚æ•°
    if args.image:
        # ä½¿ç”¨æä¾›çš„å›¾åƒè·¯å¾„
        image_path = args.image
        output_path = args.output
        scene_type = args.scene
        
        custom_categories = args.custom_categories
        enable_open_vocab = args.open_vocab
        
        print(f"ğŸ–¼ï¸ å¤„ç†å›¾åƒ: {image_path}")
        if custom_categories:
            print(f"ğŸ” ä½¿ç”¨è‡ªå®šä¹‰ç±»åˆ«: {', '.join(custom_categories)}")
        print(f"ğŸ”¤ å¼€æ”¾è¯æ±‡æ£€æµ‹: {'å¯ç”¨' if enable_open_vocab else 'ç¦ç”¨'}")
        
        results, output_path = detect_indoor_image(
            image_path, 
            output_path, 
            scene_type,
            custom_categories=custom_categories,
            enable_open_vocab=enable_open_vocab
        )
        
        if results is not None:
            print("\nâœ… æ£€æµ‹å®Œæˆ!")
            print(f"   ç»“æœå·²ä¿å­˜è‡³: {output_path}")
            
            # å¦‚æœæœ‰å¼€æ”¾è¯æ±‡æ£€æµ‹ç»“æœï¼Œæ˜¾ç¤ºæ›´å¤šä¿¡æ¯
            if enable_open_vocab and 'open_vocab_results' in results and results['open_vocab_results']:
                open_vocab_labels = results['open_vocab_results']['labels']
                if open_vocab_labels:
                    print(f"   å¼€æ”¾è¯æ±‡æ£€æµ‹ç»“æœ: {len(open_vocab_labels)} ä¸ªç‰©ä½“")
                    print(f"   å¼€æ”¾è¯æ±‡ç±»åˆ«: {set(open_vocab_labels)}")
        else:
            print("\nâŒ æ£€æµ‹å¤±è´¥")
            
    elif args.demo:
        # è¿è¡Œæ¼”ç¤º - æ£€æŸ¥ä¸åŒå®¤å†…åœºæ™¯ä¸­çš„ç›®æ ‡æ£€æµ‹
        print("ğŸ¬ è¿è¡Œå®¤å†…åœºæ™¯æ£€æµ‹æ¼”ç¤º...")
        
        # æŸ¥æ‰¾ç¤ºä¾‹å›¾åƒ
        demo_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "demo_images")
        os.makedirs(demo_dir, exist_ok=True)
        
        # å¦‚æœæ²¡æœ‰ç¤ºä¾‹å›¾åƒï¼Œæˆ‘ä»¬åˆ›å»ºä¸€äº›
        if not os.listdir(demo_dir):
            print("ğŸ“· åˆ›å»ºç¤ºä¾‹å›¾åƒ...")
            # åˆ›å»ºå‡ ä¸ªæµ‹è¯•å›¾åƒ
            scenes = ["bathroom", "kitchen", "living_room", "bedroom"]
            for scene in scenes:
                test_img_path = os.path.join(demo_dir, f"{scene}_demo.jpg")
                test_image = np.ones((480, 640, 3), dtype=np.uint8) * 200
                
                # æ·»åŠ åœºæ™¯åç§°
                cv2.putText(test_image, f"{scene.upper()} DEMO", (50, 240), 
                            cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 0), 3)
                
                # æ·»åŠ ä¸åŒçš„å½¢çŠ¶
                color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))
                if scene == "bathroom":
                    cv2.rectangle(test_image, (400, 100), (500, 300), color, 2)
                elif scene == "kitchen":
                    cv2.circle(test_image, (450, 240), 100, color, -1)
                elif scene == "living_room":
                    pts = np.array([[350, 100], [500, 150], [450, 300], [300, 250]], np.int32)
                    cv2.polylines(test_image, [pts], True, color, 3)
                else:
                    cv2.rectangle(test_image, (350, 150), (500, 300), color, -1)
                    
                cv2.imwrite(test_img_path, test_image)
                print(f"   åˆ›å»ºäº†ç¤ºä¾‹å›¾åƒ: {test_img_path}")
        
        # å¤„ç†æ¯ä¸ªç¤ºä¾‹å›¾åƒ
        demo_images = [f for f in os.listdir(demo_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]
        
        if not demo_images:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç¤ºä¾‹å›¾åƒ")
        else:
            for image_file in demo_images:
                image_path = os.path.join(demo_dir, image_file)
                scene_type = next((s for s in ['bathroom', 'kitchen', 'bedroom', 'living_room'] 
                                   if s in image_file), None)
                
                print(f"\nğŸ“· å¤„ç†ç¤ºä¾‹å›¾åƒ: {image_file}")
                if scene_type:
                    print(f"   åœºæ™¯ç±»å‹: {scene_type}")
                    
                # ä¸ºæ¼”ç¤ºå¯ç”¨å¼€æ”¾è¯æ±‡æ£€æµ‹
                output_path = os.path.join(demo_dir, f"{os.path.splitext(image_file)[0]}_result.jpg")
                results, _ = detect_indoor_image(
                    image_path, 
                    output_path, 
                    scene_type,
                    enable_open_vocab=True
                )
                
                if results is not None:
                    print(f"   âœ“ æ£€æµ‹å®Œæˆï¼Œç»“æœä¿å­˜è‡³: {output_path}")
                else:
                    print(f"   âŒ æ£€æµ‹å¤±è´¥")
            
            print("\nğŸ‰ æ¼”ç¤ºå®Œæˆ!")
            
    else:
        # æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
        print("\nâš ï¸ è¯·æä¾›å›¾åƒè·¯å¾„æˆ–ä½¿ç”¨æ¼”ç¤ºæ¨¡å¼")
        print("ä½¿ç”¨ç¤ºä¾‹:")
        print("  python indoor_vild.py --image path/to/image.jpg")
        print("  python indoor_vild.py --image path/to/image.jpg --scene bathroom")
        print("  python indoor_vild.py --demo")
        print("\nè¿è¡Œ 'python indoor_vild.py --help' æŸ¥çœ‹æ‰€æœ‰é€‰é¡¹")
    
    try:
        # æ ¹æ®å¼€å…³æ§åˆ¶æ˜¯å¦æ‰§è¡Œè®­ç»ƒå’Œæ£€æµ‹
        if ENABLE_TRAINING:
            print("\nğŸ”„ å¼€å§‹è®­ç»ƒæµç¨‹...")
            run_fixed_training()
        else:
            print("\nâ­ï¸ è®­ç»ƒè¿‡ç¨‹å·²è·³è¿‡ (ENABLE_TRAINING=False)")
        
        if ENABLE_DETECTION:
            print("\nğŸ” æµ‹è¯•æ£€æµ‹å™¨...")
            test_fixed_detector()
        else:
            print("\nâ­ï¸ æ£€æµ‹è¿‡ç¨‹å·²è·³è¿‡ (ENABLE_DETECTION=False)")
        
        # ç¡®ä¿å…³é—­æ‰€æœ‰matplotlibå›¾å½¢ï¼Œé˜²æ­¢ç¨‹åºå¡ä½
        plt.close('all')
        
        print("\nâœ… ç³»ç»Ÿè¿è¡Œå®Œæˆ!")
    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        traceback.print_exc()
        
        # å…³é—­æ‰€æœ‰å›¾å½¢ï¼Œç¡®ä¿ç¨‹åºèƒ½å¤Ÿé€€å‡º
        try:
            plt.close('all')
        except:
            pass
        
        print("\nç³»ç»Ÿå·²å°è¯•æ¢å¤å¹¶é€€å‡º")