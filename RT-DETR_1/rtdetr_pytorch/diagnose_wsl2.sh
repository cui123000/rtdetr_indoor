#!/bin/bash
# WSL2 RT-DETR è®­ç»ƒè¯Šæ–­å’Œé¢„çƒ­è„šæœ¬

echo "=========================================="
echo "ğŸ” RT-DETR WSL2 ç¯å¢ƒè¯Šæ–­"
echo "=========================================="

# 1. æ£€æŸ¥GPU
echo -e "\nğŸ“Š 1. GPU ä¿¡æ¯æ£€æŸ¥"
echo "----------------------------------------"
nvidia-smi --query-gpu=name,driver_version,memory.total --format=csv,noheader
nvidia-smi --query-gpu=temperature.gpu,power.draw,power.limit --format=csv,noheader

# 2. æ£€æŸ¥ç³»ç»Ÿèµ„æº
echo -e "\nğŸ’» 2. ç³»ç»Ÿèµ„æºæ£€æŸ¥"
echo "----------------------------------------"
echo "CPUæ ¸å¿ƒæ•°: $(nproc)"
echo "æ€»å†…å­˜: $(free -h | awk '/^Mem:/ {print $2}')"
echo "å¯ç”¨å†…å­˜: $(free -h | awk '/^Mem:/ {print $7}')"
echo "ç£ç›˜ç©ºé—´: $(df -h /home | awk 'NR==2 {print $4}')"

# 3. æ£€æŸ¥Pythonç¯å¢ƒ
echo -e "\nğŸ 3. Python ç¯å¢ƒæ£€æŸ¥"
echo "----------------------------------------"
source ~/miniconda3/etc/profile.d/conda.sh
conda activate rtdetr
python -c "
import torch
print(f'PyTorch: {torch.__version__}')
print(f'CUDA available: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'CUDA version: {torch.version.cuda}')
    print(f'GPU: {torch.cuda.get_device_name(0)}')
    print(f'GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')
"

# 4. æ£€æŸ¥é…ç½®æ–‡ä»¶
echo -e "\nâš™ï¸  4. è®­ç»ƒé…ç½®æ£€æŸ¥"
echo "----------------------------------------"
CONFIG="/home/cui/rtdetr_indoor/RT-DETR/rtdetr_pytorch/configs/rtdetr/include/dataloader.yml"
echo "batch_size (train): $(grep -A 2 'train_dataloader:' $CONFIG | grep 'batch_size:' | awk '{print $2}')"
echo "num_workers (train): $(grep -A 3 'train_dataloader:' $CONFIG | grep 'num_workers:' | awk '{print $2}')"

# 5. GPUé¢„çƒ­æµ‹è¯•
echo -e "\nğŸ”¥ 5. GPU é¢„çƒ­æµ‹è¯•ï¼ˆé™ä½åˆå§‹è´Ÿè½½å†²å‡»ï¼‰"
echo "----------------------------------------"
python << 'PYTHON_EOF'
import torch
import torch.nn as nn
import time

print("å¼€å§‹GPUé¢„çƒ­...")

# åˆ›å»ºä¸€ä¸ªå°æ¨¡å‹é¢„çƒ­GPU
model = nn.Sequential(
    nn.Conv2d(3, 64, 3, padding=1),
    nn.ReLU(),
    nn.Conv2d(64, 64, 3, padding=1),
    nn.ReLU()
).cuda()

# å°æ‰¹é‡æ•°æ®é¢„çƒ­
for i in range(5):
    x = torch.randn(2, 3, 640, 640).cuda()
    with torch.cuda.amp.autocast():
        y = model(x)
    loss = y.sum()
    loss.backward()
    print(f"  é¢„çƒ­ {i+1}/5 å®Œæˆ")
    time.sleep(1)

# æ¸…ç†
del model, x, y, loss
torch.cuda.empty_cache()
print("âœ… GPUé¢„çƒ­å®Œæˆï¼Œæ˜¾å­˜å·²æ¸…ç†")
PYTHON_EOF

echo -e "\n=========================================="
echo "âœ… è¯Šæ–­å®Œæˆ"
echo "=========================================="
echo ""
echo "ğŸ’¡ å¦‚æœè®­ç»ƒè¿˜æ˜¯ç«‹å³å´©æºƒï¼Œè¯·å°è¯•ï¼š"
echo "   1. åœ¨Windowsä¸­é™ä½GPUæ€§èƒ½æ¨¡å¼"
echo "   2. é™åˆ¶WSL2å†…å­˜ (åˆ›å»º .wslconfig)"
echo "   3. æ›´æ–°NVIDIAé©±åŠ¨"
echo "   4. æ£€æŸ¥Windowsäº‹ä»¶æŸ¥çœ‹å™¨ä¸­çš„é”™è¯¯æ—¥å¿—"
echo ""
