#!/usr/bin/env python3
"""
å¿«é€Ÿæµ‹è¯•SEA_Attention_Adaptiveæ¨¡å—çš„æ•°å€¼ç¨³å®šæ€§
"""

import torch
import torch.nn as nn
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "ultralytics"))

def test_sea_attention_stability():
    """æµ‹è¯•SEAæ³¨æ„åŠ›æ¨¡å—çš„æ•°å€¼ç¨³å®šæ€§"""
    print("ğŸ§ª æµ‹è¯•SEA_Attention_Adaptiveæ•°å€¼ç¨³å®šæ€§...")
    
    try:
        from ultralytics.nn.modules.enhanced_attention import SEA_Attention_Adaptive
        
        # æµ‹è¯•ä¸åŒé€šé“æ•°å’Œè¾“å…¥å°ºå¯¸
        test_cases = [
            (160, 40, 40),  # P4å±‚
            (256, 20, 20),  # P5å±‚
            (128, 80, 80),  # P3å±‚
        ]
        
        for channels, h, w in test_cases:
            print(f"\nğŸ“Š æµ‹è¯• channels={channels}, size={h}x{w}")
            
            # åˆ›å»ºæ¨¡å—
            sea_module = SEA_Attention_Adaptive(channels)
            sea_module.eval()
            
            # åˆ›å»ºæµ‹è¯•è¾“å…¥
            x = torch.randn(2, channels, h, w)  # batch=2
            
            # æµ‹è¯•æ­£å¸¸æƒ…å†µ
            print("  âœ… æ­£å¸¸è¾“å…¥æµ‹è¯•...")
            with torch.no_grad():
                output = sea_module(x)
                if torch.isnan(output).any():
                    print("  âŒ æ­£å¸¸è¾“å…¥äº§ç”Ÿ nan")
                    return False
                else:
                    print(f"  âœ… è¾“å‡ºèŒƒå›´: [{output.min():.4f}, {output.max():.4f}]")
            
            # æµ‹è¯•æç«¯æƒ…å†µ
            print("  ğŸ”¥ æç«¯è¾“å…¥æµ‹è¯•...")
            x_extreme = torch.randn(2, channels, h, w) * 100  # å¤§æ•°å€¼
            with torch.no_grad():
                output_extreme = sea_module(x_extreme)
                if torch.isnan(output_extreme).any():
                    print("  âš ï¸  æç«¯è¾“å…¥äº§ç”Ÿ nan (å·²é€šè¿‡é™çº§å¤„ç†)")
                else:
                    print(f"  âœ… æç«¯è¾“å…¥è¾“å‡ºèŒƒå›´: [{output_extreme.min():.4f}, {output_extreme.max():.4f}]")
            
            # æµ‹è¯•åŒ…å«nançš„è¾“å…¥
            print("  ğŸ’¥ nanè¾“å…¥æµ‹è¯•...")
            x_nan = torch.randn(2, channels, h, w)
            x_nan[0, 0, 0, 0] = float('nan')
            with torch.no_grad():
                output_nan = sea_module(x_nan)
                if torch.isnan(output_nan).any():
                    print("  âš ï¸  nanè¾“å…¥äº§ç”Ÿ nanè¾“å‡º")
                else:
                    print("  âœ… nanè¾“å…¥å·²è¢«å¤„ç†ï¼Œè¾“å‡ºæ­£å¸¸")
        
        print("\nğŸ‰ SEA_Attention_Adaptive æ•°å€¼ç¨³å®šæ€§æµ‹è¯•å®Œæˆ!")
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_gradient_flow():
    """æµ‹è¯•æ¢¯åº¦æµåŠ¨"""
    print("\nğŸ”„ æµ‹è¯•æ¢¯åº¦æµåŠ¨...")
    
    try:
        from ultralytics.nn.modules.enhanced_attention import SEA_Attention_Adaptive
        
        # åˆ›å»ºæ¨¡å—
        sea_module = SEA_Attention_Adaptive(160)
        sea_module.train()
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        x = torch.randn(1, 160, 40, 40, requires_grad=True)
        
        # å‰å‘ä¼ æ’­
        output = sea_module(x)
        loss = output.sum()
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        # æ£€æŸ¥æ¢¯åº¦
        has_grad = False
        for name, param in sea_module.named_parameters():
            if param.grad is not None:
                if torch.isnan(param.grad).any():
                    print(f"  âŒ {name} æ¢¯åº¦åŒ…å« nan")
                    return False
                else:
                    has_grad = True
                    print(f"  âœ… {name} æ¢¯åº¦æ­£å¸¸: [{param.grad.min():.6f}, {param.grad.max():.6f}]")
        
        if has_grad:
            print("  ğŸ‰ æ¢¯åº¦æµåŠ¨æµ‹è¯•é€šè¿‡!")
            return True
        else:
            print("  âš ï¸  æ²¡æœ‰æ£€æµ‹åˆ°æ¢¯åº¦")
            return False
            
    except Exception as e:
        print(f"âŒ æ¢¯åº¦æµ‹è¯•å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ¤– SEA_Attention_Adaptive ç¨³å®šæ€§æµ‹è¯•")
    print("=" * 60)
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    
    # å¯ç”¨å¼‚å¸¸æ£€æµ‹
    torch.autograd.set_detect_anomaly(True)
    
    # è¿è¡Œæµ‹è¯•
    stability_ok = test_sea_attention_stability()
    gradient_ok = test_gradient_flow()
    
    if stability_ok and gradient_ok:
        print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡! SEAæ¨¡å—æ•°å€¼ç¨³å®šæ€§è‰¯å¥½")
        sys.exit(0)
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥! éœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
        sys.exit(1)
