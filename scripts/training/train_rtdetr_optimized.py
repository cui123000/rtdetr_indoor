#!/usr/bin/env python3
"""
ä¼˜åŒ–çš„RT-DETRè®­ç»ƒè„šæœ¬ - è§£å†³å†…å­˜æ³„æ¼å’Œé€Ÿåº¦é—®é¢˜
"""

import os
import sys
import yaml
import torch
import gc
from pathlib import Path
import psutil
import threading
import time

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "ultralytics"))

def setup_memory_optimization():
    """è®¾ç½®å†…å­˜ä¼˜åŒ–"""
    print("ğŸ”§ é…ç½®å†…å­˜ä¼˜åŒ–è®¾ç½®...")
    
    # CUDAå†…å­˜ç®¡ç†
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128,expandable_segments:True'
    os.environ['CUDA_LAUNCH_BLOCKING'] = '0'  # å¼‚æ­¥æ‰§è¡Œä»¥æé«˜é€Ÿåº¦
    
    # PyTorchè®¾ç½®
    torch.backends.cudnn.benchmark = True  # å¯ç”¨cuDNN auto-tuner
    torch.backends.cudnn.deterministic = False  # å…è®¸éç¡®å®šæ€§æ“ä½œä»¥æé«˜é€Ÿåº¦
    torch.backends.cuda.matmul.allow_tf32 = True  # å¯ç”¨TF32ä»¥æé«˜é€Ÿåº¦
    torch.backends.cudnn.allow_tf32 = True
    
    # è®¾ç½®çº¿ç¨‹æ•°
    torch.set_num_threads(4)
    os.environ['OMP_NUM_THREADS'] = '4'
    os.environ['MKL_NUM_THREADS'] = '4'
    
    if torch.cuda.is_available():
        # æ¸…ç†åˆå§‹GPUç¼“å­˜
        torch.cuda.empty_cache()
        gc.collect()
        
        # è®¾ç½®å†…å­˜åˆ†æ•°
        torch.cuda.set_per_process_memory_fraction(0.85)
        
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
        print(f"   CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"   PyTorchç‰ˆæœ¬: {torch.__version__}")

def monitor_memory():
    """å†…å­˜ç›‘æ§å™¨ï¼ˆåå°çº¿ç¨‹ï¼‰"""
    def monitor():
        while True:
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated(0) / 1e9
                cached = torch.cuda.memory_reserved(0) / 1e9
                if allocated > 8.0:  # å¦‚æœGPUå†…å­˜è¶…è¿‡8GBï¼Œè§¦å‘æ¸…ç†
                    torch.cuda.empty_cache()
                    gc.collect()
            time.sleep(30)  # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡
    
    monitor_thread = threading.Thread(target=monitor, daemon=True)
    monitor_thread.start()

def get_optimized_config(model_choice):
    """è·å–ä¼˜åŒ–çš„è®­ç»ƒé…ç½®"""
    
    model_configs = {
        '1': {
            'file': 'rtdetr-l.yaml',
            'name': 'rtdetr_l_optimized',
            'batch': 6,      # åŸå§‹æ¨¡å‹å¯ä»¥ç”¨æ›´å¤§batch
            'lr0': 0.001,
        },
        '2': {
            'file': 'rtdetr-mnv4-hybrid-m.yaml', 
            'name': 'rtdetr_mnv4_hybrid_optimized',
            'batch': 4,      # æ··åˆæ¨¡å‹ä¸­ç­‰batch
            'lr0': 0.0008,
        },
        '3': {
            'file': 'rtdetr-mnv4-hybrid-m-sea.yaml',
            'name': 'rtdetr_mnv4_sea_optimized',
            'batch': 3,      # SEAæ¨¡å‹æœ€å°batchä»¥èŠ‚çœå†…å­˜
            'lr0': 0.0005,
        }
    }
    
    if model_choice not in model_configs:
        raise ValueError(f"æ— æ•ˆçš„æ¨¡å‹é€‰æ‹©: {model_choice}")
    
    model_config = model_configs[model_choice]
    
    # åŸºç¡€ä¼˜åŒ–é…ç½®
    config = {
        'task': 'detect',
        'mode': 'train',
        'model': f'/home/cui/rtdetr_indoor/ultralytics/ultralytics/cfg/models/rt-detr/{model_config["file"]}',
        # æŒ‡å‘æ•°æ®ç›˜ä¸Šçš„æ•°æ®é›†YAML
        'data': '/root/autodl-tmp/database/homeobjects/HomeObjects-3K.yaml',
        
        # æ ¸å¿ƒè®­ç»ƒå‚æ•° - ä¼˜åŒ–ç‰ˆæœ¬
        'epochs': 100,
        'batch': model_config['batch'],
        'imgsz': 640,
        'patience': 15,
        
        # æ€§èƒ½ä¼˜åŒ–è®¾ç½®
        'device': '0',
        'workers': 2,           # å‡å°‘workersé¿å…CPUç“¶é¢ˆ
        'amp': True,            # æ··åˆç²¾åº¦è®­ç»ƒ
        'cache': False,         # å…³é—­ç¼“å­˜èŠ‚çœå†…å­˜
        'rect': True,           # çŸ©å½¢è®­ç»ƒæé«˜æ•ˆç‡
        'single_cls': False,    # å¤šç±»æ£€æµ‹
        
        # ä¼˜åŒ–å™¨è®¾ç½®
        'optimizer': 'AdamW',
        'lr0': model_config['lr0'],
        'lrf': 0.01,
        'momentum': 0.937,
        'weight_decay': 0.0005,
        'warmup_epochs': 3.0,
        'warmup_momentum': 0.8,
        'warmup_bias_lr': 0.1,
        'cos_lr': True,
        
        # æ•°æ®å¢å¼º - è½»é‡åŒ–è®¾ç½®
        'hsv_h': 0.015,
        'hsv_s': 0.7,
        'hsv_v': 0.4,
        'degrees': 0.0,         # å…³é—­æ—‹è½¬èŠ‚çœè®¡ç®—
        'translate': 0.1,
        'scale': 0.5,
        'shear': 0.0,          # å…³é—­å‰ªåˆ‡èŠ‚çœè®¡ç®—
        'perspective': 0.0,     # å…³é—­é€è§†å˜æ¢èŠ‚çœè®¡ç®—
        'flipud': 0.0,
        'fliplr': 0.5,
        'mosaic': 0.5,
        'mixup': 0.0,          # å…³é—­mixupèŠ‚çœå†…å­˜
        'copy_paste': 0.0,     # å…³é—­copy_pasteèŠ‚çœå†…å­˜
        
        # æŸå¤±æƒé‡
        'box': 7.5,
        'cls': 0.5, 
        'dfl': 1.5,
        
        # éªŒè¯è®¾ç½®
        'val': True,
        'conf': 0.25,
        'iou': 0.7,
        'max_det': 300,
        
        # ä¿å­˜è®¾ç½®
        'save': True,
        'save_period': 10,      # å‡å°‘ä¿å­˜é¢‘ç‡
        'project': 'runs/detect',
        'name': model_config['name'],
        'exist_ok': True,
        
        # å…¶ä»–ä¼˜åŒ–è®¾ç½®
        'verbose': True,
        'seed': 42,
        'deterministic': False,
        'plots': True,
        'close_mosaic': 10,
        
        # å†…å­˜ä¼˜åŒ–ä¸“ç”¨è®¾ç½®
        'overlap_mask': False,  # å…³é—­é‡å maskèŠ‚çœå†…å­˜
        'mask_ratio': 4,        # å‡å°‘maskæ¯”ä¾‹
    }
    
    return config

def cleanup_memory():
    """å¼ºåˆ¶æ¸…ç†å†…å­˜"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    gc.collect()

def train_model(model_choice):
    """è®­ç»ƒæ¨¡å‹"""
    try:
        # è®¾ç½®ç¯å¢ƒ
        setup_memory_optimization()
        monitor_memory()
        
        # å¯¼å…¥ultralytics
        print("ğŸ“¦ å¯¼å…¥Ultralytics...")
        from ultralytics import RTDETR
        
        # è·å–é…ç½®
        config = get_optimized_config(model_choice)
        
        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹: {config['name']}")
        print(f"ğŸ“„ é…ç½®æ–‡ä»¶: {config['model']}")
        print(f"ğŸ“Š æ‰¹æ¬¡å¤§å°: {config['batch']}")
        print(f"ğŸ¯ å­¦ä¹ ç‡: {config['lr0']}")
        print("=" * 60)
        
        # åˆ›å»ºæ¨¡å‹
        model = RTDETR(config['model'])
        
        # å¼€å§‹è®­ç»ƒ
        results = model.train(**{k: v for k, v in config.items() if k not in ['model']})
        
        print("\nâœ… è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“Š æœ€ä½³mAP50: {results.best_fitness}")
        
        # æ¸…ç†å†…å­˜
        cleanup_memory()
        
        return results
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå‡ºé”™: {e}")
        cleanup_memory()
        raise

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸƒâ€â™‚ï¸ RT-DETR ä¼˜åŒ–è®­ç»ƒè„šæœ¬")
    print("=" * 50)
    
    print("\nğŸ“‹ å¯ç”¨æ¨¡å‹:")
    print("1. RT-DETR-L (åŸå§‹)")
    print("2. RT-DETR + MobileNetV4")  
    print("3. RT-DETR + MobileNetV4 + SEA")
    
    while True:
        try:
            choice = input("\nè¯·é€‰æ‹©æ¨¡å‹ (1-3): ").strip()
            if choice in ['1', '2', '3']:
                break
            else:
                print("âŒ è¯·è¾“å…¥ 1, 2 æˆ– 3")
        except KeyboardInterrupt:
            print("\nğŸ‘‹ é€€å‡ºè®­ç»ƒ")
            return
    
    try:
        results = train_model(choice)
        print(f"\nğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆ!")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        cleanup_memory()
    except Exception as e:
        print(f"\nğŸ’¥ è®­ç»ƒå¤±è´¥: {e}")
        cleanup_memory()

if __name__ == "__main__":
    main()
