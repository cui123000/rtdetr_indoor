#!/usr/bin/env python3
"""
RTX 4090ä¸“ç”¨RT-DETRè‡ªåŠ¨è®­ç»ƒè„šæœ¬ - å…¨å±€é…ç½®ç‰ˆ
ä½¿ç”¨æ™ºèƒ½ç­›é€‰çš„HomeObjectsæ•°æ®é›†ï¼Œæœ€å¿«é€Ÿåº¦å®Œæˆè®­ç»ƒ
"""

import os
import sys
import yaml
import torch
import gc
import time
import shutil
from pathlib import Path
import threading

# ==================== æ¨¡å‹é…ç½®é€‰æ‹© ====================
MODEL_CONFIGS = {
    '1': {
        'file': 'rtdetr-l.yaml',
        'name': 'rtdetr_l_homeobjects_smart_optimized',
        'batch': 10,        # é™ä½æ‰¹æ¬¡é¿å…NaN
        'lr0': 0.0001,     # å¤§å¹…é™ä½å­¦ä¹ ç‡é˜²æ­¢NaN
        'workers': 4,      # å‡å°‘workersæå‡ç¨³å®šæ€§
        'epochs': 100,     # å‡å°‘epochsï¼Œæ•°æ®é›†è¾ƒå°
        'warmup_epochs': 10.0, # å¢åŠ é¢„çƒ­æœŸ
        'amp': False,      # ç¦ç”¨AMPæå‡ç¨³å®šæ€§
        'cache': False,    # ç¦ç”¨ç¼“å­˜é¿å…å†…å­˜é—®é¢˜
    },
    '2': {
        'file': 'rtdetr-mnv4-hybrid-m.yaml', 
        'name': 'rtdetr_mnv4_hybrid_rtx4090_safe',
        'batch': 8,        # æ›´ä¿å®ˆçš„batch size
        'lr0': 0.00008,    # æ›´ä½çš„å­¦ä¹ ç‡
        'workers': 4,      # å‡å°‘workers
        'epochs': 100,     # MNV4éœ€è¦æ›´å¤šè®­ç»ƒè½®æ•°
        'warmup_epochs': 12.0, # æ›´é•¿é¢„çƒ­æœŸ
        'amp': False,      # ç¦ç”¨AMP
        'cache': False,    # ç¦ç”¨ç¼“å­˜
    },
    '3': {
        'file': 'rtdetr-mnv4-hybrid-m-sea.yaml',
        'name': 'rtdetr_mnv4_sea_rtx4090_safe',
        'batch': 6,        # æœ€ä¿å®ˆçš„batch size
        'lr0': 0.00006,    # æœ€ä½å­¦ä¹ ç‡
        'workers': 4,      # å‡å°‘workers
        'epochs': 100,     # SEAç‰ˆæœ¬éœ€è¦æœ€å¤šè®­ç»ƒè½®æ•°
        'warmup_epochs': 15.0, # æœ€é•¿é¢„çƒ­æœŸ
        'amp': False,      # ç¦ç”¨AMP
        'cache': False,    # ç¦ç”¨ç¼“å­˜
    }   
}

# é€‰æ‹©è¦è®­ç»ƒçš„æ¨¡å‹ (ä¿®æ”¹è¿™é‡Œæ¥é€‰æ‹©ä¸åŒæ¨¡å‹)
SELECTED_MODEL = '1'  # '1'=RT-DETR-L, '2'=RT-DETR+MNV4, '3'=RT-DETR+MNV4+SEA

# æ·»åŠ æ—¶é—´ä¼°ç®—åŠŸèƒ½
def estimate_training_time():
    """ä¼°ç®—è®­ç»ƒæ—¶é—´"""
    current_model = get_model_config(SELECTED_MODEL)
    
    # åŸºäºRTX 4090çš„æ€§èƒ½ä¼°ç®— (æ›´æ–°ä¸ºå®é™…è§‚å¯Ÿå€¼)
    rtx4090_speeds = {
        '1': 4.5,    # RT-DETR-L å®é™…è§‚å¯Ÿé€Ÿåº¦æ›´æ–°
        '2': 5.8,    # RT-DETR-MNV4 é¢„è®¡é€Ÿåº¦
        '3': 4.2     # RT-DETR-MNV4-SEA é¢„è®¡é€Ÿåº¦
    }
    
    estimated_speed = rtx4090_speeds.get(SELECTED_MODEL, 4.0)
    iterations_per_epoch = 6400 // current_model['batch']  # æ›´æ–°ä¸ºæ–°çš„è®­ç»ƒæ ·æœ¬æ•°
    seconds_per_epoch = iterations_per_epoch / estimated_speed
    total_hours = (seconds_per_epoch * current_model['epochs']) / 3600
    
    return {
        'speed': estimated_speed,
        'iterations_per_epoch': iterations_per_epoch,
        'seconds_per_epoch': seconds_per_epoch,
        'total_hours': total_hours,
        'epochs': current_model['epochs']
    }

# ==================== å…¨å±€è®­ç»ƒé…ç½® ====================
def get_model_config(model_choice):
    """è·å–é€‰å®šæ¨¡å‹çš„é…ç½®"""
    if model_choice not in MODEL_CONFIGS:
        raise ValueError(f"æ— æ•ˆçš„æ¨¡å‹é€‰æ‹©: {model_choice}. å¯é€‰: {list(MODEL_CONFIGS.keys())}")
    return MODEL_CONFIGS[model_choice]

# è·å–å½“å‰é€‰æ‹©çš„æ¨¡å‹é…ç½®
current_model = get_model_config(SELECTED_MODEL)

GLOBAL_CONFIG = {
    # è·¯å¾„é…ç½®
    'dataset_path': '/home/cui/rtdetr_indoor/datasets/homeobjects_extended_yolo_smart/homeobjects_extended_smart.yaml',
    'model_config': f'/home/cui/rtdetr_indoor/ultralytics/ultralytics/cfg/models/rt-detr/{current_model["file"]}',
    'save_dir': '/root/autodl-tmp/rtdetr_weights',  # æƒé‡ä¿å­˜ç›®å½•
    'project_name': current_model['name'],
    
    # è®­ç»ƒå‚æ•° - ä½¿ç”¨æ¨¡å‹ç‰¹å®šé…ç½®
    'epochs': current_model['epochs'],     # è®­ç»ƒè½®æ•°
    'batch_size': current_model['batch'],  # ä½¿ç”¨æ¨¡å‹ç‰¹å®šæ‰¹æ¬¡å¤§å°
    'img_size': 640,                      # è¾“å…¥å›¾åƒå°ºå¯¸
    'workers': current_model['workers'],   # ä½¿ç”¨æ¨¡å‹ç‰¹å®šworkersæ•°
    'patience': 40,                       # è¿›ä¸€æ­¥å¢åŠ patience
    
    # å­¦ä¹ ç‡ç­–ç•¥ - ä½¿ç”¨æ¨¡å‹ç‰¹å®šé…ç½®
    'lr0': current_model['lr0'],          # ä½¿ç”¨æ¨¡å‹ç‰¹å®šå­¦ä¹ ç‡
    'lrf': 0.2,                          # æé«˜æœ€ç»ˆå­¦ä¹ ç‡å› å­
    'warmup_epochs': current_model['warmup_epochs'], # ä½¿ç”¨æ¨¡å‹ç‰¹å®šé¢„çƒ­è½®æ•°
    'cos_lr': True,                      # ä½™å¼¦å­¦ä¹ ç‡è¡°å‡
    
    # ä¼˜åŒ–å™¨è®¾ç½® - æ›´ä¿å®ˆå‚æ•°
    'optimizer': 'AdamW',
    'weight_decay': 0.00005,             # å¤§å¹…é™ä½æƒé‡è¡°å‡
    'momentum': 0.8,                     # é™ä½momentum
    
    # ä¿®å¤éªŒè¯é—®é¢˜çš„å…³é”®è®¾ç½®
    'save_period': 10, 
    'plots': True,
    'save_json': True,         # ä¿å­˜éªŒè¯ç»“æœJSONç”¨äºåˆ†æ
    
    # æ•°æ®å¢å¼º - æåº¦ä¿å®ˆé˜²æ­¢è®­ç»ƒä¸ç¨³å®š
    'hsv_h': 0.005,          # æå°è‰²è°ƒå˜åŒ–
    'hsv_s': 0.1,            # æå°é¥±å’Œåº¦å˜åŒ–
    'hsv_v': 0.1,            # æå°æ˜åº¦å˜åŒ–
    'degrees': 1.0,          # æå°æ—‹è½¬
    'translate': 0.02,       # æå°å¹³ç§»
    'scale': 0.1,            # æå°ç¼©æ”¾
    'fliplr': 0.3,           # å‡å°‘ç¿»è½¬
    'mosaic': 0.1,           # å¤§å¹…å‡å°‘mosaic
    'mixup': 0.0,            # å®Œå…¨ç¦ç”¨mixup
    'copy_paste': 0.0,       # å®Œå…¨ç¦ç”¨copy_paste
    
    # RTX 4090ä¸“ç”¨ä¼˜åŒ– - ä½¿ç”¨æ¨¡å‹ç‰¹å®šç¨³å®šæ€§è®¾ç½®
    'amp': current_model.get('amp', False),    # ä½¿ç”¨æ¨¡å‹ç‰¹å®šAMPè®¾ç½®
    'cache': current_model.get('cache', False), # ä½¿ç”¨æ¨¡å‹ç‰¹å®šç¼“å­˜è®¾ç½®
    'rect': False,             # å…³é—­çŸ©å½¢è®­ç»ƒï¼Œä½¿ç”¨æ ‡å‡†æ­£æ–¹å½¢è®­ç»ƒ - é‡è¦!
    'single_cls': False,
    'close_mosaic': 30,        # æå‰æ›´å¤šå…³é—­mosaic
    
    # GPUè®¾ç½® - ç¨³å®šæ€§ä¼˜å…ˆï¼Œé¿å…ç¡®å®šæ€§è­¦å‘Š
    'device': '0',             # ä½¿ç”¨ç¬¬ä¸€å—GPU
    'dnn': False,
    'half': False,             # ç¦ç”¨half precision
    'deterministic': False,    # ç¦ç”¨ä¸¥æ ¼ç¡®å®šæ€§é¿å…CuBLASè­¦å‘Š
    'seed': 42,                # ä¿æŒéšæœºç§å­ç¡®ä¿ç›¸å¯¹ä¸€è‡´æ€§
    'verbose': True,
    
    # éªŒè¯å’Œæ£€æµ‹è®¾ç½® - å…³é”®ä¿®å¤RT-DETRéªŒè¯é”™è¯¯
    'val': True,
    'conf': 0.001,             # é™ä½ç½®ä¿¡åº¦é˜ˆå€¼
    'iou': 0.6,                # é™ä½IoUé˜ˆå€¼
    'max_det': 300,
    'augment': False,          # éªŒè¯æ—¶ä¸ä½¿ç”¨å¢å¼º
    'save_txt': False,         # ç¦ç”¨æ–‡æœ¬ä¿å­˜
    'save_conf': False,        # ç¦ç”¨ç½®ä¿¡åº¦ä¿å­˜
    'save_crop': False,        # ç¦ç”¨è£å‰ªä¿å­˜
}

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "ultralytics"))

def setup_rtx4090_environment():
    """è®¾ç½®RTX 4090ä¼˜åŒ–ç¯å¢ƒ"""
    print("ğŸš€ è®¾ç½®RTX 4090ä¼˜åŒ–ç¯å¢ƒ...")
    
    # ä¿®å¤æ–‡ä»¶æè¿°ç¬¦é™åˆ¶
    try:
        import resource
        soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)
        new_soft = min(65536, hard)
        resource.setrlimit(resource.RLIMIT_NOFILE, (new_soft, hard))
        print(f"   âœ… æ–‡ä»¶æè¿°ç¬¦é™åˆ¶: {new_soft}")
    except Exception as e:
        print(f"   âš ï¸ æ— æ³•è®¾ç½®æ–‡ä»¶æè¿°ç¬¦: {e}")
    
    # RTX 4090ä¸“ç”¨CUDAä¼˜åŒ– - ä¿®å¤å†…å­˜åˆ†é…å™¨é”™è¯¯
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:256,expandable_segments:False'
    os.environ['CUDA_LAUNCH_BLOCKING'] = '0'
    os.environ['TORCH_CUDNN_V8_API_ENABLED'] = '1'
    os.environ['OMP_NUM_THREADS'] = '6'   # å‡å°‘çº¿ç¨‹æ•°é¿å…å†²çª
    os.environ['MKL_NUM_THREADS'] = '6'
    os.environ['TORCH_NUM_WORKERS'] = str(current_model['workers'])
    
    # ç¦ç”¨æœ‰é—®é¢˜çš„CUDAåŠŸèƒ½
    os.environ['CUDA_MODULE_LOADING'] = 'LAZY'
    os.environ['TORCH_CUDA_ARCH_LIST'] = ''  # è®©PyTorchè‡ªåŠ¨æ£€æµ‹
    
    # PyTorchä¼˜åŒ–è®¾ç½®
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    torch.set_num_threads(6)  # å‡å°‘çº¿ç¨‹æ•°é¿å…å†²çª
    
    if torch.cuda.is_available():
        torch.cuda.set_per_process_memory_fraction(0.85)  # ä½¿ç”¨85%æ˜¾å­˜ï¼Œæ›´å®‰å…¨
        torch.cuda.empty_cache()
        
        # è¯¦ç»†GPUä¿¡æ¯
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        
        print(f"   âœ… GPU: {gpu_name}")
        print(f"   âœ… æ€»æ˜¾å­˜: {gpu_memory:.1f}GB")
        print(f"   âœ… å¯ç”¨æ˜¾å­˜: {gpu_memory * 0.85:.1f}GB")
        print(f"   âœ… TF32ä¼˜åŒ–: å·²å¯ç”¨")
        print(f"   âš ï¸ å®‰å…¨æ¨¡å¼: expandable_segments=False")
        
        # GPUæ€§èƒ½æµ‹è¯•
        try:
            test_tensor = torch.randn(1000, 1000).cuda()
            start_time = time.time()
            for _ in range(100):
                _ = torch.mm(test_tensor, test_tensor)
            torch.cuda.synchronize()
            gpu_test_time = time.time() - start_time
            del test_tensor
            torch.cuda.empty_cache()
            print(f"   âœ… GPUæ€§èƒ½æµ‹è¯•: {gpu_test_time:.3f}s (æ­£å¸¸ < 1.0s)")
        except Exception as e:
            print(f"   âŒ GPUæµ‹è¯•å¤±è´¥: {e}")
            raise
    else:
        raise RuntimeError("âŒ CUDAä¸å¯ç”¨")

def check_dataset():
    """æ£€æŸ¥æ•°æ®é›†"""
    print("ğŸ“Š æ£€æŸ¥æ•°æ®é›†...")
    
    dataset_path = Path(GLOBAL_CONFIG['dataset_path'])
    if not dataset_path.exists():
        raise FileNotFoundError(f"âŒ æ•°æ®é›†é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {dataset_path}")
    
    # è·å–æ•°æ®é›†æ ¹ç›®å½•
    with open(dataset_path, 'r') as f:
        dataset_config = yaml.safe_load(f)
    
    dataset_root = Path(dataset_config['path'])
    train_dir = dataset_root / dataset_config['train']
    val_dir = dataset_root / dataset_config['val']
    
    if not train_dir.exists() or not val_dir.exists():
        raise FileNotFoundError(f"âŒ æ•°æ®é›†å›¾åƒç›®å½•ä¸å­˜åœ¨")
    
    # ç»Ÿè®¡å›¾åƒæ•°é‡
    train_count = len(list(train_dir.glob('*.jpg')))
    val_count = len(list(val_dir.glob('*.jpg')))
    
    print(f"   âœ… è®­ç»ƒå›¾åƒ: {train_count}")
    print(f"   âœ… éªŒè¯å›¾åƒ: {val_count}")
    print(f"   âœ… æ€»è®¡: {train_count + val_count}")
    print(f"   âœ… ç±»åˆ«æ•°: {dataset_config['nc']}")
    
    return dataset_config

def setup_save_directory():
    """è®¾ç½®ä¿å­˜ç›®å½•"""
    print("ğŸ’¾ è®¾ç½®æƒé‡ä¿å­˜ç›®å½•...")
    
    save_dir = Path(GLOBAL_CONFIG['save_dir'])
    
    # å°è¯•åˆ›å»ºç›®å½•
    try:
        save_dir.mkdir(parents=True, exist_ok=True)
        print(f"   âœ… æƒé‡ä¿å­˜ç›®å½•: {save_dir}")
    except PermissionError:
        # å¦‚æœæ²¡æœ‰æƒé™ï¼Œä½¿ç”¨ç”¨æˆ·ä¸»ç›®å½•
        alternative_dir = Path.home() / 'rtdetr_weights'
        alternative_dir.mkdir(parents=True, exist_ok=True)
        GLOBAL_CONFIG['save_dir'] = str(alternative_dir)
        print(f"   âš ï¸ æƒé™ä¸è¶³ï¼Œä½¿ç”¨å¤‡ç”¨ç›®å½•: {alternative_dir}")
        
    return Path(GLOBAL_CONFIG['save_dir'])

def create_training_config():
    """åˆ›å»ºè®­ç»ƒé…ç½®"""
    print("âš™ï¸ åˆ›å»ºè®­ç»ƒé…ç½®...")
    
    config = {
        'task': 'detect',
        'mode': 'train',
        'model': GLOBAL_CONFIG['model_config'],
        'data': GLOBAL_CONFIG['dataset_path'],
        
        # è®­ç»ƒå‚æ•°
        'epochs': GLOBAL_CONFIG['epochs'],
        'batch': GLOBAL_CONFIG['batch_size'],
        'imgsz': GLOBAL_CONFIG['img_size'],
        'patience': GLOBAL_CONFIG['patience'],
        'workers': GLOBAL_CONFIG['workers'],
        'device': GLOBAL_CONFIG['device'],
        
        # ä¼˜åŒ–è®¾ç½®
        'optimizer': GLOBAL_CONFIG['optimizer'],
        'lr0': GLOBAL_CONFIG['lr0'],
        'lrf': GLOBAL_CONFIG['lrf'],
        'momentum': GLOBAL_CONFIG['momentum'],
        'weight_decay': GLOBAL_CONFIG['weight_decay'],
        'warmup_epochs': GLOBAL_CONFIG['warmup_epochs'],
        'warmup_momentum': 0.8,
        'warmup_bias_lr': 0.1,
        'cos_lr': GLOBAL_CONFIG['cos_lr'],
        
        # æ•°æ®å¢å¼º
        'hsv_h': GLOBAL_CONFIG['hsv_h'],
        'hsv_s': GLOBAL_CONFIG['hsv_s'],
        'hsv_v': GLOBAL_CONFIG['hsv_v'],
        'degrees': GLOBAL_CONFIG['degrees'],
        'translate': GLOBAL_CONFIG['translate'],
        'scale': GLOBAL_CONFIG['scale'],
        'fliplr': GLOBAL_CONFIG['fliplr'],
        'mosaic': GLOBAL_CONFIG['mosaic'],
        'mixup': GLOBAL_CONFIG['mixup'],
        'copy_paste': GLOBAL_CONFIG['copy_paste'],
        'shear': 0.0,
        'perspective': 0.0,
        'flipud': 0.0,
        
        # RTX 4090ä¼˜åŒ–
        'amp': GLOBAL_CONFIG['amp'],
        'cache': GLOBAL_CONFIG['cache'],
        'rect': GLOBAL_CONFIG['rect'],
        'single_cls': GLOBAL_CONFIG['single_cls'],
        'dnn': GLOBAL_CONFIG['dnn'],
        'half': GLOBAL_CONFIG['half'],
        'deterministic': GLOBAL_CONFIG['deterministic'],
        'close_mosaic': GLOBAL_CONFIG['close_mosaic'],
        
        # ä¿å­˜è®¾ç½®
        'save': True,
        'save_period': GLOBAL_CONFIG['save_period'],
        'save_json': GLOBAL_CONFIG['save_json'],
        'plots': GLOBAL_CONFIG['plots'],
        'val': GLOBAL_CONFIG['val'],
        'project': GLOBAL_CONFIG['save_dir'],
        'name': GLOBAL_CONFIG['project_name'],
        'exist_ok': True,
        
        # éªŒè¯è®¾ç½® - å…³é”®ä¿®å¤
        'conf': GLOBAL_CONFIG['conf'],
        'iou': GLOBAL_CONFIG['iou'],
        'max_det': GLOBAL_CONFIG['max_det'],
        'augment': GLOBAL_CONFIG['augment'],
        
        # RT-DETRç‰¹æ®Šè®¾ç½® - ä¿®å¤éªŒè¯é”™è¯¯
        'save_txt': False,         # ç¦ç”¨æ–‡æœ¬ä¿å­˜
        'save_conf': False,        # ç¦ç”¨ç½®ä¿¡åº¦ä¿å­˜
        'save_crop': False,        # ç¦ç”¨è£å‰ªä¿å­˜
        'rect': False,             # ç¡®ä¿ä½¿ç”¨æ–¹å½¢å›¾åƒé¿å…ç¼©æ”¾é—®é¢˜
        
        # æŸå¤±æƒé‡è°ƒæ•´ - é’ˆå¯¹RT-DETRä¼˜åŒ–
        'box': 7.5,
        'cls': 0.5,
        'dfl': 1.5,
        
        # å…¶ä»–è®¾ç½®
        'verbose': GLOBAL_CONFIG['verbose'],
        'seed': GLOBAL_CONFIG['seed'],
        'overlap_mask': True,
        'mask_ratio': 4,
        'dropout': 0.0,  # ç¦ç”¨dropoutä»¥è·å¾—æ›´ç¨³å®šçš„è®­ç»ƒ
    }
    
    return config

def gpu_memory_monitor():
    """GPUå†…å­˜ç›‘æ§ - æ›´æ¿€è¿›çš„æ¸…ç†"""
    def monitor():
        while True:
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated(0) / 1e9
                if allocated > 16.0:  # è¶…è¿‡16GBæ—¶æ¸…ç†ï¼Œæ›´ä¿å®ˆ
                    print(f"ğŸ§¹ è§¦å‘GPUå†…å­˜æ¸…ç†: {allocated:.1f}GB")
                    torch.cuda.empty_cache()
                    gc.collect()
                    torch.cuda.synchronize()
                    new_allocated = torch.cuda.memory_allocated(0) / 1e9
                    print(f"   æ¸…ç†å: {new_allocated:.1f}GB")
            time.sleep(10)  # æ›´é¢‘ç¹çš„æ£€æŸ¥
    
    monitor_thread = threading.Thread(target=monitor, daemon=True)
    monitor_thread.start()

def force_cuda_cleanup():
    """å¼ºåˆ¶CUDAå†…å­˜æ¸…ç†"""
    try:
        torch.cuda.empty_cache()
        if hasattr(torch.cuda, 'ipc_collect'):
            torch.cuda.ipc_collect()
        gc.collect()
        torch.cuda.synchronize()
        print("ğŸ§¹ å¼ºåˆ¶CUDAæ¸…ç†å®Œæˆ")
    except Exception as e:
        print(f"âš ï¸ CUDAæ¸…ç†è­¦å‘Š: {e}")

def copy_best_weights(results, config):
    """å¤åˆ¶æœ€ä½³æƒé‡åˆ°æŒ‡å®šä½ç½®"""
    try:
        project_dir = Path(config['project']) / config['name']
        weights_dir = project_dir / 'weights'
        best_weight = weights_dir / 'best.pt'
        
        if best_weight.exists():
            # å¤åˆ¶åˆ°ç›®æ ‡ç›®å½•
            final_name = f"homeobjects_rtdetr_best_{time.strftime('%Y%m%d_%H%M%S')}.pt"
            if config['project'].startswith('/root/autodl-tmp'):
                final_path = Path('/root/autodl-tmp') / final_name
            else:
                final_path = Path(config['project']) / final_name
                
            shutil.copy2(best_weight, final_path)
            print(f"âœ… æœ€ä½³æƒé‡å·²ä¿å­˜: {final_path}")
            
            # åˆ›å»ºä¿¡æ¯æ–‡ä»¶
            info_file = final_path.with_suffix('.txt')
            with open(info_file, 'w') as f:
                f.write(f"RT-DETR HomeObjectsè®­ç»ƒç»“æœ\n")
                f.write(f"è®­ç»ƒæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"æ¨¡å‹é€‰æ‹©: {SELECTED_MODEL} - {current_model['name']}\n")
                f.write(f"æ¨¡å‹æ–‡ä»¶: {current_model['file']}\n")
                f.write(f"æ•°æ®é›†: HomeObjectsæ‰©å±•ç‰ˆ (æ™ºèƒ½ç­›é€‰)\n")
                f.write(f"æ‰¹æ¬¡å¤§å°: {config['batch']}\n")
                f.write(f"å­¦ä¹ ç‡: {config['lr0']}\n")
                f.write(f"è®­ç»ƒè½®æ•°: {config['epochs']}\n")
                f.write(f"Workers: {config['workers']}\n")
                f.write(f"æƒé‡æ–‡ä»¶: {final_name}\n")
            
            print(f"ğŸ“„ è®­ç»ƒä¿¡æ¯å·²ä¿å­˜: {info_file}")
            
        else:
            print("âŒ æœªæ‰¾åˆ°æœ€ä½³æƒé‡æ–‡ä»¶")
            
    except Exception as e:
        print(f"âŒ æƒé‡å¤åˆ¶å¤±è´¥: {e}")

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("ğŸï¸ RT-DETR HomeObjects è‡ªåŠ¨è®­ç»ƒå™¨ (RTX 4090ä¼˜åŒ–)")
    print("=" * 70)
    
    # æ˜¾ç¤ºå¯é€‰æ¨¡å‹
    print("ğŸ“‹ å¯é€‰æ¨¡å‹é…ç½® (é˜²NaNç¨³å®šç‰ˆ):")
    for key, config in MODEL_CONFIGS.items():
        marker = "ğŸ‘‰" if key == SELECTED_MODEL else "  "
        print(f"{marker} {key}. {config['file']}")
        print(f"     batch={config['batch']}, lr={config['lr0']}, epochs={config['epochs']}")
        print(f"     warmup={config['warmup_epochs']}, amp={config['amp']}, cache={config['cache']}")
    
    print(f"\nğŸ¯ å½“å‰é€‰æ‹©: æ¨¡å‹ {SELECTED_MODEL} (ç¨³å®šé…ç½®)")
    print("ğŸ’¡ è¦æ›´æ”¹æ¨¡å‹ï¼Œè¯·ä¿®æ”¹è„šæœ¬ä¸­çš„ SELECTED_MODEL å˜é‡")
    print("ğŸ›¡ï¸ æ‰€æœ‰æ¨¡å‹å·²é…ç½®é˜²NaNå‚æ•°: ä½å­¦ä¹ ç‡ + ç¦ç”¨AMP + é•¿é¢„çƒ­æœŸ")
    
    # è®­ç»ƒæ—¶é—´ä¼°ç®—
    time_estimate = estimate_training_time()
    print(f"\nâ±ï¸ è®­ç»ƒæ—¶é—´ä¼°ç®—:")
    print(f"   é¢„è®¡é€Ÿåº¦: {time_estimate['speed']:.1f} it/s")
    print(f"   æ¯epochè¿­ä»£æ•°: {time_estimate['iterations_per_epoch']}")
    print(f"   æ¯epochæ—¶é—´: {time_estimate['seconds_per_epoch']/60:.1f} åˆ†é’Ÿ")
    print(f"   æ€»è®­ç»ƒæ—¶é—´: {time_estimate['total_hours']:.1f} å°æ—¶")
    print("=" * 70)
    
    # äº¤äº’å¼ç¡®è®¤
    print("\nğŸ¤” è®­ç»ƒå‰ç¡®è®¤:")
    print(f"1. æ¨¡å‹: {current_model['name']}")
    print(f"2. é¢„è®¡æ—¶é—´: {time_estimate['total_hours']:.1f} å°æ—¶")
    print(f"3. Batchå¤§å°: {current_model['batch']}")
    
    # æ·»åŠ ç®€å•çš„ç”¨æˆ·äº¤äº’
    try:
        confirm = input("\nç¡®è®¤å¼€å§‹è®­ç»ƒ? (y/n): ").strip().lower()
        if confirm != 'y':
            print("âŒ å–æ¶ˆè®­ç»ƒ")
            return
    except KeyboardInterrupt:
        print("\nâŒ ç”¨æˆ·å–æ¶ˆ")
        return
    
    try:
        # ç¯å¢ƒè®¾ç½®
        setup_rtx4090_environment()
        dataset_config = check_dataset()
        save_dir = setup_save_directory()
        
        # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
        print("\nğŸ¯ æœ€ç»ˆè®­ç»ƒé…ç½®:")
        print(f"   æ¨¡å‹é€‰æ‹©: {SELECTED_MODEL} - {current_model['name']}")
        print(f"   æ¨¡å‹æ–‡ä»¶: {current_model['file']}")
        print(f"   æ•°æ®é›†: HomeObjectsæ‰©å±•ç‰ˆ ({dataset_config['nc']}ç±»)")
        print(f"   æ‰¹æ¬¡å¤§å°: {GLOBAL_CONFIG['batch_size']}")
        print(f"   è®­ç»ƒè½®æ•°: {GLOBAL_CONFIG['epochs']}")
        print(f"   å­¦ä¹ ç‡: {GLOBAL_CONFIG['lr0']}")
        print(f"   Workers: {GLOBAL_CONFIG['workers']}")
        print(f"   æƒé‡ä¿å­˜: {save_dir}")
        print(f"   é¢„çƒ­è½®æ•°: {GLOBAL_CONFIG['warmup_epochs']}")
        print("=" * 70)
        
        # å¯åŠ¨å†…å­˜ç›‘æ§
        gpu_memory_monitor()
        
        # å¼ºåˆ¶æ¸…ç†åˆå§‹çŠ¶æ€
        force_cuda_cleanup()
        
        # å¯¼å…¥ultralytics
        print("ğŸ“¦ å¯¼å…¥Ultralytics...")
        from ultralytics import RTDETR
        
        # åˆ›å»ºé…ç½®
        config = create_training_config()
        
        # å®‰å…¨çš„GPUé¢„çƒ­ - é¿å…å†…å­˜åˆ†é…å™¨é”™è¯¯
        print("ğŸ”¥ å®‰å…¨GPUé¢„çƒ­...")
        try:
            # ä½¿ç”¨å°æ‰¹æ¬¡é¢„çƒ­é¿å…å†…å­˜é—®é¢˜
            small_data = torch.randn(2, 3, 640, 640).cuda()
            small_conv = torch.nn.Conv2d(3, 32, 3, padding=1).cuda()
            
            # é¢„çƒ­å¾ªç¯
            with torch.no_grad():
                for i in range(5):
                    _ = small_conv(small_data)
                    if i % 2 == 0:
                        torch.cuda.empty_cache()
            
            torch.cuda.synchronize()
            del small_data, small_conv
            force_cuda_cleanup()
            print("âœ… GPUé¢„çƒ­å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ GPUé¢„çƒ­è­¦å‘Š: {e}")
            force_cuda_cleanup()
        
        # åˆ›å»ºæ¨¡å‹å¹¶å¼€å§‹è®­ç»ƒ
        print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
        try:
            model = RTDETR(config['model'])
            print("âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            print(f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
            force_cuda_cleanup()
            raise
        
        # è®­ç»ƒå¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # å¼€å§‹è®­ç»ƒ - æ·»åŠ å¼‚å¸¸å¤„ç†
        try:
            print("ğŸ¯ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
            results = model.train(**{k: v for k, v in config.items() if k != 'model'})
            
            training_time = (time.time() - start_time) / 3600  # è½¬æ¢ä¸ºå°æ—¶
            print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ! å®é™…ç”¨æ—¶: {training_time:.2f} å°æ—¶")
            
            # å¤„ç†è®­ç»ƒç»“æœ
            copy_best_weights(results, config)
            
            # è®­ç»ƒæ€»ç»“
            print(f"\nğŸ“Š è®­ç»ƒæ€»ç»“:")
            print(f"   æ¨¡å‹: {current_model['name']}")
            print(f"   å®é™…è®­ç»ƒæ—¶é—´: {training_time:.2f} å°æ—¶")
            print(f"   é¢„ä¼°æ—¶é—´: {time_estimate['total_hours']:.1f} å°æ—¶")
            print(f"   æ—¶é—´å·®å¼‚: {abs(training_time - time_estimate['total_hours']):.2f} å°æ—¶")
            
        except RuntimeError as e:
            if "expandable_segment" in str(e) or "CUDA" in str(e):
                print(f"\nâŒ CUDAå†…å­˜åˆ†é…å™¨é”™è¯¯: {e}")
                print("ğŸ’¡ å»ºè®®çš„è§£å†³æ–¹æ¡ˆ:")
                print("   1. é‡å¯Pythonè¿›ç¨‹æ¸…ç†CUDAçŠ¶æ€")
                print("   2. é™ä½batch size (å½“å‰: {})".format(current_model['batch']))
                print("   3. æ£€æŸ¥GPUé©±åŠ¨å’ŒPyTorchç‰ˆæœ¬å…¼å®¹æ€§")
                force_cuda_cleanup()
                raise
            else:
                print(f"\nâŒ è®­ç»ƒè¿è¡Œæ—¶é”™è¯¯: {e}")
                force_cuda_cleanup()
                raise
        except Exception as e:
            print(f"\nâŒ æœªçŸ¥è®­ç»ƒé”™è¯¯: {e}")
            force_cuda_cleanup()
            raise
        
        # æœ€ç»ˆæ¸…ç†
        try:
            del model
            force_cuda_cleanup()
        except:
            pass
        
        print("âœ… è®­ç»ƒä»»åŠ¡å…¨éƒ¨å®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå‡ºé”™: {e}")
        
        # è¯¦ç»†çš„é”™è¯¯åˆ†æ
        if "expandable_segment" in str(e):
            print("ğŸ”§ CUDAå†…å­˜åˆ†é…å™¨é”™è¯¯è§£å†³æ–¹æ¡ˆ:")
            print("   - è¿™æ˜¯PyTorchçš„CUDAå†…å­˜ç®¡ç†é—®é¢˜")
            print("   - å·²åœ¨è„šæœ¬ä¸­è®¾ç½® expandable_segments=False")
            print("   - è¯·é‡å¯Pythonè¿›ç¨‹åé‡æ–°è¿è¡Œ")
        elif "CUDA out of memory" in str(e):
            print("ğŸ”§ GPUå†…å­˜ä¸è¶³è§£å†³æ–¹æ¡ˆ:")
            print(f"   - å½“å‰batch size: {current_model['batch']}")
            print("   - å»ºè®®å‡å°‘batch sizeåˆ° 6-8")
            print("   - æˆ–å‡å°‘workersæ•°é‡")
        
        force_cuda_cleanup()
        raise

if __name__ == "__main__":
    main()