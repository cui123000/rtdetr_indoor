#!/usr/bin/env python3
"""
ä¸ºRTX 4090ä¼˜åŒ–çš„RT-DETRè®­ç»ƒè„šæœ¬
è§£å†³è®­ç»ƒæ…¢å’Œå†…å­˜æ³„æ¼é—®é¢˜ + æ–‡ä»¶æè¿°ç¬¦é—®é¢˜
"""

import os
import sys
import yaml
import torch
import gc
from pathlib import Path
import threading
import time
import resource
import multiprocessing

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "ultralytics"))

def fix_file_descriptor_limit():
    """ä¿®å¤æ–‡ä»¶æè¿°ç¬¦é™åˆ¶é—®é¢˜"""
    print("ğŸ”§ ä¿®å¤æ–‡ä»¶æè¿°ç¬¦é™åˆ¶...")
    
    try:
        # è·å–å½“å‰é™åˆ¶
        soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)
        print(f"   å½“å‰æ–‡ä»¶æè¿°ç¬¦é™åˆ¶: {soft} (è½¯é™åˆ¶) / {hard} (ç¡¬é™åˆ¶)")
        
        # è®¾ç½®æ›´é«˜çš„è½¯é™åˆ¶
        new_soft = min(65536, hard)  # è®¾ç½®ä¸º65536æˆ–ç¡¬é™åˆ¶çš„è¾ƒå°å€¼
        resource.setrlimit(resource.RLIMIT_NOFILE, (new_soft, hard))
        
        print(f"   âœ… æ–°çš„æ–‡ä»¶æè¿°ç¬¦é™åˆ¶: {new_soft}")
        
        # è®¾ç½®ç¯å¢ƒå˜é‡é™åˆ¶workers
        os.environ['TORCH_NUM_WORKERS'] = '2'  # å¼ºåˆ¶é™åˆ¶workersæ•°é‡
        
    except Exception as e:
        print(f"   âš ï¸ æ— æ³•ä¿®æ”¹æ–‡ä»¶æè¿°ç¬¦é™åˆ¶: {e}")
        print("   ğŸ’¡ å»ºè®®åœ¨ç³»ç»Ÿçº§åˆ«å¢åŠ æ–‡ä»¶æè¿°ç¬¦é™åˆ¶")

def setup_rtx4090_optimization():
    """ä¸ºRTX 4090è®¾ç½®ä¸“é—¨çš„ä¼˜åŒ–"""
    print("ğŸš€ ä¸ºRTX 4090è®¾ç½®ä¸“é—¨ä¼˜åŒ–...")
    
    # é¦–å…ˆä¿®å¤æ–‡ä»¶æè¿°ç¬¦é—®é¢˜
    fix_file_descriptor_limit()
    
    # RTX 4090ä¸“ç”¨CUDAè®¾ç½® - ä¿å®ˆé…ç½®é¿å…é©±åŠ¨é”™è¯¯
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:256,expandable_segments:False'
    os.environ['CUDA_LAUNCH_BLOCKING'] = '0'
    os.environ['TORCH_CUDNN_V8_API_ENABLED'] = '1'
    
    # å¯ç”¨RTX 4090çš„ä¼˜åŒ–ç‰¹æ€§ï¼Œä½†æ›´ä¿å®ˆ
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    
    # è®¾ç½®åˆç†çš„çº¿ç¨‹æ•°
    torch.set_num_threads(4)  # å‡å°‘çº¿ç¨‹æ•°é¿å…èµ„æºç«äº‰
    os.environ['OMP_NUM_THREADS'] = '4'
    os.environ['MKL_NUM_THREADS'] = '4'
    
    if torch.cuda.is_available():
        # RTX 4090æ˜¾å­˜å……è¶³ï¼Œä½†ä¿å®ˆä½¿ç”¨85%é¿å…OOM
        torch.cuda.set_per_process_memory_fraction(0.85)
        
        # æ¸…ç†åˆå§‹ç¼“å­˜
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        
        print(f"   âœ… GPU: {torch.cuda.get_device_name(0)}")
        print(f"   âœ… æ˜¾å­˜é™åˆ¶: 85% (~22GB)")
        
        # æ£€æŸ¥CUDAçŠ¶æ€
        try:
            test_tensor = torch.randn(100, 100).cuda()
            del test_tensor
            torch.cuda.empty_cache()
            print("   âœ… CUDAçŠ¶æ€æ­£å¸¸")
        except Exception as e:
            print(f"   âŒ CUDAæµ‹è¯•å¤±è´¥: {e}")
            raise
        print(f"   âœ… TF32åŠ é€Ÿ: å¯ç”¨")
        print(f"   âœ… Flash Attention: å¯ç”¨")

def memory_monitor_rtx4090():
    """RTX 4090ä¸“ç”¨å†…å­˜ç›‘æ§"""
    def monitor():
        while True:
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated(0) / 1e9
                cached = torch.cuda.memory_reserved(0) / 1e9
                
                # RTX 4090æ˜¾å­˜é˜ˆå€¼æ›´é«˜
                if allocated > 20.0:  # 20GBä»¥ä¸Šæ—¶æ¸…ç†
                    torch.cuda.empty_cache()
                    gc.collect()
                    print(f"ğŸ§¹ è‡ªåŠ¨æ¸…ç†GPUå†…å­˜: {allocated:.1f}GB -> {torch.cuda.memory_allocated(0)/1e9:.1f}GB")
            
            time.sleep(15)  # æ¯15ç§’æ£€æŸ¥ä¸€æ¬¡
    
    monitor_thread = threading.Thread(target=monitor, daemon=True)
    monitor_thread.start()

def get_rtx4090_config(model_choice):
    """RTX 4090ä¼˜åŒ–çš„è®­ç»ƒé…ç½®"""

    model_configs = {
                '1': {
            'file': 'rtdetr-l.yaml',
            'name': 'rtdetr_l_homeobjects_smart',
            'batch': 16,       # RTX 4090ä¼˜åŒ–
            'lr0': 0.003,      # æ›´å¤§batchå¯¹åº”æ›´é«˜å­¦ä¹ ç‡
            'workers': 6,      # å¢åŠ workersæå‡æ•°æ®åŠ è½½é€Ÿåº¦
        },
        '2': {
            'file': 'rtdetr-mnv4-hybrid-m.yaml', 
            'name': 'rtdetr_mnv4_hybrid_rtx4090',
            'batch': 8,        # MNV4æ··åˆç‰ˆæœ¬
            'lr0': 0.0015,
            'workers': 4,      # åˆç†çš„workersæ•°é‡
        },
        '3': {
            'file': 'rtdetr-mnv4-hybrid-m-sea.yaml',
            'name': 'rtdetr_mnv4_sea_rtx4090',
            'batch': 6,        # SEAç‰ˆæœ¬æœ€ä¿å®ˆçš„batch
            'lr0': 0.0012,
            'workers': 4,      # åˆç†çš„workersæ•°é‡
        }   
    }

    if model_choice not in model_configs:
        raise ValueError(f"æ— æ•ˆçš„æ¨¡å‹é€‰æ‹©: {model_choice}")

    model_config = model_configs[model_choice]

    # RTX 4090ä¸“ç”¨é…ç½®
    config = {
        'task': 'detect',
        'mode': 'train',
        'model': f'/home/cui/rtdetr_indoor/ultralytics/ultralytics/cfg/models/rt-detr/{model_config["file"]}',
                'data': '/home/cui/rtdetr_indoor/datasets/homeobjects_extended_yolo_smart/homeobjects_extended_smart.yaml',

        # RTX 4090ä¼˜åŒ–çš„æ ¸å¿ƒå‚æ•°
        'epochs': 150,          # å¢åŠ è®­ç»ƒè½®æ•°ä»¥å……åˆ†è®­ç»ƒ
        'batch': model_config['batch'],
        'imgsz': 640,
        'patience': 25,         # å¢åŠ patienceé¿å…è¿‡æ—©åœæ­¢

        # ç¨³å®šæ€§ä¼˜åŒ–è®¾ç½® - é˜²æ­¢å†…å­˜æ³„æ¼
        'device': '0',
        'workers': model_config['workers'],
        'amp': True,            # å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
        'cache': 'ram',         # ç¼“å­˜åˆ°å†…å­˜ä»¥åŠ é€Ÿæ•°æ®åŠ è½½
        'rect': True,           # çŸ©å½¢è®­ç»ƒ
        'single_cls': False,

        # RTX 4090ä¼˜åŒ–çš„å­¦ä¹ ç‡è®¾ç½®
        'optimizer': 'AdamW',
        'lr0': model_config['lr0'],
        'lrf': 0.001,           # æ›´æ¿€è¿›çš„å­¦ä¹ ç‡è¡°å‡
        'momentum': 0.937,
        'weight_decay': 0.0005,
        'warmup_epochs': 3.0,
        'warmup_momentum': 0.8,
        'warmup_bias_lr': 0.1,
        'cos_lr': True,

        # å†…å­˜å®‰å…¨çš„æ•°æ®å¢å¼ºè®¾ç½® - ä¸ºäº†æœ€å¿«é€Ÿåº¦å…³é—­å¤æ‚å¢å¼º
        'hsv_h': 0.01,          # å‡å°‘HSVå¢å¼ºä»¥åŠ é€Ÿ
        'hsv_s': 0.5,
        'hsv_v': 0.3,
        'degrees': 0.0,         # å…³é—­æ—‹è½¬å‡å°‘è®¡ç®—
        'translate': 0.05,      # å‡å°‘å¹³ç§»å¢å¼º
        'scale': 0.3,           # å‡å°‘ç¼©æ”¾å¢å¼º
        'shear': 0.0,           # å…³é—­å‰ªåˆ‡å‡å°‘è®¡ç®—
        'perspective': 0.0,     # å…³é—­é€è§†å˜æ¢é˜²æ­¢å†…å­˜æ³„æ¼
        'flipud': 0.0,
        'fliplr': 0.3,          # å‡å°‘æ°´å¹³ç¿»è½¬
        'mosaic': 0.0,          # å…³é—­mosaicé˜²æ­¢å†…å­˜æ³„æ¼
        'mixup': 0.0,           # å…³é—­mixupé˜²æ­¢å†…å­˜æ³„æ¼
        'copy_paste': 0.0,      # å…³é—­copy_pasteé˜²æ­¢å†…å­˜æ³„æ¼

        # æŸå¤±æƒé‡
        'box': 7.5,
        'cls': 0.5,
        'dfl': 1.5,

        # éªŒè¯è®¾ç½®
        'val': True,
        'conf': 0.25,
        'iou': 0.7,
        'max_det': 300,

        # ä¿å­˜è®¾ç½® - åªä¿å­˜æœ€ä½³æƒé‡åˆ°æŒ‡å®šä½ç½®
                # ä¿å­˜è®¾ç½® - åªä¿å­˜æœ€ä½³æƒé‡åˆ°æŒ‡å®šä½ç½®
        'save': True,
        'save_period': -1,      # ç¦ç”¨å®šæœŸä¿å­˜
        'save_json': False,     # ä¸ä¿å­˜JSONç»“æœ
        'save_hybrid': False,   # ä¸ä¿å­˜æ··åˆæ ¼å¼
        'save_crop': False,     # ä¸ä¿å­˜è£å‰ªå›¾åƒ
        'save_txt': False,      # ä¸ä¿å­˜txtç»“æœ
        'project': '/root/autodl-tmp/rtdetr_weights',  # ç°åœ¨cuiç”¨æˆ·æœ‰æƒé™äº†
        'name': model_config['name'],
        'exist_ok': True,

        # RTX 4090ä¸“ç”¨è®¾ç½®
        'verbose': True,
        'seed': 42,
        'deterministic': False,
        'plots': True,
        'close_mosaic': 10,
        'overlap_mask': True,   # RTX 4090å¯ä»¥å¤„ç†é‡å mask
        'mask_ratio': 4,

        # é«˜çº§ä¼˜åŒ–è®¾ç½®
        'profile': False,       # å…³é—­æ€§èƒ½åˆ†æä»¥æé«˜é€Ÿåº¦
        'half': False,          # RTX 4090ç”¨FP16å¯èƒ½ä¸ç¨³å®šï¼Œç”¨AMPå°±å¤Ÿäº†
        'dnn': False,           # ä¸ä½¿ç”¨OpenCV DNN
    }

    return config

def train_with_rtx4090_optimization(model_choice):
    """RTX 4090ä¼˜åŒ–è®­ç»ƒ"""
    try:
        # è®¾ç½®ç¯å¢ƒ
        setup_rtx4090_optimization()
        memory_monitor_rtx4090()
        
        # å¯¼å…¥ultralytics
        print("ğŸ“¦ å¯¼å…¥Ultralytics...")
        from ultralytics import RTDETR
        
        # è·å–é…ç½®
        config = get_rtx4090_config(model_choice)
        
        print(f"\nğŸš€ RTX 4090ä¼˜åŒ–è®­ç»ƒå¼€å§‹")
        print(f"ğŸ“„ æ¨¡å‹: {config['model'].split('/')[-1]}")
        print(f"ğŸ“Š æ‰¹æ¬¡å¤§å°: {config['batch']}")
        print(f"ğŸ¯ å­¦ä¹ ç‡: {config['lr0']}")
        print(f"ğŸ‘¥ Workers: {config['workers']}")
        print(f"ğŸ§  å†…å­˜ç¼“å­˜: {config['cache']}")
        print("=" * 60)
        
        # æ˜¾å­˜é¢„çƒ­
        print("ğŸ”¥ GPUé¢„çƒ­ä¸­...")
        dummy_data = torch.randn(1, 3, 640, 640).cuda()
        for _ in range(10):
            _ = torch.nn.functional.conv2d(dummy_data, torch.randn(64, 3, 3, 3).cuda())
        torch.cuda.synchronize()
        del dummy_data
        torch.cuda.empty_cache()
        print("âœ… GPUé¢„çƒ­å®Œæˆ")
        
        # åˆ›å»ºæ¨¡å‹
        model = RTDETR(config['model'])
        
        # å¼€å§‹è®­ç»ƒ
        results = model.train(**{k: v for k, v in config.items() if k not in ['model']})
        
        print("\nğŸ‰ è®­ç»ƒå®Œæˆ!")
        
        # å¤åˆ¶æœ€ä½³æƒé‡åˆ°æŒ‡å®šä½ç½®
        try:
            import shutil
            from pathlib import Path
            
            # æŸ¥æ‰¾æœ€ä½³æƒé‡æ–‡ä»¶
            project_dir = Path(config['project']) / config['name']
            best_weight = project_dir / 'weights' / 'best.pt'
            last_weight = project_dir / 'weights' / 'last.pt'
            
            if best_weight.exists():
                # å¤åˆ¶æœ€ä½³æƒé‡
                final_path = Path('/root/autodl-tmp') / f"{config['name']}_best.pt"
                shutil.copy2(best_weight, final_path)
                print(f"âœ… æœ€ä½³æƒé‡å·²ä¿å­˜åˆ°: {final_path}")
                
                # åˆ›å»ºæƒé‡ä¿¡æ¯æ–‡ä»¶
                info_file = final_path.with_suffix('.txt')
                with open(info_file, 'w') as f:
                    f.write(f"æ¨¡å‹: {config['model'].split('/')[-1]}\n")
                    f.write(f"æ•°æ®é›†: HomeObjectsæ‰©å±•ç‰ˆ (æ™ºèƒ½ç­›é€‰)\n")
                    f.write(f"è®­ç»ƒæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write(f"æ‰¹æ¬¡å¤§å°: {config['batch']}\n")
                    f.write(f"å­¦ä¹ ç‡: {config['lr0']}\n")
                    f.write(f"è®­ç»ƒè½®æ•°: {config['epochs']}\n")
                
                print(f"ğŸ“ æƒé‡ä¿¡æ¯å·²ä¿å­˜åˆ°: {info_file}")
            else:
                print("âŒ æœªæ‰¾åˆ°æœ€ä½³æƒé‡æ–‡ä»¶")
                
        except Exception as e:
            print(f"âŒ æƒé‡å¤åˆ¶å¤±è´¥: {e}")
        
        # ä½¿ç”¨æ­£ç¡®çš„å±æ€§è·å–è®­ç»ƒç»“æœ
        if hasattr(results, 'fitness'):
            fitness_score = results.fitness()
            print(f"ğŸ“Š æœ€ç»ˆfitnessè¯„åˆ†: {fitness_score}")
        elif hasattr(results, 'mean_results'):
            mean_results = results.mean_results()
            print(f"ğŸ“Š å¹³å‡ç»“æœ: P={mean_results[0]:.3f}, R={mean_results[1]:.3f}, mAP50={mean_results[2]:.3f}, mAP50-95={mean_results[3]:.3f}")
        
        # æœ€ç»ˆæ¸…ç†
        del model
        torch.cuda.empty_cache()
        gc.collect()
        
        return results
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå‡ºé”™: {e}")
        torch.cuda.empty_cache()
        gc.collect()
        raise

def quick_speed_test():
    """å¿«é€Ÿé€Ÿåº¦æµ‹è¯•"""
    print("âš¡ RTX 4090é€Ÿåº¦æµ‹è¯•")
    print("=" * 30)
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨")
        return
    
    # æµ‹è¯•ä¸åŒbatch sizeçš„å®é™…é€Ÿåº¦
    batch_sizes = [4, 6, 8, 12, 16]
    img_size = 640
    
    for batch_size in batch_sizes:
        try:
            print(f"\nğŸ“Š æµ‹è¯• batch_size={batch_size}")
            
            # æ¨¡æ‹ŸRT-DETRè¾“å…¥
            data = torch.randn(batch_size, 3, img_size, img_size).cuda()
            
            # æ¨¡æ‹Ÿä¸€ä¸ªç®€å•çš„å‰å‘ä¼ æ’­
            conv1 = torch.nn.Conv2d(3, 64, 3, padding=1).cuda()
            conv2 = torch.nn.Conv2d(64, 128, 3, padding=1).cuda()
            
            # é¢„çƒ­
            with torch.no_grad():
                for _ in range(10):
                    x = conv1(data)
                    x = torch.relu(x)
                    x = conv2(x)
            
            torch.cuda.synchronize()
            
            # è®¡æ—¶
            start_time = time.time()
            iterations = 50
            
            with torch.no_grad():
                for _ in range(iterations):
                    x = conv1(data)
                    x = torch.relu(x)
                    x = conv2(x)
            
            torch.cuda.synchronize()
            end_time = time.time()
            
            total_time = end_time - start_time
            fps = iterations * batch_size / total_time
            memory_used = torch.cuda.memory_allocated() / 1e9
            
            print(f"   å¤„ç†é€Ÿåº¦: {fps:.2f} imgs/sec")
            print(f"   GPUå†…å­˜: {memory_used:.2f}GB")
            print(f"   æ¯æ‰¹æ¬¡æ—¶é—´: {total_time/iterations*1000:.2f}ms")
            
            # æ¸…ç†
            del data, conv1, conv2, x
            torch.cuda.empty_cache()
            
        except torch.cuda.OutOfMemoryError:
            print(f"   âŒ OOM - batch_size={batch_size}")
            torch.cuda.empty_cache()
        except Exception as e:
            print(f"   âŒ é”™è¯¯: {e}")
            torch.cuda.empty_cache()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸï¸  RTX 4090ä¸“ç”¨RT-DETRè®­ç»ƒä¼˜åŒ–å™¨")
    print("=" * 50)

    while True:
        print("\nğŸ“‹ é€‰é¡¹:")
        print("1. RTX 4090é€Ÿåº¦æµ‹è¯•")
        print("2. å¼€å§‹ä¼˜åŒ–è®­ç»ƒ - RT-DETR-L")
        print("3. å¼€å§‹ä¼˜åŒ–è®­ç»ƒ - RT-DETR+MNV4")
        print("4. å¼€å§‹ä¼˜åŒ–è®­ç»ƒ - RT-DETR+MNV4+SEA")
        print("5. å†…å­˜çŠ¶æ€æ£€æŸ¥")
        print("6. é€€å‡º")

        try:
            choice = input("\nè¯·é€‰æ‹© (1-6): ").strip()

            if choice == '1':
                quick_speed_test()

            elif choice in ['2', '3', '4']:
                model_map = {'2': '1', '3': '2', '4': '3'}
                model_choice = model_map[choice]

                confirm = input(f"ç¡®è®¤å¼€å§‹è®­ç»ƒ? (y/n): ").strip().lower()
                if confirm == 'y':
                    train_with_rtx4090_optimization(model_choice)
                else:
                    print("âŒ å–æ¶ˆè®­ç»ƒ")

            elif choice == '5':
                if torch.cuda.is_available():
                    allocated = torch.cuda.memory_allocated() / 1e9
                    cached = torch.cuda.memory_reserved() / 1e9
                    total = torch.cuda.get_device_properties(0).total_memory / 1e9

                    print(f"ğŸ”¥ GPU: {torch.cuda.get_device_name(0)}")
                    print(f"   æ€»æ˜¾å­˜: {total:.1f}GB")
                    print(f"   å·²ä½¿ç”¨: {allocated:.1f}GB ({allocated/total*100:.1f}%)")
                    print(f"   å·²ç¼“å­˜: {cached:.1f}GB ({cached/total*100:.1f}%)")
                    print(f"   å¯ç”¨: {total-allocated:.1f}GB")
                else:
                    print("âŒ CUDAä¸å¯ç”¨")

            elif choice == '6':
                print("ğŸ‘‹ é€€å‡º")
                break

            else:
                print("âŒ è¯·è¾“å…¥ 1-6")

        except KeyboardInterrupt:
            print("\nğŸ‘‹ é€€å‡º")
            break
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")

if __name__ == "__main__":
    main()
