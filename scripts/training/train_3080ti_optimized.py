#!/usr/bin/env python3
"""
ä¸ºRTX 3080Tiä¼˜åŒ–çš„RT-DETRè®­ç»ƒè„šæœ¬
è§£å†³è®­ç»ƒæ…¢å’Œå†…å­˜æ³„æ¼é—®é¢˜ + æ–‡ä»¶æè¿°ç¬¦é—®é¢˜
"""

import os
import sys
import yaml
import torch
import gc
from pathlib import Path
import threading
import time
import resource
import argparse

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "ultralytics"))

# å…¨å±€è®¾ç½®è®­ç»ƒæ¨¡å¼ç‰ˆæœ¬å’Œå…³æœºé€‰é¡¹
DEFAULT_TRAIN_MODE =1  # 1: RT-DETR-L, 2: RT-DETR+MNV4, 3: RT-DETR+MNV4+SEA
SHUTDOWN_AFTER_TRAIN = True  # è®¾ç½®ä¸º True è¡¨ç¤ºè®­ç»ƒå®Œæˆåè‡ªåŠ¨å…³æœº

# ä¿®å¤æ–‡ä»¶æè¿°ç¬¦é™åˆ¶é—®é¢˜
def fix_file_descriptor_limit():
    print("ğŸ”§ ä¿®å¤æ–‡ä»¶æè¿°ç¬¦é™åˆ¶...")

    try:
        # è·å–å½“å‰é™åˆ¶
        soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)
        print(f"   å½“å‰æ–‡ä»¶æè¿°ç¬¦é™åˆ¶: {soft} (è½¯é™åˆ¶) / {hard} (ç¡¬é™åˆ¶)")

        # è®¾ç½®è½¯é™åˆ¶ä¸º 65536
        new_soft = min(65536, hard)  # è®¾ç½®ä¸º65536æˆ–ç¡¬é™åˆ¶çš„è¾ƒå°å€¼
        resource.setrlimit(resource.RLIMIT_NOFILE, (new_soft, hard))

        print(f"   âœ… æ–°çš„æ–‡ä»¶æè¿°ç¬¦é™åˆ¶: {new_soft}")

        # è®¾ç½®ç¯å¢ƒå˜é‡é™åˆ¶ workers
        os.environ['TORCH_NUM_WORKERS'] = '2'  # å¼ºåˆ¶é™åˆ¶ workers æ•°é‡

    except Exception as e:
        print(f"   âš ï¸ æ— æ³•ä¿®æ”¹æ–‡ä»¶æè¿°ç¬¦é™åˆ¶: {e}")
        print("   ğŸ’¡ å»ºè®®åœ¨ç³»ç»Ÿçº§åˆ«å¢åŠ æ–‡ä»¶æè¿°ç¬¦é™åˆ¶")

# ä¸ºRTX 3080Tiè®¾ç½®ä¸“é—¨çš„ä¼˜åŒ–
def setup_rtx3080ti_optimization():
    print("ğŸš€ ä¸ºRTX 3080Tiè®¾ç½®ä¸“é—¨ä¼˜åŒ–...")

    # é¦–å…ˆä¿®å¤æ–‡ä»¶æè¿°ç¬¦é—®é¢˜
    fix_file_descriptor_limit()

    # RTX 3080Tiä¸“ç”¨CUDAè®¾ç½® - ä¿å®ˆé…ç½®é¿å…é©±åŠ¨é”™è¯¯
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'  # ç§»é™¤ expandable_segments
    os.environ['CUDA_LAUNCH_BLOCKING'] = '0'
    os.environ['TORCH_CUDNN_V8_API_ENABLED'] = '1'

    # å¯ç”¨RTX 3080Tiçš„ä¼˜åŒ–ç‰¹æ€§
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True

    # è®¾ç½®åˆç†çš„çº¿ç¨‹æ•°
    torch.set_num_threads(4)  # å‡å°‘çº¿ç¨‹æ•°é¿å…èµ„æºç«äº‰
    os.environ['OMP_NUM_THREADS'] = '4'
    os.environ['MKL_NUM_THREADS'] = '4'

    if torch.cuda.is_available():
        # RTX 3080Tiæ˜¾å­˜è¾ƒå°‘ï¼Œä½¿ç”¨80%é¿å…OOM
        torch.cuda.set_per_process_memory_fraction(0.8)

        # æ¸…ç†åˆå§‹ç¼“å­˜
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

        print(f"   âœ… GPU: {torch.cuda.get_device_name(0)}")
        print(f"   âœ… æ˜¾å­˜é™åˆ¶: 80% (~10GB)")

        # æ£€æŸ¥CUDAçŠ¶æ€
        try:
            test_tensor = torch.randn(100, 100).cuda()
            del test_tensor
            torch.cuda.empty_cache()
            print("   âœ… CUDAçŠ¶æ€æ­£å¸¸")
        except Exception as e:
            print(f"   âŒ CUDAæµ‹è¯•å¤±è´¥: {e}")
            raise
        print(f"   âœ… TF32åŠ é€Ÿ: å¯ç”¨")

# RTX 3080Tiä¸“ç”¨å†…å­˜ç›‘æ§
def memory_monitor_rtx3080ti():
    def monitor():
        while True:
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated(0) / 1e9
                cached = torch.cuda.memory_reserved(0) / 1e9
                
                # RTX 3080Tiæ˜¾å­˜é˜ˆå€¼è¾ƒä½
                if allocated > 9.0:  # 9GBä»¥ä¸Šæ—¶æ¸…ç†
                    torch.cuda.empty_cache()
                    gc.collect()
                    print(f"ğŸ§¹ è‡ªåŠ¨æ¸…ç†GPUå†…å­˜: {allocated:.1f}GB -> {torch.cuda.memory_allocated(0)/1e9:.1f}GB")
            
            time.sleep(15)  # æ¯15ç§’æ£€æŸ¥ä¸€æ¬¡
    
    monitor_thread = threading.Thread(target=monitor, daemon=True)
    monitor_thread.start()

# RTX 3080Tiä¼˜åŒ–çš„è®­ç»ƒé…ç½®
def get_rtx3080ti_config(model_choice):
    model_configs = {
        '1': {
            'file': 'rtdetr-l.yaml',
            'name': 'rtdetr_l_rtx3080ti',
            'batch': 8,       # é™ä½batché¿å…å†…å­˜æ³„æ¼
            'lr0': 0.001,     # ç›¸åº”è°ƒæ•´å­¦ä¹ ç‡
            'workers': 8,     # å¢åŠ æ•°æ®åŠ è½½å™¨çº¿ç¨‹æ•°
        },
        '2': {
            'file': 'rtdetr-mnv4-hybrid-m.yaml', 
            'name': 'rtdetr_mnv4_hybrid_rtx3080ti',
            'batch': 8,       # å¢åŠ æ‰¹æ¬¡å¤§å°
            'lr0': 0.0008,
            'workers': 8,     # å¢åŠ æ•°æ®åŠ è½½å™¨çº¿ç¨‹æ•°
        },
        '3': {
            'file': 'rtdetr-mnv4-hybrid-m-sea.yaml',
            'name': 'rtdetr_mnv4_sea_rtx3080ti',
            'batch': 8,       # å¢åŠ æ‰¹æ¬¡å¤§å°
            'lr0': 0.0006,
            'workers': 8,     # å¢åŠ æ•°æ®åŠ è½½å™¨çº¿ç¨‹æ•°
        }   
    }

    if model_choice not in model_configs:
        raise ValueError(f"æ— æ•ˆçš„æ¨¡å‹é€‰æ‹©: {model_choice}")

    model_config = model_configs[model_choice]

    # RTX 3080Tiä¸“ç”¨é…ç½®
    config = {
        'task': 'detect',
        'mode': 'train',
        'model': str(project_root / f"ultralytics/ultralytics/cfg/models/rt-detr/{model_config['file']}"),
        'data': str(project_root / "datasets/homeobjects-3K/HomeObjects-3K.yaml"),

        # RTX 3080Tiä¼˜åŒ–çš„æ ¸å¿ƒå‚æ•°
        'epochs': 100,
        'batch': model_config['batch'],
        'imgsz': 640,
        'patience': 20,

        # ç¨³å®šæ€§ä¼˜åŒ–è®¾ç½® - é˜²æ­¢å†…å­˜æ³„æ¼
        'device': '0',
        'workers': model_config['workers'],
        'amp': True,            # æ··åˆç²¾åº¦è®­ç»ƒ
        'cache': False,         # å…³é—­ç¼“å­˜é¿å…æ–‡ä»¶æè¿°ç¬¦é—®é¢˜
        'rect': True,           # çŸ©å½¢è®­ç»ƒ
        'single_cls': False,

        # RTX 3080Tiä¼˜åŒ–çš„å­¦ä¹ ç‡è®¾ç½®
        'optimizer': 'AdamW',
        'lr0': model_config['lr0'],
        'lrf': 0.001,           # æ›´æ¿€è¿›çš„å­¦ä¹ ç‡è¡°å‡
        'momentum': 0.937,
        'weight_decay': 0.0005,
        'warmup_epochs': 3.0,
        'warmup_momentum': 0.8,
        'warmup_bias_lr': 0.1,
        'cos_lr': True,

        # å†…å­˜å®‰å…¨çš„æ•°æ®å¢å¼ºè®¾ç½®
        'hsv_h': 0.015,
        'hsv_s': 0.7,
        'hsv_v': 0.4,
        'degrees': 0.0,         # å…³é—­æ—‹è½¬å‡å°‘è®¡ç®—
        'translate': 0.1,
        'scale': 0.5,
        'shear': 0.0,           # å…³é—­å‰ªåˆ‡å‡å°‘è®¡ç®—
        'perspective': 0.0,     # å…³é—­é€è§†å˜æ¢é˜²æ­¢å†…å­˜æ³„æ¼
        'flipud': 0.0,
        'fliplr': 0.5,
        'mosaic': 0.0,          # å…³é—­mosaicé˜²æ­¢å†…å­˜æ³„æ¼
        'mixup': 0.0,           # å…³é—­mixupé˜²æ­¢å†…å­˜æ³„æ¼
        'copy_paste': 0.0,      # å…³é—­copy_pasteé˜²æ­¢å†…å­˜æ³„æ¼

        # æŸå¤±æƒé‡
        'box': 7.5,
        'cls': 0.5,
        'dfl': 1.5,

        # éªŒè¯è®¾ç½®
        'val': True,
        'conf': 0.25,
        'iou': 0.7,
        'max_det': 300,

        # ä¿å­˜è®¾ç½®
        'save': True,
        'save_period': 20,  # æ¯ 20 ä¸ª epoch ä¿å­˜ä¸€æ¬¡
        'project': '/root/autodl-tmp/runs/detect',
        'name': model_config['name'],
        'exist_ok': True,

        # RTX 3080Tiä¸“ç”¨è®¾ç½®
        'verbose': True,
        'seed': 42,
        'deterministic': False,
        'plots': True,
        'close_mosaic': 10,
        'overlap_mask': True,   # RTX 3080Tiå¯ä»¥å¤„ç†é‡å mask
        'mask_ratio': 4,

        # é«˜çº§ä¼˜åŒ–è®¾ç½®
        'profile': False,       # å…³é—­æ€§èƒ½åˆ†æä»¥æé«˜é€Ÿåº¦
        'half': False,          # RTX 3080Tiç”¨FP16å¯èƒ½ä¸ç¨³å®šï¼Œç”¨AMPå°±å¤Ÿäº†
        'dnn': False,           # ä¸ä½¿ç”¨OpenCV DNN
    }

    return config

# RTX 3080Tiä¼˜åŒ–è®­ç»ƒ
def train_with_rtx3080ti_optimization(model_choice):
    try:
        # è®¾ç½®ç¯å¢ƒ
        setup_rtx3080ti_optimization()
        memory_monitor_rtx3080ti()

        # å¯¼å…¥ultralytics
        print("ğŸ“¦ å¯¼å…¥Ultralytics...")
        from ultralytics import RTDETR

        # è·å–é…ç½®
        config = get_rtx3080ti_config(model_choice)

        print(f"\nğŸš€ RTX 3080Tiä¼˜åŒ–è®­ç»ƒå¼€å§‹")
        print(f"ğŸ“„ æ¨¡å‹: {config['model'].split('/')[-1]}")
        print(f"ğŸ“Š æ‰¹æ¬¡å¤§å°: {config['batch']}")
        print(f"ğŸ¯ å­¦ä¹ ç‡: {config['lr0']}")
        print(f"ğŸ‘¥ Workers: {config['workers']}")
        print(f"ğŸ§  å†…å­˜ç¼“å­˜: {config['cache']}")
        print("=" * 60)

        # æ˜¾å­˜é¢„çƒ­
        print("ğŸ”¥ GPUé¢„çƒ­ä¸­...")
        dummy_data = torch.randn(1, 3, 640, 640).cuda()
        for _ in range(10):
            _ = torch.nn.functional.conv2d(dummy_data, torch.randn(64, 3, 3, 3).cuda())
        torch.cuda.synchronize()
        del dummy_data
        torch.cuda.empty_cache()
        print("âœ… GPUé¢„çƒ­å®Œæˆ")

        # åˆ›å»ºæ¨¡å‹
        model = RTDETR(config['model'])

        # å¼€å§‹è®­ç»ƒ
        results = model.train(**{k: v for k, v in config.items() if k not in ['model']})

        print("\nğŸ‰ è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“Š æœ€ä½³mAP50: {results.mean_results()[2]}")  # ä½¿ç”¨ mean_results æ–¹æ³•è·å– mAP50

        # æœ€ç»ˆæ¸…ç†
        del model
        torch.cuda.empty_cache()
        gc.collect()

        # è‡ªåŠ¨å…³æœº
        if SHUTDOWN_AFTER_TRAIN:
            print("ğŸ‘‹ è®­ç»ƒå®Œæˆï¼Œç³»ç»Ÿå°†åœ¨ 1 åˆ†é’Ÿåå…³æœº...")
            os.system("shutdown -h +1")
        else:
            print("ğŸ‘‹ è®­ç»ƒå®Œæˆï¼Œè‡ªåŠ¨é€€å‡ºç¨‹åº...")
            sys.exit(0)

    except Exception as e:
        print(f"âŒ è®­ç»ƒå‡ºé”™: {e}")
        torch.cuda.empty_cache()
        gc.collect()
        raise

# è§£æå‘½ä»¤è¡Œå‚æ•°
def parse_arguments():
    parser = argparse.ArgumentParser(description="RTX 3080Ti ä¸“ç”¨ RT-DETR è®­ç»ƒè„šæœ¬")
    parser.add_argument(
        "--mode",
        type=int,
        choices=[1, 2, 3],
        default=DEFAULT_TRAIN_MODE,  # ä½¿ç”¨å…¨å±€é»˜è®¤æ¨¡å¼
        help="é€‰æ‹©è®­ç»ƒæ¨¡å¼: 1 (RT-DETR-L), 2 (RT-DETR+MNV4), 3 (RT-DETR+MNV4+SEA)"
    )
    parser.add_argument(
        "--shutdown",
        action="store_true",
        help="è®­ç»ƒå®Œæˆåè‡ªåŠ¨å…³æœº"
    )
    return parser.parse_args()

# ä¸»å‡½æ•°
def main():
    print("ğŸï¸  RTX 3080Tiä¸“ç”¨RT-DETRè®­ç»ƒä¼˜åŒ–å™¨")
    print("=" * 50)

    args = parse_arguments()
    model_choice = str(args.mode)

    try:
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å¼ {model_choice}...")
        train_with_rtx3080ti_optimization(model_choice)
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")

if __name__ == "__main__":
    main()