#!/usr/bin/env python3
"""
CUDAé”™è¯¯ç´§æ€¥ä¿®å¤ - ä¸“é—¨å¤„ç†RTX 4090çš„CUDAå†…å­˜è®¿é—®é”™è¯¯
"""

import os
import sys
import torch
import gc
import warnings

def setup_cuda_debug_mode():
    """è®¾ç½®CUDAè°ƒè¯•æ¨¡å¼"""
    print("ğŸ”§ è®¾ç½®CUDAè°ƒè¯•æ¨¡å¼...")
    
    # å¯ç”¨åŒæ­¥CUDAè°ƒç”¨ä»¥å‡†ç¡®å®šä½é”™è¯¯
    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
    
    # å¯ç”¨è®¾å¤‡ç«¯æ–­è¨€
    os.environ['TORCH_USE_CUDA_DSA'] = '1'
    
    # ä¸¥æ ¼çš„å†…å­˜æ£€æŸ¥
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128,expandable_segments:False'
    
    # ç¦ç”¨å¯èƒ½å¯¼è‡´é—®é¢˜çš„ä¼˜åŒ–
    os.environ['TORCH_CUDNN_V8_API_ENABLED'] = '0'
    
    print("âœ… CUDAè°ƒè¯•æ¨¡å¼å·²å¯ç”¨")

def emergency_gpu_cleanup():
    """ç´§æ€¥GPUæ¸…ç†"""
    print("ğŸ§¹ æ‰§è¡Œç´§æ€¥GPUæ¸…ç†...")
    
    if torch.cuda.is_available():
        # æ¸…ç©ºæ‰€æœ‰CUDAç¼“å­˜
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        
        # é‡ç½®CUDAä¸Šä¸‹æ–‡ï¼ˆå¦‚æœå¯èƒ½ï¼‰
        try:
            torch.cuda.reset_peak_memory_stats()
            print("âœ… CUDAç»Ÿè®¡å·²é‡ç½®")
        except:
            print("âš ï¸  æ— æ³•é‡ç½®CUDAç»Ÿè®¡")
        
        # æ˜¾ç¤ºå½“å‰æ˜¾å­˜çŠ¶æ€
        allocated = torch.cuda.memory_allocated(0) / 1e9
        cached = torch.cuda.memory_reserved(0) / 1e9
        total = torch.cuda.get_device_properties(0).total_memory / 1e9
        
        print(f"ğŸ“Š æ˜¾å­˜çŠ¶æ€: {allocated:.2f}GB/{total:.1f}GB (ç¼“å­˜:{cached:.2f}GB)")
        
        if allocated > 0.1:
            print("âš ï¸  æ£€æµ‹åˆ°æœªé‡Šæ”¾çš„æ˜¾å­˜")
    
    # Pythonåƒåœ¾å›æ”¶
    gc.collect()
    print("âœ… ç´§æ€¥æ¸…ç†å®Œæˆ")

def create_safe_training_config():
    """åˆ›å»ºè¶…å®‰å…¨çš„è®­ç»ƒé…ç½®"""
    config = {
        # æ¨¡å‹é…ç½®
        'model': '/home/cui/rtdetr_indoor/RT-DETR/rtdetr_pytorch/configs/rtdetr/rtdetr_r18vd_6x_coco.yml',
        'data': '/home/cui/rtdetr_indoor/datasets/indoor_training/data.yaml',
        'project': '/home/cui/rtdetr_indoor',
        'name': 'rtdetr_safe_training',
        
        # è¶…ä¿å®ˆè®¾ç½®
        'epochs': 100,
        'batch': 1,          # æœ€å°æ‰¹æ¬¡
        'imgsz': 640,
        'patience': 30,
        
        # æœ€å®‰å…¨çš„è®¾å¤‡è®¾ç½®
        'device': '0',
        'workers': 0,        # ç¦ç”¨å¤šè¿›ç¨‹
        'amp': False,        # ç¦ç”¨æ··åˆç²¾åº¦
        'cache': False,      # ç¦ç”¨ç¼“å­˜
        'rect': False,       # ç¦ç”¨çŸ©å½¢è®­ç»ƒ
        'single_cls': False,
        'save_period': 10,   # é¢‘ç¹ä¿å­˜
        
        # ä¿å®ˆçš„ä¼˜åŒ–å™¨è®¾ç½®
        'optimizer': 'SGD',  # ä½¿ç”¨æ›´ç¨³å®šçš„SGD
        'lr0': 0.001,        # æ›´å°çš„å­¦ä¹ ç‡
        'lrf': 0.01,
        'momentum': 0.9,
        'weight_decay': 0.0001,
        'warmup_epochs': 1.0,
        'warmup_momentum': 0.8,
        'warmup_bias_lr': 0.01,
        'cos_lr': False,     # ç¦ç”¨ä½™å¼¦å­¦ä¹ ç‡
        
        # ç¦ç”¨æ‰€æœ‰æ•°æ®å¢å¼º
        'hsv_h': 0.0,
        'hsv_s': 0.0,
        'hsv_v': 0.0,
        'degrees': 0.0,
        'translate': 0.0,
        'scale': 0.0,
        'shear': 0.0,
        'perspective': 0.0,
        'flipud': 0.0,
        'fliplr': 0.0,
        'mosaic': 0.0,
        'mixup': 0.0,
        'copy_paste': 0.0,
        
        # éªŒè¯è®¾ç½®
        'val': True,
        'plots': False,      # ç¦ç”¨ç»˜å›¾
        'save': True,
        'save_txt': False,
        'save_conf': False,
        'save_json': False,
        'half': False,       # ç¦ç”¨FP16
        'dnn': False,
        'verbose': True,
    }
    
    return config

def safe_train():
    """å®‰å…¨è®­ç»ƒæ¨¡å¼"""
    try:
        # 1. è®¾ç½®è°ƒè¯•æ¨¡å¼
        setup_cuda_debug_mode()
        
        # 2. ç´§æ€¥æ¸…ç†
        emergency_gpu_cleanup()
        
        # 3. å¯¼å…¥å¿…è¦çš„åº“
        print("ğŸ“š å¯¼å…¥è®­ç»ƒåº“...")
        from ultralytics import RTDETR
        
        # 4. è®¾ç½®æœ€ä¿å®ˆçš„PyTorchè®¾ç½®
        print("âš™ï¸  è®¾ç½®PyTorch...")
        torch.backends.cudnn.benchmark = False  # ç¦ç”¨benchmark
        torch.backends.cudnn.deterministic = True
        torch.backends.cuda.matmul.allow_tf32 = False  # ç¦ç”¨TF32
        torch.backends.cudnn.allow_tf32 = False
        
        # è®¾ç½®æ˜¾å­˜é™åˆ¶ä¸º50%
        if torch.cuda.is_available():
            torch.cuda.set_per_process_memory_fraction(0.5)
            print("ğŸ“Š æ˜¾å­˜é™åˆ¶è®¾ç½®ä¸º50%")
        
        # 5. åˆ›å»ºé…ç½®
        config = create_safe_training_config()
        
        print("ğŸš€ å¼€å§‹è¶…å®‰å…¨æ¨¡å¼è®­ç»ƒ...")
        print("âš ï¸  æ³¨æ„ï¼šæ­¤æ¨¡å¼è®­ç»ƒé€Ÿåº¦è¾ƒæ…¢ï¼Œä½†ç¨³å®šæ€§æœ€é«˜")
        
        # 6. åˆå§‹åŒ–æ¨¡å‹
        model = RTDETR('rtdetr-l.pt')
        
        # 7. å¼€å§‹è®­ç»ƒ
        results = model.train(**config)
        
        print("ğŸ‰ å®‰å…¨è®­ç»ƒå®Œæˆ!")
        if hasattr(results, 'fitness'):
            fitness_score = results.fitness()
            print(f"ğŸ“Š æœ€ç»ˆfitnessè¯„åˆ†: {fitness_score}")
        
        return results
        
    except Exception as e:
        print(f"âŒ å®‰å…¨è®­ç»ƒä¹Ÿå¤±è´¥äº†: {e}")
        print("ğŸ” å»ºè®®æ£€æŸ¥:")
        print("1. GPUç¡¬ä»¶æ˜¯å¦æ­£å¸¸")
        print("2. CUDAé©±åŠ¨æ˜¯å¦éœ€è¦æ›´æ–°")
        print("3. æ˜¯å¦å­˜åœ¨ç¡¬ä»¶è¿‡çƒ­é—®é¢˜")
        raise

def cuda_memory_test():
    """CUDAå†…å­˜æµ‹è¯•"""
    print("ğŸ§ª æ‰§è¡ŒCUDAå†…å­˜æµ‹è¯•...")
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨")
        return False
    
    try:
        # æµ‹è¯•å°å¼ é‡
        print("ğŸ“ æµ‹è¯•å°å¼ é‡...")
        x = torch.randn(100, 100).cuda()
        y = torch.randn(100, 100).cuda()
        z = x + y
        del x, y, z
        torch.cuda.empty_cache()
        print("âœ… å°å¼ é‡æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•ä¸­ç­‰å¼ é‡
        print("ğŸ“ æµ‹è¯•ä¸­ç­‰å¼ é‡...")
        x = torch.randn(1000, 1000).cuda()
        y = torch.randn(1000, 1000).cuda()
        z = torch.matmul(x, y)
        del x, y, z
        torch.cuda.empty_cache()
        print("âœ… ä¸­ç­‰å¼ é‡æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•å¤§å¼ é‡ï¼ˆæ¨¡æ‹Ÿè®­ç»ƒï¼‰
        print("ğŸ“ æµ‹è¯•å¤§å¼ é‡...")
        x = torch.randn(4, 3, 640, 640).cuda()  # æ¨¡æ‹Ÿbatch
        y = torch.randn(4, 256, 20, 20).cuda()  # æ¨¡æ‹Ÿç‰¹å¾å›¾
        del x, y
        torch.cuda.empty_cache()
        print("âœ… å¤§å¼ é‡æµ‹è¯•é€šè¿‡")
        
        print("ğŸ‰ CUDAå†…å­˜æµ‹è¯•å…¨éƒ¨é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ CUDAå†…å­˜æµ‹è¯•å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    print("ğŸš¨ CUDAé”™è¯¯ç´§æ€¥ä¿®å¤å·¥å…·")
    print("=" * 50)
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "test":
            cuda_memory_test()
        elif sys.argv[1] == "clean":
            emergency_gpu_cleanup()
        elif sys.argv[1] == "train":
            safe_train()
    else:
        print("ä½¿ç”¨æ–¹æ³•:")
        print("python cuda_emergency_fix.py test   # æµ‹è¯•CUDAå†…å­˜")
        print("python cuda_emergency_fix.py clean  # ç´§æ€¥æ¸…ç†GPU")
        print("python cuda_emergency_fix.py train  # è¶…å®‰å…¨è®­ç»ƒ")
        
        choice = input("é€‰æ‹©æ“ä½œ (test/clean/train): ").strip().lower()
        
        if choice == "test":
            cuda_memory_test()
        elif choice == "clean":
            emergency_gpu_cleanup()
        elif choice == "train":
            safe_train()
        else:
            print("æ— æ•ˆé€‰æ‹©")
