#!/usr/bin/env python3
"""
è®­ç»ƒæ€§èƒ½è¯Šæ–­å·¥å…·
åˆ†æè®­ç»ƒé€Ÿåº¦ç“¶é¢ˆå’Œå†…å­˜é—®é¢˜
"""

import torch
import time
import os
import psutil
import subprocess
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "ultralytics"))

def diagnose_system():
    """è¯Šæ–­ç³»ç»Ÿæ€§èƒ½"""
    print("ğŸ” ç³»ç»Ÿæ€§èƒ½è¯Šæ–­")
    print("=" * 50)
    
    # CPUä¿¡æ¯
    print(f"ğŸ’» CPU: {psutil.cpu_count()} cores @ {psutil.cpu_freq().max:.0f}MHz")
    print(f"   ä½¿ç”¨ç‡: {psutil.cpu_percent(interval=1):.1f}%")
    
    # å†…å­˜ä¿¡æ¯
    memory = psutil.virtual_memory()
    print(f"ğŸ§  å†…å­˜: {memory.total/1e9:.1f}GB æ€»é‡")
    print(f"   ä½¿ç”¨ç‡: {memory.percent:.1f}%")
    print(f"   å¯ç”¨: {memory.available/1e9:.1f}GB")
    
    # GPUä¿¡æ¯
    if torch.cuda.is_available():
        print(f"ğŸ”¥ GPU: {torch.cuda.get_device_name(0)}")
        print(f"   CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"   PyTorchç‰ˆæœ¬: {torch.__version__}")
        
        props = torch.cuda.get_device_properties(0)
        print(f"   GPUå†…å­˜: {props.total_memory/1e9:.1f}GB")
        print(f"   è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
        try:
            print(f"   å¤šå¤„ç†å™¨: {props.multi_processor_count}")
        except AttributeError:
            print(f"   å¤šå¤„ç†å™¨: æ— æ³•è·å–")
    else:
        print("âŒ GPUä¸å¯ç”¨")
    
    # ç£ç›˜I/O
    disk = psutil.disk_usage('/')
    print(f"ğŸ’¾ ç£ç›˜: {disk.free/1e9:.1f}GB å¯ç”¨ / {disk.total/1e9:.1f}GB æ€»é‡")

def test_gpu_performance():
    """æµ‹è¯•GPUæ€§èƒ½"""
    print("\nğŸš€ GPUæ€§èƒ½æµ‹è¯•")
    print("=" * 30)
    
    if not torch.cuda.is_available():
        print("âŒ GPUä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
        return
    
    device = torch.device('cuda:0')
    
    # æµ‹è¯•ä¸åŒbatch sizeçš„æ€§èƒ½
    batch_sizes = [1, 2, 4, 8]
    img_size = 640
    
    for batch_size in batch_sizes:
        try:
            print(f"\nğŸ“Š æµ‹è¯• batch_size={batch_size}")
            
            # åˆ›å»ºéšæœºæ•°æ®
            data = torch.randn(batch_size, 3, img_size, img_size).to(device)
            
            # é¢„çƒ­
            for _ in range(5):
                _ = torch.nn.functional.conv2d(data, torch.randn(64, 3, 3, 3).to(device))
            
            torch.cuda.synchronize()
            
            # è®¡æ—¶æµ‹è¯•
            start_time = time.time()
            iterations = 20
            
            for _ in range(iterations):
                _ = torch.nn.functional.conv2d(data, torch.randn(64, 3, 3, 3).to(device))
            
            torch.cuda.synchronize()
            end_time = time.time()
            
            fps = iterations * batch_size / (end_time - start_time)
            memory_used = torch.cuda.memory_allocated() / 1e9
            
            print(f"   FPS: {fps:.2f}")
            print(f"   GPUå†…å­˜: {memory_used:.2f}GB")
            
            # æ¸…ç†å†…å­˜
            del data
            torch.cuda.empty_cache()
            
        except torch.cuda.OutOfMemoryError:
            print(f"   âŒ OOM - batch_size={batch_size} è¶…å‡ºå†…å­˜é™åˆ¶")
            torch.cuda.empty_cache()

def test_dataloader_performance():
    """æµ‹è¯•æ•°æ®åŠ è½½æ€§èƒ½"""
    print("\nğŸ“¦ æ•°æ®åŠ è½½æ€§èƒ½æµ‹è¯•") 
    print("=" * 30)
    
    dataset_path = "/home/cui/rtdetr_indoor/datasets/homeobjects-3K"
    
    if not Path(dataset_path).exists():
        print(f"âŒ æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {dataset_path}")
        return
    
    try:
        from ultralytics.data import YOLODataset
        from torch.utils.data import DataLoader
        
        # æµ‹è¯•ä¸åŒworkeræ•°é‡
        worker_counts = [0, 2, 4, 8]
        batch_size = 4
        
        for num_workers in worker_counts:
            try:
                print(f"\nğŸ”„ æµ‹è¯• workers={num_workers}")
                
                # åˆ›å»ºæ•°æ®é›†
                dataset = YOLODataset(
                    img_path=f"{dataset_path}/images/train",
                    imgsz=640,
                    augment=False,
                    cache=False
                )
                
                dataloader = DataLoader(
                    dataset,
                    batch_size=batch_size,
                    num_workers=num_workers,
                    shuffle=False,
                    pin_memory=True
                )
                
                # è®¡æ—¶
                start_time = time.time()
                batch_count = 0
                
                for batch in dataloader:
                    batch_count += 1
                    if batch_count >= 20:  # åªæµ‹è¯•å‰20ä¸ªbatch
                        break
                
                end_time = time.time()
                
                if batch_count > 0:
                    speed = batch_count / (end_time - start_time)
                    print(f"   é€Ÿåº¦: {speed:.2f} batches/sec")
                    print(f"   å›¾ç‰‡/ç§’: {speed * batch_size:.2f}")
                
            except Exception as e:
                print(f"   âŒ é”™è¯¯: {e}")
    
    except ImportError as e:
        print(f"âŒ æ— æ³•å¯¼å…¥ultralytics: {e}")

def diagnose_model_complexity():
    """è¯Šæ–­æ¨¡å‹å¤æ‚åº¦"""
    print("\nğŸ§  æ¨¡å‹å¤æ‚åº¦åˆ†æ")
    print("=" * 30)
    
    models = {
        'RT-DETR-L': '/home/cui/rtdetr_indoor/ultralytics/ultralytics/cfg/models/rt-detr/rtdetr-l.yaml',
        'RT-DETR+MNV4': '/home/cui/rtdetr_indoor/ultralytics/ultralytics/cfg/models/rt-detr/rtdetr-mnv4-hybrid-m.yaml',
        'RT-DETR+MNV4+SEA': '/home/cui/rtdetr_indoor/ultralytics/ultralytics/cfg/models/rt-detr/rtdetr-mnv4-hybrid-m-sea.yaml'
    }
    
    try:
        from ultralytics import RTDETR
        
        for name, config_path in models.items():
            if not Path(config_path).exists():
                print(f"âŒ {name}: é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ {config_path}")
                continue
                
            try:
                print(f"\nğŸ“‹ {name}:")
                
                # åˆ›å»ºæ¨¡å‹
                model = RTDETR(config_path)
                
                # è®¡ç®—å‚æ•°é‡
                total_params = sum(p.numel() for p in model.model.parameters())
                trainable_params = sum(p.numel() for p in model.model.parameters() if p.requires_grad)
                
                print(f"   æ€»å‚æ•°: {total_params/1e6:.2f}M")
                print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params/1e6:.2f}M")
                
                # æµ‹è¯•å‰å‘ä¼ æ’­é€Ÿåº¦
                if torch.cuda.is_available():
                    device = torch.device('cuda:0')
                    model.model.to(device)
                    
                    # åˆ›å»ºæµ‹è¯•è¾“å…¥
                    test_input = torch.randn(1, 3, 640, 640).to(device)
                    
                    # é¢„çƒ­
                    with torch.no_grad():
                        for _ in range(5):
                            _ = model.model(test_input)
                    
                    torch.cuda.synchronize()
                    
                    # è®¡æ—¶
                    start_time = time.time()
                    with torch.no_grad():
                        for _ in range(20):
                            _ = model.model(test_input)
                    torch.cuda.synchronize()
                    end_time = time.time()
                    
                    inference_time = (end_time - start_time) / 20 * 1000  # ms
                    fps = 1000 / inference_time
                    
                    print(f"   æ¨ç†æ—¶é—´: {inference_time:.2f}ms")
                    print(f"   FPS: {fps:.2f}")
                    
                    # å†…å­˜ä½¿ç”¨
                    memory_used = torch.cuda.memory_allocated() / 1e9
                    print(f"   GPUå†…å­˜: {memory_used:.2f}GB")
                
                # æ¸…ç†
                del model
                torch.cuda.empty_cache()
                
            except Exception as e:
                print(f"   âŒ é”™è¯¯: {e}")
    
    except ImportError as e:
        print(f"âŒ æ— æ³•å¯¼å…¥RTDETR: {e}")

def get_optimization_suggestions():
    """è·å–ä¼˜åŒ–å»ºè®®"""
    print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®")
    print("=" * 30)
    
    suggestions = []
    
    # æ£€æŸ¥GPUå†…å­˜
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        if gpu_memory < 8:
            suggestions.append("ğŸ”¸ GPUå†…å­˜è¾ƒå°(<8GB), å»ºè®®ä½¿ç”¨batch_size=2-3")
        elif gpu_memory < 12:
            suggestions.append("ğŸ”¸ GPUå†…å­˜ä¸­ç­‰(<12GB), å»ºè®®ä½¿ç”¨batch_size=4-6")
        else:
            suggestions.append("ğŸ”¸ GPUå†…å­˜å……è¶³(â‰¥12GB), å¯ä»¥ä½¿ç”¨batch_size=6-8")
    
    # æ£€æŸ¥CPU
    cpu_count = psutil.cpu_count()
    if cpu_count < 8:
        suggestions.append("ğŸ”¸ CPUæ ¸å¿ƒè¾ƒå°‘, å»ºè®®workers=2-4")
    else:
        suggestions.append("ğŸ”¸ CPUæ ¸å¿ƒå……è¶³, å¯ä»¥ä½¿ç”¨workers=4-8")
    
    # æ£€æŸ¥ç³»ç»Ÿå†…å­˜
    memory = psutil.virtual_memory()
    if memory.total < 16e9:
        suggestions.append("ğŸ”¸ ç³»ç»Ÿå†…å­˜è¾ƒå°(<16GB), å»ºè®®å…³é—­cache, å‡å°‘workers")
    
    # é€šç”¨ä¼˜åŒ–å»ºè®®
    suggestions.extend([
        "ğŸ”¸ ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (amp=True)",
        "ğŸ”¸ å¯ç”¨cuDNN benchmark (torch.backends.cudnn.benchmark=True)",
        "ğŸ”¸ å…³é—­ä¸å¿…è¦çš„æ•°æ®å¢å¼º (mixup=0, copy_paste=0)",
        "ğŸ”¸ ä½¿ç”¨çŸ©å½¢è®­ç»ƒ (rect=True)",
        "ğŸ”¸ å®šæœŸæ¸…ç†GPUç¼“å­˜",
        "ğŸ”¸ å¯¹äºSEAæ¨¡å‹, ä½¿ç”¨æ›´å°çš„batch_sizeå’Œå­¦ä¹ ç‡",
        "ğŸ”¸ ç›‘æ§å†…å­˜ä½¿ç”¨, é¿å…å†…å­˜æ³„æ¼",
    ])
    
    for suggestion in suggestions:
        print(suggestion)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ©º RT-DETRè®­ç»ƒæ€§èƒ½è¯Šæ–­å·¥å…·")
    print("=" * 40)
    
    while True:
        print("\nğŸ“‹ è¯Šæ–­é€‰é¡¹:")
        print("1. ç³»ç»Ÿæ€§èƒ½è¯Šæ–­")
        print("2. GPUæ€§èƒ½æµ‹è¯•")
        print("3. æ•°æ®åŠ è½½æ€§èƒ½æµ‹è¯•")
        print("4. æ¨¡å‹å¤æ‚åº¦åˆ†æ") 
        print("5. è·å–ä¼˜åŒ–å»ºè®®")
        print("6. å®Œæ•´è¯Šæ–­")
        print("7. é€€å‡º")
        
        try:
            choice = input("\nè¯·é€‰æ‹© (1-7): ").strip()
            
            if choice == '1':
                diagnose_system()
            elif choice == '2':
                test_gpu_performance()
            elif choice == '3':
                test_dataloader_performance()
            elif choice == '4':
                diagnose_model_complexity()
            elif choice == '5':
                get_optimization_suggestions()
            elif choice == '6':
                print("ğŸ” æ‰§è¡Œå®Œæ•´è¯Šæ–­...")
                diagnose_system()
                test_gpu_performance()
                test_dataloader_performance()
                diagnose_model_complexity()
                get_optimization_suggestions()
            elif choice == '7':
                print("ğŸ‘‹ é€€å‡º")
                break
            else:
                print("âŒ è¯·è¾“å…¥ 1-7")
                
        except KeyboardInterrupt:
            print("\nğŸ‘‹ é€€å‡º")
            break
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")

if __name__ == "__main__":
    main()
